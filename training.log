2025-03-09 06:37:15,755 - INFO - {'checkpoints': {'checkpoint_interval': 2000, 'checkpoints_path': 'checkpoints', 'checkpoints_path_is_shared_file_system': False, 'resume_checkpoint_path': None, 'save_final_state': False, 'save_initial_state': False}, 'data_stages': [{'data': {'dataset': {'dataset_folder': ['datasets/smollm2-corpus'], 'dataset_weights': [1.0]}, 'num_loading_workers': 0, 'seed': 8}, 'name': 'stable phase', 'start_training_step': 1}], 'general': {'benchmark_csv_path': None, 'consumed_train_samples': None, 'ignore_sanity_checks': True, 'project': 'smollm2', 'run': 'smollm2-135M', 'seed': 8, 'step': None}, 'logging': {'iteration_step_info_interval': 1, 'log_level': 'info', 'log_level_replica': 'info'}, 'model': {'ddp_bucket_cap_mb': 25, 'dtype': 'bfloat16', 'init_method': {'std': 0.041666666666666664}, 'make_vocab_size_divisible_by': 1, 'model_config': {'bos_token_id': 0, 'eos_token_id': 0, 'hidden_act': 'silu', 'hidden_size': 576, 'initializer_range': 0.041666666666666664, 'intermediate_size': 1536, 'is_llama_config': True, 'max_position_embeddings': 2048, 'num_attention_heads': 9, 'num_hidden_layers': 30, 'num_key_value_heads': 3, 'pad_token_id': None, 'pretraining_tp': 1, 'rms_norm_eps': 1e-05, 'rope_interleaved': False, 'rope_scaling': None, 'rope_theta': 10000.0, 'tie_word_embeddings': True, 'use_cache': True, 'vocab_size': 49152, 's3_bucket': 'deepseek-v3-train-mar-2025', 's3_checkpoint_folder': 'checkpoints', 's3_log_folder': 'logs', 's3_log_file_name': 'training.log', 'compression_ratio': 4, 'num_experts': 4, 'num_shared_experts': 1, 'top_k': 2}}, 'optimizer': {'accumulate_grad_in_fp32': True, 'clip_grad': 1.0, 'learning_rate_scheduler': {'learning_rate': 0.003, 'lr_decay_starting_step': 1600000, 'lr_decay_steps': 400000, 'lr_decay_style': 'linear', 'lr_warmup_steps': 2000, 'lr_warmup_style': 'linear', 'min_decay_lr': 0}, 'optimizer_factory': {'adam_beta1': 0.9, 'adam_beta2': 0.95, 'adam_eps': 1e-08, 'name': 'adamW', 'torch_adam_is_fused': True}, 'weight_decay': 0.01, 'zero_stage': 0}, 'parallelism': {'dp': 64, 'expert_parallel_size': 1, 'pp': 1, 'pp_engine': '1f1b', 'recompute_layer': False, 'tp': 1, 'tp_linear_async_communication': True, 'tp_mode': 'REDUCE_SCATTER', 'tp_recompute_allgather': True}, 'profiler': None, 'tokenizer': {'tokenizer_max_length': None, 'tokenizer_name_or_path': 'HuggingFaceTB/cosmo2-tokenizer', 'tokenizer_revision': None}, 'tokens': {'batch_accumulation_per_replica': 1, 'limit_test_batches': 0, 'limit_val_batches': 0, 'micro_batch_size': 8, 'sequence_length': 512, 'train_steps': 2000000, 'val_check_interval': 1000}}
2025-03-09 06:37:20,135 - INFO - DeepSeekV3Model(
  (embed_tokens): Embedding(49152, 576)
  (rotary_emb): RotaryPositionalEmbedding()
  (layers): ModuleList(
    (0-29): 30 x LlamaDecoderLayer(
      (self_attn): MultiHeadLatentAttention(
        (kv_proj_D): Linear(in_features=576, out_features=144, bias=False)
        (q_proj_D): Linear(in_features=576, out_features=144, bias=False)
        (k_proj_U): Linear(in_features=144, out_features=288, bias=False)
        (v_proj_U): Linear(in_features=144, out_features=576, bias=False)
        (q_proj_U): Linear(in_features=144, out_features=288, bias=False)
        (rope_k): Linear(in_features=576, out_features=288, bias=False)
        (rope_q): Linear(in_features=144, out_features=288, bias=False)
        (o_proj): Linear(in_features=576, out_features=576, bias=False)
        (rotary_emb): RotaryPositionalEmbedding()
      )
      (input_layernorm): LlamaRMSNorm()
      (mlp): LlamaMLP(
        (moe): DeepSeekMOE(
          (shared_experts): ModuleList(
            (0): DeepSeekExpertLayer(
              (gate_proj): Linear(in_features=576, out_features=1536, bias=False)
              (up_proj): Linear(in_features=576, out_features=1536, bias=False)
              (down_proj): Linear(in_features=1536, out_features=576, bias=False)
              (act_fn): SiLU()
            )
          )
          (routed_experts): ModuleList(
            (0-2): 3 x DeepSeekExpertLayer(
              (gate_proj): Linear(in_features=576, out_features=1536, bias=False)
              (up_proj): Linear(in_features=576, out_features=1536, bias=False)
              (down_proj): Linear(in_features=1536, out_features=576, bias=False)
              (act_fn): SiLU()
            )
          )
          (routing_fn): Linear(in_features=576, out_features=3, bias=False)
        )
      )
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (lm_head): Linear(in_features=576, out_features=49152, bias=False)
)
2025-03-09 06:37:20,138 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:37:20,140 - INFO - Total parameters: 373030938
2025-03-09 06:37:20,292 - INFO - tokenizer parallelism set to false
2025-03-09 06:37:20,292 - INFO - dataset loading config set
2025-03-09 06:37:21,042 - INFO - dataset loaded
2025-03-09 06:37:21,043 - INFO - Training with learning rate schedule:
2025-03-09 06:37:21,043 - INFO - Max LR: 0.0003
2025-03-09 06:37:21,043 - INFO - Warmup Steps: 3000
2025-03-09 06:37:21,043 - INFO - Max Steps: 60000
2025-03-09 06:37:21,043 - INFO - Min LR: 1.4999999999999999e-05
2025-03-09 06:37:21,043 - INFO - Gradient Accumulation Steps: 1
2025-03-09 06:37:21,043 - INFO - Effective Batch Size: 8
2025-03-09 06:37:21,043 - INFO - 
GPU Memory Stats at start of training:
2025-03-09 06:37:21,043 - INFO - GPU Memory allocated: 1431.94 MB
2025-03-09 06:37:21,044 - INFO - GPU Memory reserved: 1614.00 MB
2025-03-09 06:37:21,044 - INFO - Max GPU Memory allocated: 1431.94 MB
2025-03-09 06:37:23,752 - INFO - Batch 1, Running Avg Loss: 11.52892
2025-03-09 06:37:23,925 - INFO - Batch 0 finished
2025-03-09 06:37:23,925 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:37:41,064 - INFO - Batch 26, Running Avg Loss: 11.49811
2025-03-09 06:37:58,059 - INFO - Batch 51, Running Avg Loss: 11.43193
2025-03-09 06:38:15,074 - INFO - Batch 76, Running Avg Loss: 11.35670
2025-03-09 06:38:32,155 - INFO - Batch 101, Running Avg Loss: 11.22341
2025-03-09 06:38:32,169 - INFO - Batch 100 finished
2025-03-09 06:38:32,170 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:38:49,487 - INFO - Batch 126, Running Avg Loss: 10.95237
2025-03-09 06:39:07,071 - INFO - Batch 151, Running Avg Loss: 10.67801
2025-03-09 06:39:24,940 - INFO - Batch 176, Running Avg Loss: 10.45184
2025-03-09 06:39:42,646 - INFO - Batch 201, Running Avg Loss: 10.24213
2025-03-09 06:39:42,665 - INFO - Batch 200 finished
2025-03-09 06:39:42,666 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:40:00,359 - INFO - Batch 226, Running Avg Loss: 10.03996
2025-03-09 06:40:18,119 - INFO - Batch 251, Running Avg Loss: 9.85758
2025-03-09 06:40:35,887 - INFO - Batch 276, Running Avg Loss: 9.69139
2025-03-09 06:40:53,623 - INFO - Batch 301, Running Avg Loss: 9.53905
2025-03-09 06:40:53,642 - INFO - Batch 300 finished
2025-03-09 06:40:53,643 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:41:11,433 - INFO - Batch 326, Running Avg Loss: 9.39036
2025-03-09 06:41:29,068 - INFO - Batch 351, Running Avg Loss: 9.25570
2025-03-09 06:41:46,847 - INFO - Batch 376, Running Avg Loss: 9.12058
2025-03-09 06:42:04,675 - INFO - Batch 401, Running Avg Loss: 9.00010
2025-03-09 06:42:04,692 - INFO - Batch 400 finished
2025-03-09 06:42:04,692 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:42:22,668 - INFO - Batch 426, Running Avg Loss: 8.89024
2025-03-09 06:42:40,418 - INFO - Batch 451, Running Avg Loss: 8.78599
2025-03-09 06:42:58,160 - INFO - Batch 476, Running Avg Loss: 8.68759
2025-03-09 06:43:15,906 - INFO - Batch 501, Running Avg Loss: 8.59315
2025-03-09 06:43:15,924 - INFO - Batch 500 finished
2025-03-09 06:43:15,924 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:43:33,655 - INFO - Batch 526, Running Avg Loss: 8.51155
2025-03-09 06:43:51,435 - INFO - Batch 551, Running Avg Loss: 8.43339
2025-03-09 06:44:09,213 - INFO - Batch 576, Running Avg Loss: 8.35374
2025-03-09 06:44:26,967 - INFO - Batch 601, Running Avg Loss: 8.28313
2025-03-09 06:44:26,983 - INFO - Batch 600 finished
2025-03-09 06:44:26,983 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:44:44,695 - INFO - Batch 626, Running Avg Loss: 8.21394
2025-03-09 06:45:02,380 - INFO - Batch 651, Running Avg Loss: 8.15349
2025-03-09 06:45:20,261 - INFO - Batch 676, Running Avg Loss: 8.09077
2025-03-09 06:45:37,970 - INFO - Batch 701, Running Avg Loss: 8.03328
2025-03-09 06:45:37,985 - INFO - Batch 700 finished
2025-03-09 06:45:37,986 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:45:55,599 - INFO - Batch 726, Running Avg Loss: 7.98192
2025-03-09 06:46:13,236 - INFO - Batch 751, Running Avg Loss: 7.92664
2025-03-09 06:46:30,889 - INFO - Batch 776, Running Avg Loss: 7.87488
2025-03-09 06:46:48,495 - INFO - Batch 801, Running Avg Loss: 7.82737
2025-03-09 06:46:48,511 - INFO - Batch 800 finished
2025-03-09 06:46:48,511 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:47:06,291 - INFO - Batch 826, Running Avg Loss: 7.77950
2025-03-09 06:47:24,078 - INFO - Batch 851, Running Avg Loss: 7.73376
2025-03-09 06:47:41,710 - INFO - Batch 876, Running Avg Loss: 7.69159
2025-03-09 06:47:59,235 - INFO - Batch 901, Running Avg Loss: 7.64929
2025-03-09 06:47:59,248 - INFO - Batch 900 finished
2025-03-09 06:47:59,248 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:48:16,864 - INFO - Batch 926, Running Avg Loss: 7.60409
2025-03-09 06:48:34,508 - INFO - Batch 951, Running Avg Loss: 7.56157
2025-03-09 06:48:52,041 - INFO - Batch 976, Running Avg Loss: 7.52415
2025-03-09 06:49:09,553 - INFO - Batch 1001, Running Avg Loss: 7.48562
2025-03-09 06:49:09,567 - INFO - Batch 1000 finished
2025-03-09 06:49:09,567 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:49:27,069 - INFO - Batch 1026, Running Avg Loss: 7.44780
2025-03-09 06:49:44,623 - INFO - Batch 1051, Running Avg Loss: 7.41283
2025-03-09 06:50:02,146 - INFO - Batch 1076, Running Avg Loss: 7.38041
2025-03-09 06:50:19,420 - INFO - Batch 1101, Running Avg Loss: 7.34525
2025-03-09 06:50:19,432 - INFO - Batch 1100 finished
2025-03-09 06:50:19,432 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:50:36,689 - INFO - Batch 1126, Running Avg Loss: 7.31237
2025-03-09 06:50:53,966 - INFO - Batch 1151, Running Avg Loss: 7.28113
2025-03-09 06:51:11,320 - INFO - Batch 1176, Running Avg Loss: 7.25119
2025-03-09 06:51:28,933 - INFO - Batch 1201, Running Avg Loss: 7.21807
2025-03-09 06:51:28,948 - INFO - Batch 1200 finished
2025-03-09 06:51:28,949 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:51:46,685 - INFO - Batch 1226, Running Avg Loss: 7.18937
2025-03-09 06:52:04,269 - INFO - Batch 1251, Running Avg Loss: 7.16054
2025-03-09 06:52:21,854 - INFO - Batch 1276, Running Avg Loss: 7.13149
2025-03-09 06:52:39,452 - INFO - Batch 1301, Running Avg Loss: 7.10319
2025-03-09 06:52:39,467 - INFO - Batch 1300 finished
2025-03-09 06:52:39,467 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:52:56,981 - INFO - Batch 1326, Running Avg Loss: 7.07510
2025-03-09 06:53:14,685 - INFO - Batch 1351, Running Avg Loss: 7.04937
2025-03-09 06:53:32,267 - INFO - Batch 1376, Running Avg Loss: 7.02114
2025-03-09 06:53:49,830 - INFO - Batch 1401, Running Avg Loss: 6.99535
2025-03-09 06:53:49,848 - INFO - Batch 1400 finished
2025-03-09 06:53:49,848 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:54:07,333 - INFO - Batch 1426, Running Avg Loss: 6.97140
2025-03-09 06:54:24,943 - INFO - Batch 1451, Running Avg Loss: 6.94611
2025-03-09 06:54:42,403 - INFO - Batch 1476, Running Avg Loss: 6.92099
2025-03-09 06:55:00,022 - INFO - Batch 1501, Running Avg Loss: 6.89804
2025-03-09 06:55:00,038 - INFO - Batch 1500 finished
2025-03-09 06:55:00,038 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:55:17,643 - INFO - Batch 1526, Running Avg Loss: 6.87442
2025-03-09 06:55:35,170 - INFO - Batch 1551, Running Avg Loss: 6.85284
2025-03-09 06:55:52,667 - INFO - Batch 1576, Running Avg Loss: 6.83140
2025-03-09 06:56:10,199 - INFO - Batch 1601, Running Avg Loss: 6.80972
2025-03-09 06:56:10,214 - INFO - Batch 1600 finished
2025-03-09 06:56:10,215 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:56:27,757 - INFO - Batch 1626, Running Avg Loss: 6.78826
2025-03-09 06:56:45,239 - INFO - Batch 1651, Running Avg Loss: 6.76814
2025-03-09 06:57:02,741 - INFO - Batch 1676, Running Avg Loss: 6.74808
2025-03-09 06:57:20,200 - INFO - Batch 1701, Running Avg Loss: 6.72688
2025-03-09 06:57:20,217 - INFO - Batch 1700 finished
2025-03-09 06:57:20,217 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:57:37,912 - INFO - Batch 1726, Running Avg Loss: 6.70693
2025-03-09 06:57:55,545 - INFO - Batch 1751, Running Avg Loss: 6.68862
2025-03-09 06:58:13,045 - INFO - Batch 1776, Running Avg Loss: 6.66905
2025-03-09 06:58:30,484 - INFO - Batch 1801, Running Avg Loss: 6.64957
2025-03-09 06:58:30,499 - INFO - Batch 1800 finished
2025-03-09 06:58:30,499 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:58:48,015 - INFO - Batch 1826, Running Avg Loss: 6.63080
2025-03-09 06:59:05,616 - INFO - Batch 1851, Running Avg Loss: 6.61227
2025-03-09 06:59:23,118 - INFO - Batch 1876, Running Avg Loss: 6.59383
2025-03-09 06:59:40,744 - INFO - Batch 1901, Running Avg Loss: 6.57590
2025-03-09 06:59:40,760 - INFO - Batch 1900 finished
2025-03-09 06:59:40,761 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 06:59:58,307 - INFO - Batch 1926, Running Avg Loss: 6.55824
2025-03-09 07:00:15,716 - INFO - Batch 1951, Running Avg Loss: 6.54076
2025-03-09 07:00:32,873 - INFO - Batch 1976, Running Avg Loss: 6.52344
2025-03-09 07:00:50,176 - INFO - Batch 2001, Running Avg Loss: 6.50683
2025-03-09 07:00:50,188 - INFO - Batch 2000 finished
2025-03-09 07:00:50,188 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:01:07,448 - INFO - Batch 2026, Running Avg Loss: 6.48857
2025-03-09 07:01:24,774 - INFO - Batch 2051, Running Avg Loss: 6.47232
2025-03-09 07:01:42,074 - INFO - Batch 2076, Running Avg Loss: 6.45617
2025-03-09 07:01:59,583 - INFO - Batch 2101, Running Avg Loss: 6.44009
2025-03-09 07:01:59,599 - INFO - Batch 2100 finished
2025-03-09 07:01:59,599 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:02:17,167 - INFO - Batch 2126, Running Avg Loss: 6.42435
2025-03-09 07:02:34,647 - INFO - Batch 2151, Running Avg Loss: 6.40961
2025-03-09 07:02:52,080 - INFO - Batch 2176, Running Avg Loss: 6.39462
2025-03-09 07:03:09,484 - INFO - Batch 2201, Running Avg Loss: 6.37896
2025-03-09 07:03:09,502 - INFO - Batch 2200 finished
2025-03-09 07:03:09,503 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:03:26,993 - INFO - Batch 2226, Running Avg Loss: 6.36300
2025-03-09 07:03:44,569 - INFO - Batch 2251, Running Avg Loss: 6.34857
2025-03-09 07:04:02,203 - INFO - Batch 2276, Running Avg Loss: 6.33418
2025-03-09 07:04:19,699 - INFO - Batch 2301, Running Avg Loss: 6.31869
2025-03-09 07:04:19,717 - INFO - Batch 2300 finished
2025-03-09 07:04:19,718 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:04:37,165 - INFO - Batch 2326, Running Avg Loss: 6.30378
2025-03-09 07:04:54,690 - INFO - Batch 2351, Running Avg Loss: 6.29037
2025-03-09 07:05:12,213 - INFO - Batch 2376, Running Avg Loss: 6.27539
2025-03-09 07:05:29,849 - INFO - Batch 2401, Running Avg Loss: 6.26135
2025-03-09 07:05:29,866 - INFO - Batch 2400 finished
2025-03-09 07:05:29,867 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:05:47,333 - INFO - Batch 2426, Running Avg Loss: 6.24768
2025-03-09 07:06:04,793 - INFO - Batch 2451, Running Avg Loss: 6.23429
2025-03-09 07:06:22,297 - INFO - Batch 2476, Running Avg Loss: 6.22143
2025-03-09 07:06:39,995 - INFO - Batch 2501, Running Avg Loss: 6.20876
2025-03-09 07:06:40,010 - INFO - 
GPU Memory Stats at step 2500:
2025-03-09 07:06:40,011 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 07:06:40,011 - INFO - GPU Memory reserved: 16544.00 MB
2025-03-09 07:06:40,011 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 07:06:40,011 - INFO - learning rate: 0.00025010
2025-03-09 07:06:40,011 - INFO - Ep 1 (Step 002500): Avg loss 6.209 | 10244096 tokens seen
2025-03-09 07:06:40,011 - INFO - optimizer lr: 0.00025010
2025-03-09 07:06:40,011 - INFO - scheduler lr: 0.00025010
2025-03-09 07:06:40,012 - INFO - Selected prompt: With the development of science and technology, computer has become more and more 
2025-03-09 07:06:40,012 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:06:40,012 - INFO - random_topk: 3
2025-03-09 07:06:40,012 - INFO - random_temperature: 0.7203031098500081
2025-03-09 07:06:40,012 - INFO - global step 2500 , batch_idx 2500 => generating text
2025-03-09 07:06:40,012 - INFO - Generating on device cuda
2025-03-09 07:07:13,626 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:07:13,626 - INFO - With the development of science and technology, computer has become more and more 20s when it comes to a new school student, a group of friends, and you might have a new school that has become a new and important. In this course unit, we will delve into the fascinating world of social science, specifically exploring how people in the field can be used in the field of the world. We will explore the concept of digital communication with a community of the world around you, and learn about the ways that it is to be more important to understand the ways in which people can be used and engaged in the field of the United States.

Imagine you're playing a new school where you were working with a new school. You would find out about what you need to do, but you would need to take a new life. This is where the study of a community, or even if you were working on a new school, you would need to use your own unique abilities and feelings. This is what happens with a person who has been working with a community, or that it can be challenging.

Now, let's imagine a story about a group of people who lived in the world. You would use your own story and take a story to create a new school, or maybe even a group of a group of people who live in
2025-03-09 07:07:13,626 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:07:40,142 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_2500_steps_avg_loss_6.20876_optimizer_lr_0.00025010.pth
2025-03-09 07:07:40,249 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 07:07:40,249 - INFO - Batch 2500 finished
2025-03-09 07:07:40,249 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:07:57,545 - INFO - Batch 2526, Running Avg Loss: 6.19570
2025-03-09 07:08:15,007 - INFO - Batch 2551, Running Avg Loss: 6.18249
2025-03-09 07:08:32,741 - INFO - Batch 2576, Running Avg Loss: 6.17053
2025-03-09 07:08:50,241 - INFO - Batch 2601, Running Avg Loss: 6.15918
2025-03-09 07:08:50,256 - INFO - Batch 2600 finished
2025-03-09 07:08:50,256 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:09:07,734 - INFO - Batch 2626, Running Avg Loss: 6.14653
2025-03-09 07:09:25,274 - INFO - Batch 2651, Running Avg Loss: 6.13490
2025-03-09 07:09:42,834 - INFO - Batch 2676, Running Avg Loss: 6.12250
2025-03-09 07:10:00,400 - INFO - Batch 2701, Running Avg Loss: 6.11072
2025-03-09 07:10:00,415 - INFO - Batch 2700 finished
2025-03-09 07:10:00,415 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:10:18,003 - INFO - Batch 2726, Running Avg Loss: 6.09869
2025-03-09 07:10:35,432 - INFO - Batch 2751, Running Avg Loss: 6.08734
2025-03-09 07:10:52,524 - INFO - Batch 2776, Running Avg Loss: 6.07659
2025-03-09 07:11:09,562 - INFO - Batch 2801, Running Avg Loss: 6.06448
2025-03-09 07:11:09,574 - INFO - Batch 2800 finished
2025-03-09 07:11:09,575 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:11:26,967 - INFO - Batch 2826, Running Avg Loss: 6.05375
2025-03-09 07:11:44,396 - INFO - Batch 2851, Running Avg Loss: 6.04215
2025-03-09 07:12:01,899 - INFO - Batch 2876, Running Avg Loss: 6.03019
2025-03-09 07:12:19,409 - INFO - Batch 2901, Running Avg Loss: 6.01871
2025-03-09 07:12:19,424 - INFO - Batch 2900 finished
2025-03-09 07:12:19,425 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:12:36,936 - INFO - Batch 2926, Running Avg Loss: 6.00718
2025-03-09 07:12:54,458 - INFO - Batch 2951, Running Avg Loss: 5.99701
2025-03-09 07:13:11,945 - INFO - Batch 2976, Running Avg Loss: 5.98619
2025-03-09 07:13:29,399 - INFO - Batch 3001, Running Avg Loss: 5.97592
2025-03-09 07:13:29,415 - INFO - Batch 3000 finished
2025-03-09 07:13:29,415 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:13:46,896 - INFO - Batch 3026, Running Avg Loss: 5.96500
2025-03-09 07:14:04,420 - INFO - Batch 3051, Running Avg Loss: 5.95422
2025-03-09 07:14:21,984 - INFO - Batch 3076, Running Avg Loss: 5.94294
2025-03-09 07:14:39,697 - INFO - Batch 3101, Running Avg Loss: 5.93278
2025-03-09 07:14:39,712 - INFO - Batch 3100 finished
2025-03-09 07:14:39,713 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:14:57,145 - INFO - Batch 3126, Running Avg Loss: 5.92242
2025-03-09 07:15:14,579 - INFO - Batch 3151, Running Avg Loss: 5.91234
2025-03-09 07:15:32,160 - INFO - Batch 3176, Running Avg Loss: 5.90257
2025-03-09 07:15:49,752 - INFO - Batch 3201, Running Avg Loss: 5.89343
2025-03-09 07:15:49,769 - INFO - Batch 3200 finished
2025-03-09 07:15:49,770 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:16:07,290 - INFO - Batch 3226, Running Avg Loss: 5.88411
2025-03-09 07:16:24,815 - INFO - Batch 3251, Running Avg Loss: 5.87459
2025-03-09 07:16:42,185 - INFO - Batch 3276, Running Avg Loss: 5.86500
2025-03-09 07:16:59,600 - INFO - Batch 3301, Running Avg Loss: 5.85621
2025-03-09 07:16:59,616 - INFO - Batch 3300 finished
2025-03-09 07:16:59,617 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:17:17,006 - INFO - Batch 3326, Running Avg Loss: 5.84691
2025-03-09 07:17:34,526 - INFO - Batch 3351, Running Avg Loss: 5.83801
2025-03-09 07:17:51,999 - INFO - Batch 3376, Running Avg Loss: 5.82996
2025-03-09 07:18:09,617 - INFO - Batch 3401, Running Avg Loss: 5.82122
2025-03-09 07:18:09,634 - INFO - Batch 3400 finished
2025-03-09 07:18:09,634 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:18:27,073 - INFO - Batch 3426, Running Avg Loss: 5.81293
2025-03-09 07:18:44,585 - INFO - Batch 3451, Running Avg Loss: 5.80493
2025-03-09 07:19:02,047 - INFO - Batch 3476, Running Avg Loss: 5.79667
2025-03-09 07:19:19,448 - INFO - Batch 3501, Running Avg Loss: 5.78865
2025-03-09 07:19:19,465 - INFO - Batch 3500 finished
2025-03-09 07:19:19,465 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:19:36,834 - INFO - Batch 3526, Running Avg Loss: 5.78077
2025-03-09 07:19:54,227 - INFO - Batch 3551, Running Avg Loss: 5.77310
2025-03-09 07:20:11,683 - INFO - Batch 3576, Running Avg Loss: 5.76592
2025-03-09 07:20:29,181 - INFO - Batch 3601, Running Avg Loss: 5.75863
2025-03-09 07:20:29,198 - INFO - Batch 3600 finished
2025-03-09 07:20:29,199 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:20:46,485 - INFO - Batch 3626, Running Avg Loss: 5.75155
2025-03-09 07:21:03,564 - INFO - Batch 3651, Running Avg Loss: 5.74388
2025-03-09 07:21:20,841 - INFO - Batch 3676, Running Avg Loss: 5.73620
2025-03-09 07:21:38,187 - INFO - Batch 3701, Running Avg Loss: 5.72950
2025-03-09 07:21:38,204 - INFO - Batch 3700 finished
2025-03-09 07:21:38,204 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:21:55,666 - INFO - Batch 3726, Running Avg Loss: 5.72201
2025-03-09 07:22:13,134 - INFO - Batch 3751, Running Avg Loss: 5.71527
2025-03-09 07:22:30,557 - INFO - Batch 3776, Running Avg Loss: 5.70813
2025-03-09 07:22:47,960 - INFO - Batch 3801, Running Avg Loss: 5.70147
2025-03-09 07:22:47,976 - INFO - Batch 3800 finished
2025-03-09 07:22:47,976 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:23:05,585 - INFO - Batch 3826, Running Avg Loss: 5.69512
2025-03-09 07:23:23,060 - INFO - Batch 3851, Running Avg Loss: 5.68792
2025-03-09 07:23:40,516 - INFO - Batch 3876, Running Avg Loss: 5.68103
2025-03-09 07:23:57,977 - INFO - Batch 3901, Running Avg Loss: 5.67455
2025-03-09 07:23:57,993 - INFO - Batch 3900 finished
2025-03-09 07:23:57,993 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:24:15,483 - INFO - Batch 3926, Running Avg Loss: 5.66799
2025-03-09 07:24:33,144 - INFO - Batch 3951, Running Avg Loss: 5.66115
2025-03-09 07:24:50,552 - INFO - Batch 3976, Running Avg Loss: 5.65496
2025-03-09 07:25:07,991 - INFO - Batch 4001, Running Avg Loss: 5.64904
2025-03-09 07:25:08,008 - INFO - Batch 4000 finished
2025-03-09 07:25:08,009 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:25:25,387 - INFO - Batch 4026, Running Avg Loss: 5.64283
2025-03-09 07:25:42,784 - INFO - Batch 4051, Running Avg Loss: 5.63690
2025-03-09 07:26:00,340 - INFO - Batch 4076, Running Avg Loss: 5.63042
2025-03-09 07:26:17,939 - INFO - Batch 4101, Running Avg Loss: 5.62439
2025-03-09 07:26:17,959 - INFO - Batch 4100 finished
2025-03-09 07:26:17,960 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:26:35,380 - INFO - Batch 4126, Running Avg Loss: 5.61827
2025-03-09 07:26:52,755 - INFO - Batch 4151, Running Avg Loss: 5.61266
2025-03-09 07:27:10,293 - INFO - Batch 4176, Running Avg Loss: 5.60723
2025-03-09 07:27:27,939 - INFO - Batch 4201, Running Avg Loss: 5.60155
2025-03-09 07:27:27,957 - INFO - Batch 4200 finished
2025-03-09 07:27:27,958 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:27:45,707 - INFO - Batch 4226, Running Avg Loss: 5.59582
2025-03-09 07:28:03,143 - INFO - Batch 4251, Running Avg Loss: 5.59042
2025-03-09 07:28:20,524 - INFO - Batch 4276, Running Avg Loss: 5.58513
2025-03-09 07:28:37,988 - INFO - Batch 4301, Running Avg Loss: 5.57952
2025-03-09 07:28:38,008 - INFO - Batch 4300 finished
2025-03-09 07:28:38,008 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:28:55,534 - INFO - Batch 4326, Running Avg Loss: 5.57455
2025-03-09 07:29:12,970 - INFO - Batch 4351, Running Avg Loss: 5.56922
2025-03-09 07:29:30,574 - INFO - Batch 4376, Running Avg Loss: 5.56338
2025-03-09 07:29:48,086 - INFO - Batch 4401, Running Avg Loss: 5.55870
2025-03-09 07:29:48,102 - INFO - Batch 4400 finished
2025-03-09 07:29:48,103 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:30:05,536 - INFO - Batch 4426, Running Avg Loss: 5.55378
2025-03-09 07:30:23,047 - INFO - Batch 4451, Running Avg Loss: 5.54844
2025-03-09 07:30:40,362 - INFO - Batch 4476, Running Avg Loss: 5.54352
2025-03-09 07:30:57,543 - INFO - Batch 4501, Running Avg Loss: 5.53887
2025-03-09 07:30:57,555 - INFO - Batch 4500 finished
2025-03-09 07:30:57,556 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:31:14,903 - INFO - Batch 4526, Running Avg Loss: 5.53439
2025-03-09 07:31:32,313 - INFO - Batch 4551, Running Avg Loss: 5.53017
2025-03-09 07:31:49,797 - INFO - Batch 4576, Running Avg Loss: 5.52533
2025-03-09 07:32:07,244 - INFO - Batch 4601, Running Avg Loss: 5.52030
2025-03-09 07:32:07,259 - INFO - Batch 4600 finished
2025-03-09 07:32:07,260 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:32:24,589 - INFO - Batch 4626, Running Avg Loss: 5.51543
2025-03-09 07:32:41,929 - INFO - Batch 4651, Running Avg Loss: 5.51063
2025-03-09 07:32:59,289 - INFO - Batch 4676, Running Avg Loss: 5.50611
2025-03-09 07:33:16,751 - INFO - Batch 4701, Running Avg Loss: 5.50115
2025-03-09 07:33:16,767 - INFO - Batch 4700 finished
2025-03-09 07:33:16,768 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:33:34,247 - INFO - Batch 4726, Running Avg Loss: 5.49642
2025-03-09 07:33:51,718 - INFO - Batch 4751, Running Avg Loss: 5.49217
2025-03-09 07:34:09,169 - INFO - Batch 4776, Running Avg Loss: 5.48838
2025-03-09 07:34:26,901 - INFO - Batch 4801, Running Avg Loss: 5.48413
2025-03-09 07:34:26,917 - INFO - Batch 4800 finished
2025-03-09 07:34:26,918 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:34:44,469 - INFO - Batch 4826, Running Avg Loss: 5.47986
2025-03-09 07:35:02,014 - INFO - Batch 4851, Running Avg Loss: 5.47542
2025-03-09 07:35:19,475 - INFO - Batch 4876, Running Avg Loss: 5.47135
2025-03-09 07:35:36,846 - INFO - Batch 4901, Running Avg Loss: 5.46725
2025-03-09 07:35:36,862 - INFO - Batch 4900 finished
2025-03-09 07:35:36,863 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:35:54,241 - INFO - Batch 4926, Running Avg Loss: 5.46332
2025-03-09 07:36:11,964 - INFO - Batch 4951, Running Avg Loss: 5.45874
2025-03-09 07:36:29,549 - INFO - Batch 4976, Running Avg Loss: 5.45488
2025-03-09 07:36:47,065 - INFO - Batch 5001, Running Avg Loss: 5.45092
2025-03-09 07:36:47,084 - INFO - 
GPU Memory Stats at step 5000:
2025-03-09 07:36:47,085 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 07:36:47,085 - INFO - GPU Memory reserved: 16564.00 MB
2025-03-09 07:36:47,085 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 07:36:47,085 - INFO - learning rate: 0.00000009
2025-03-09 07:36:47,085 - INFO - Ep 1 (Step 005000): Avg loss 5.451 | 20484096 tokens seen
2025-03-09 07:36:47,085 - INFO - optimizer lr: 0.00000009
2025-03-09 07:36:47,085 - INFO - scheduler lr: 0.00000009
2025-03-09 07:36:47,086 - INFO - Selected prompt: Lobster, California spiny The California Spiny Lobster fishery is a small but locally 
2025-03-09 07:36:47,086 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:36:47,086 - INFO - random_topk: 7
2025-03-09 07:36:47,086 - INFO - random_temperature: 0.7926058671194636
2025-03-09 07:36:47,086 - INFO - global step 5000 , batch_idx 5000 => generating text
2025-03-09 07:36:47,086 - INFO - Generating on device cuda
2025-03-09 07:37:10,805 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:37:10,805 - INFO - Lobster, California spiny The California Spiny Lobster fishery is a small but locally 2890s when two people who lived in a beautiful place filled with beautiful landscapes, beautiful flowers, and stunning buildings. They all share their unique experiences, learn about their history, and discover new things that make them feel like a young age.

One day, a young girl named Lily came from a place called "Aab" in the 1900s in the late nineteenth century. He would find the way to learn about this new life and help them understand their unique experiences.

One day, while exploring the beautiful city of the Middle East, she asked her what her friends were doing - she decided to give them a try and feel happy. She explained that this was a way of understanding and appreciating their unique identity.

Touched by their work, she asked her what was wrong.

"What's a 'Sap-year-old?"

2025-03-09 07:37:10,805 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:37:37,771 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_5000_steps_avg_loss_5.45092_optimizer_lr_0.00000009.pth
2025-03-09 07:37:37,976 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 07:37:37,977 - INFO - Batch 5000 finished
2025-03-09 07:37:37,977 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:37:55,531 - INFO - Batch 5026, Running Avg Loss: 5.44653
2025-03-09 07:38:12,886 - INFO - Batch 5051, Running Avg Loss: 5.44230
2025-03-09 07:38:30,347 - INFO - Batch 5076, Running Avg Loss: 5.43868
2025-03-09 07:38:47,815 - INFO - Batch 5101, Running Avg Loss: 5.43440
2025-03-09 07:38:47,829 - INFO - Batch 5100 finished
2025-03-09 07:38:47,830 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:39:05,306 - INFO - Batch 5126, Running Avg Loss: 5.43057
2025-03-09 07:39:22,628 - INFO - Batch 5151, Running Avg Loss: 5.42689
2025-03-09 07:39:39,923 - INFO - Batch 5176, Running Avg Loss: 5.42285
2025-03-09 07:39:57,255 - INFO - Batch 5201, Running Avg Loss: 5.41920
2025-03-09 07:39:57,271 - INFO - Batch 5200 finished
2025-03-09 07:39:57,272 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:40:14,712 - INFO - Batch 5226, Running Avg Loss: 5.41567
2025-03-09 07:40:32,185 - INFO - Batch 5251, Running Avg Loss: 5.41161
2025-03-09 07:40:49,523 - INFO - Batch 5276, Running Avg Loss: 5.40807
2025-03-09 07:41:06,592 - INFO - Batch 5301, Running Avg Loss: 5.40459
2025-03-09 07:41:06,608 - INFO - Batch 5300 finished
2025-03-09 07:41:06,608 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:41:23,990 - INFO - Batch 5326, Running Avg Loss: 5.40080
2025-03-09 07:41:41,564 - INFO - Batch 5351, Running Avg Loss: 5.39745
2025-03-09 07:41:59,105 - INFO - Batch 5376, Running Avg Loss: 5.39453
2025-03-09 07:42:16,719 - INFO - Batch 5401, Running Avg Loss: 5.39109
2025-03-09 07:42:16,734 - INFO - Batch 5400 finished
2025-03-09 07:42:16,734 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:42:34,255 - INFO - Batch 5426, Running Avg Loss: 5.38768
2025-03-09 07:42:51,745 - INFO - Batch 5451, Running Avg Loss: 5.38416
2025-03-09 07:43:09,295 - INFO - Batch 5476, Running Avg Loss: 5.38102
2025-03-09 07:43:26,805 - INFO - Batch 5501, Running Avg Loss: 5.37748
2025-03-09 07:43:26,821 - INFO - Batch 5500 finished
2025-03-09 07:43:26,821 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:43:44,245 - INFO - Batch 5526, Running Avg Loss: 5.37407
2025-03-09 07:44:01,764 - INFO - Batch 5551, Running Avg Loss: 5.37096
2025-03-09 07:44:19,211 - INFO - Batch 5576, Running Avg Loss: 5.36796
2025-03-09 07:44:36,705 - INFO - Batch 5601, Running Avg Loss: 5.36467
2025-03-09 07:44:36,722 - INFO - Batch 5600 finished
2025-03-09 07:44:36,722 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:44:54,189 - INFO - Batch 5626, Running Avg Loss: 5.36128
2025-03-09 07:45:11,638 - INFO - Batch 5651, Running Avg Loss: 5.35816
2025-03-09 07:45:29,120 - INFO - Batch 5676, Running Avg Loss: 5.35514
2025-03-09 07:45:46,728 - INFO - Batch 5701, Running Avg Loss: 5.35188
2025-03-09 07:45:46,744 - INFO - Batch 5700 finished
2025-03-09 07:45:46,744 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:46:04,123 - INFO - Batch 5726, Running Avg Loss: 5.34888
2025-03-09 07:46:21,553 - INFO - Batch 5751, Running Avg Loss: 5.34601
2025-03-09 07:46:38,974 - INFO - Batch 5776, Running Avg Loss: 5.34311
2025-03-09 07:46:56,473 - INFO - Batch 5801, Running Avg Loss: 5.34027
2025-03-09 07:46:56,489 - INFO - Batch 5800 finished
2025-03-09 07:46:56,490 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:47:13,930 - INFO - Batch 5826, Running Avg Loss: 5.33746
2025-03-09 07:47:31,360 - INFO - Batch 5851, Running Avg Loss: 5.33477
2025-03-09 07:47:48,687 - INFO - Batch 5876, Running Avg Loss: 5.33200
2025-03-09 07:48:06,045 - INFO - Batch 5901, Running Avg Loss: 5.32884
2025-03-09 07:48:06,061 - INFO - Batch 5900 finished
2025-03-09 07:48:06,061 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:48:23,366 - INFO - Batch 5926, Running Avg Loss: 5.32593
2025-03-09 07:48:40,832 - INFO - Batch 5951, Running Avg Loss: 5.32310
2025-03-09 07:48:58,392 - INFO - Batch 5976, Running Avg Loss: 5.32001
2025-03-09 07:49:16,052 - INFO - Batch 6001, Running Avg Loss: 5.31761
2025-03-09 07:49:16,070 - INFO - Batch 6000 finished
2025-03-09 07:49:16,070 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:49:33,442 - INFO - Batch 6026, Running Avg Loss: 5.31462
2025-03-09 07:49:50,760 - INFO - Batch 6051, Running Avg Loss: 5.31169
2025-03-09 07:50:08,114 - INFO - Batch 6076, Running Avg Loss: 5.30897
2025-03-09 07:50:25,467 - INFO - Batch 6101, Running Avg Loss: 5.30612
2025-03-09 07:50:25,481 - INFO - Batch 6100 finished
2025-03-09 07:50:25,482 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:50:42,895 - INFO - Batch 6126, Running Avg Loss: 5.30323
2025-03-09 07:51:00,240 - INFO - Batch 6151, Running Avg Loss: 5.30049
2025-03-09 07:51:17,661 - INFO - Batch 6176, Running Avg Loss: 5.29780
2025-03-09 07:51:35,193 - INFO - Batch 6201, Running Avg Loss: 5.29487
2025-03-09 07:51:35,210 - INFO - Batch 6200 finished
2025-03-09 07:51:35,210 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:51:52,704 - INFO - Batch 6226, Running Avg Loss: 5.29230
2025-03-09 07:52:10,373 - INFO - Batch 6251, Running Avg Loss: 5.28986
2025-03-09 07:52:27,887 - INFO - Batch 6276, Running Avg Loss: 5.28745
2025-03-09 07:52:45,376 - INFO - Batch 6301, Running Avg Loss: 5.28495
2025-03-09 07:52:45,392 - INFO - Batch 6300 finished
2025-03-09 07:52:45,392 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:53:02,869 - INFO - Batch 6326, Running Avg Loss: 5.28245
2025-03-09 07:53:20,374 - INFO - Batch 6351, Running Avg Loss: 5.28008
2025-03-09 07:53:37,858 - INFO - Batch 6376, Running Avg Loss: 5.27763
2025-03-09 07:53:55,308 - INFO - Batch 6401, Running Avg Loss: 5.27511
2025-03-09 07:53:55,328 - INFO - Batch 6400 finished
2025-03-09 07:53:55,328 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:54:12,766 - INFO - Batch 6426, Running Avg Loss: 5.27255
2025-03-09 07:54:30,230 - INFO - Batch 6451, Running Avg Loss: 5.27013
2025-03-09 07:54:47,752 - INFO - Batch 6476, Running Avg Loss: 5.26785
2025-03-09 07:55:05,257 - INFO - Batch 6501, Running Avg Loss: 5.26529
2025-03-09 07:55:05,273 - INFO - Batch 6500 finished
2025-03-09 07:55:05,273 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:55:22,912 - INFO - Batch 6526, Running Avg Loss: 5.26304
2025-03-09 07:55:40,431 - INFO - Batch 6551, Running Avg Loss: 5.26041
2025-03-09 07:55:57,994 - INFO - Batch 6576, Running Avg Loss: 5.25799
2025-03-09 07:56:15,483 - INFO - Batch 6601, Running Avg Loss: 5.25563
2025-03-09 07:56:15,498 - INFO - Batch 6600 finished
2025-03-09 07:56:15,499 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:56:32,919 - INFO - Batch 6626, Running Avg Loss: 5.25311
2025-03-09 07:56:50,364 - INFO - Batch 6651, Running Avg Loss: 5.25075
2025-03-09 07:57:08,038 - INFO - Batch 6676, Running Avg Loss: 5.24824
2025-03-09 07:57:25,550 - INFO - Batch 6701, Running Avg Loss: 5.24603
2025-03-09 07:57:25,567 - INFO - Batch 6700 finished
2025-03-09 07:57:25,567 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:57:42,983 - INFO - Batch 6726, Running Avg Loss: 5.24389
2025-03-09 07:58:00,496 - INFO - Batch 6751, Running Avg Loss: 5.24192
2025-03-09 07:58:18,021 - INFO - Batch 6776, Running Avg Loss: 5.23959
2025-03-09 07:58:35,634 - INFO - Batch 6801, Running Avg Loss: 5.23750
2025-03-09 07:58:35,652 - INFO - Batch 6800 finished
2025-03-09 07:58:35,652 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 07:58:53,296 - INFO - Batch 6826, Running Avg Loss: 5.23537
2025-03-09 07:59:10,787 - INFO - Batch 6851, Running Avg Loss: 5.23319
2025-03-09 07:59:28,299 - INFO - Batch 6876, Running Avg Loss: 5.23113
2025-03-09 07:59:45,760 - INFO - Batch 6901, Running Avg Loss: 5.22924
2025-03-09 07:59:45,775 - INFO - Batch 6900 finished
2025-03-09 07:59:45,775 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:00:03,293 - INFO - Batch 6926, Running Avg Loss: 5.22696
2025-03-09 08:00:20,615 - INFO - Batch 6951, Running Avg Loss: 5.22502
2025-03-09 08:00:37,811 - INFO - Batch 6976, Running Avg Loss: 5.22307
2025-03-09 08:00:54,977 - INFO - Batch 7001, Running Avg Loss: 5.22078
2025-03-09 08:00:54,989 - INFO - Batch 7000 finished
2025-03-09 08:00:54,990 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:01:12,152 - INFO - Batch 7026, Running Avg Loss: 5.21870
2025-03-09 08:01:29,488 - INFO - Batch 7051, Running Avg Loss: 5.21675
2025-03-09 08:01:46,823 - INFO - Batch 7076, Running Avg Loss: 5.21463
2025-03-09 08:02:04,357 - INFO - Batch 7101, Running Avg Loss: 5.21225
2025-03-09 08:02:04,374 - INFO - Batch 7100 finished
2025-03-09 08:02:04,374 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:02:21,629 - INFO - Batch 7126, Running Avg Loss: 5.21018
2025-03-09 08:02:39,070 - INFO - Batch 7151, Running Avg Loss: 5.20816
2025-03-09 08:02:56,485 - INFO - Batch 7176, Running Avg Loss: 5.20583
2025-03-09 08:03:13,921 - INFO - Batch 7201, Running Avg Loss: 5.20382
2025-03-09 08:03:13,936 - INFO - Batch 7200 finished
2025-03-09 08:03:13,937 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:03:31,420 - INFO - Batch 7226, Running Avg Loss: 5.20160
2025-03-09 08:03:49,018 - INFO - Batch 7251, Running Avg Loss: 5.19986
2025-03-09 08:04:06,519 - INFO - Batch 7276, Running Avg Loss: 5.19803
2025-03-09 08:04:23,982 - INFO - Batch 7301, Running Avg Loss: 5.19611
2025-03-09 08:04:24,001 - INFO - Batch 7300 finished
2025-03-09 08:04:24,002 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:04:41,424 - INFO - Batch 7326, Running Avg Loss: 5.19410
2025-03-09 08:04:58,839 - INFO - Batch 7351, Running Avg Loss: 5.19188
2025-03-09 08:05:16,426 - INFO - Batch 7376, Running Avg Loss: 5.19022
2025-03-09 08:05:33,881 - INFO - Batch 7401, Running Avg Loss: 5.18849
2025-03-09 08:05:33,897 - INFO - Batch 7400 finished
2025-03-09 08:05:33,897 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:05:51,306 - INFO - Batch 7426, Running Avg Loss: 5.18676
2025-03-09 08:06:08,775 - INFO - Batch 7451, Running Avg Loss: 5.18472
2025-03-09 08:06:26,197 - INFO - Batch 7476, Running Avg Loss: 5.18286
2025-03-09 08:06:43,647 - INFO - Batch 7501, Running Avg Loss: 5.18111
2025-03-09 08:06:43,661 - INFO - 
GPU Memory Stats at step 7500:
2025-03-09 08:06:43,662 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 08:06:43,662 - INFO - GPU Memory reserved: 16564.00 MB
2025-03-09 08:06:43,662 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 08:06:43,662 - INFO - learning rate: 0.00000009
2025-03-09 08:06:43,662 - INFO - Ep 1 (Step 007500): Avg loss 5.181 | 30724096 tokens seen
2025-03-09 08:06:43,662 - INFO - optimizer lr: 0.00000009
2025-03-09 08:06:43,662 - INFO - scheduler lr: 0.00000009
2025-03-09 08:06:43,662 - INFO - Selected prompt: Bees are vital for pollination. You can buy leafcutter bee houses to attract 
2025-03-09 08:06:43,662 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:06:43,663 - INFO - random_topk: 1
2025-03-09 08:06:43,663 - INFO - random_temperature: 0.8225689418952165
2025-03-09 08:06:43,663 - INFO - global step 7500 , batch_idx 7500 => generating text
2025-03-09 08:06:43,663 - INFO - Generating on device cuda
2025-03-09 08:07:17,563 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:07:17,563 - INFO - Bees are vital for pollination. You can buy leafcutter bee houses to attract 1000, and you can use a special tool called "A" to help you understand what it means.

In this unit, we will explore the concept of "the term," which is a type of study that allows us to understand how different types of information are and how to use them. We will also learn about some of the most important aspects of this field.

Imagine you have a lemonade stand in your favorite video game. You want to know how to use your computer to make you feel better. You might need to know what you're doing, right? That's where the internet comes in! They use a computer to help you understand what's happening in your computer.

Now, let's talk about what a computer is. Imagine you're playing a game with your favorite video game. You want to know how to use your computer to make sure everything is safe and enjoyable. That's where the internet comes in!

Let's say you're a lemonade stand, and you need to find a computer. You want to use a computer to help you understand what you need. For example, you might need to use a computer to help you understand what you need. This is where the internet comes in.

Here's
2025-03-09 08:07:17,563 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:07:45,108 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_7500_steps_avg_loss_5.18111_optimizer_lr_0.00000009.pth
2025-03-09 08:07:45,293 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 08:07:45,293 - INFO - Batch 7500 finished
2025-03-09 08:07:45,293 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:08:02,787 - INFO - Batch 7526, Running Avg Loss: 5.17931
2025-03-09 08:08:20,258 - INFO - Batch 7551, Running Avg Loss: 5.17760
2025-03-09 08:08:37,703 - INFO - Batch 7576, Running Avg Loss: 5.17567
2025-03-09 08:08:55,174 - INFO - Batch 7601, Running Avg Loss: 5.17394
2025-03-09 08:08:55,193 - INFO - Batch 7600 finished
2025-03-09 08:08:55,193 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:09:12,607 - INFO - Batch 7626, Running Avg Loss: 5.17229
2025-03-09 08:09:29,971 - INFO - Batch 7651, Running Avg Loss: 5.17071
2025-03-09 08:09:47,324 - INFO - Batch 7676, Running Avg Loss: 5.16863
2025-03-09 08:10:04,774 - INFO - Batch 7701, Running Avg Loss: 5.16651
2025-03-09 08:10:04,793 - INFO - Batch 7700 finished
2025-03-09 08:10:04,793 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:10:22,238 - INFO - Batch 7726, Running Avg Loss: 5.16478
2025-03-09 08:10:39,706 - INFO - Batch 7751, Running Avg Loss: 5.16319
2025-03-09 08:10:57,186 - INFO - Batch 7776, Running Avg Loss: 5.16146
2025-03-09 08:11:14,567 - INFO - Batch 7801, Running Avg Loss: 5.15972
2025-03-09 08:11:14,580 - INFO - Batch 7800 finished
2025-03-09 08:11:14,580 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:11:31,894 - INFO - Batch 7826, Running Avg Loss: 5.15793
2025-03-09 08:11:49,344 - INFO - Batch 7851, Running Avg Loss: 5.15625
2025-03-09 08:12:06,770 - INFO - Batch 7876, Running Avg Loss: 5.15430
2025-03-09 08:12:24,235 - INFO - Batch 7901, Running Avg Loss: 5.15226
2025-03-09 08:12:24,250 - INFO - Batch 7900 finished
2025-03-09 08:12:24,251 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:12:41,800 - INFO - Batch 7926, Running Avg Loss: 5.15056
2025-03-09 08:12:59,293 - INFO - Batch 7951, Running Avg Loss: 5.14876
2025-03-09 08:13:16,672 - INFO - Batch 7976, Running Avg Loss: 5.14725
2025-03-09 08:13:34,027 - INFO - Batch 8001, Running Avg Loss: 5.14587
2025-03-09 08:13:34,043 - INFO - Batch 8000 finished
2025-03-09 08:13:34,043 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:13:51,389 - INFO - Batch 8026, Running Avg Loss: 5.14441
2025-03-09 08:14:08,872 - INFO - Batch 8051, Running Avg Loss: 5.14304
2025-03-09 08:14:26,270 - INFO - Batch 8076, Running Avg Loss: 5.14126
2025-03-09 08:14:43,816 - INFO - Batch 8101, Running Avg Loss: 5.13959
2025-03-09 08:14:43,833 - INFO - Batch 8100 finished
2025-03-09 08:14:43,833 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:15:01,270 - INFO - Batch 8126, Running Avg Loss: 5.13838
2025-03-09 08:15:18,868 - INFO - Batch 8151, Running Avg Loss: 5.13691
2025-03-09 08:15:36,479 - INFO - Batch 8176, Running Avg Loss: 5.13529
2025-03-09 08:15:54,067 - INFO - Batch 8201, Running Avg Loss: 5.13368
2025-03-09 08:15:54,085 - INFO - Batch 8200 finished
2025-03-09 08:15:54,086 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:16:11,606 - INFO - Batch 8226, Running Avg Loss: 5.13226
2025-03-09 08:16:29,110 - INFO - Batch 8251, Running Avg Loss: 5.13081
2025-03-09 08:16:46,576 - INFO - Batch 8276, Running Avg Loss: 5.12910
2025-03-09 08:17:04,093 - INFO - Batch 8301, Running Avg Loss: 5.12789
2025-03-09 08:17:04,107 - INFO - Batch 8300 finished
2025-03-09 08:17:04,108 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:17:21,573 - INFO - Batch 8326, Running Avg Loss: 5.12644
2025-03-09 08:17:39,059 - INFO - Batch 8351, Running Avg Loss: 5.12473
2025-03-09 08:17:56,691 - INFO - Batch 8376, Running Avg Loss: 5.12320
2025-03-09 08:18:14,168 - INFO - Batch 8401, Running Avg Loss: 5.12151
2025-03-09 08:18:14,185 - INFO - Batch 8400 finished
2025-03-09 08:18:14,185 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:18:31,638 - INFO - Batch 8426, Running Avg Loss: 5.12000
2025-03-09 08:18:49,172 - INFO - Batch 8451, Running Avg Loss: 5.11844
2025-03-09 08:19:06,537 - INFO - Batch 8476, Running Avg Loss: 5.11706
2025-03-09 08:19:23,903 - INFO - Batch 8501, Running Avg Loss: 5.11572
2025-03-09 08:19:23,919 - INFO - Batch 8500 finished
2025-03-09 08:19:23,919 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:19:41,489 - INFO - Batch 8526, Running Avg Loss: 5.11442
2025-03-09 08:19:58,976 - INFO - Batch 8551, Running Avg Loss: 5.11319
2025-03-09 08:20:16,466 - INFO - Batch 8576, Running Avg Loss: 5.11177
2025-03-09 08:20:34,057 - INFO - Batch 8601, Running Avg Loss: 5.11043
2025-03-09 08:20:34,073 - INFO - Batch 8600 finished
2025-03-09 08:20:34,073 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:20:51,473 - INFO - Batch 8626, Running Avg Loss: 5.10893
2025-03-09 08:21:09,114 - INFO - Batch 8651, Running Avg Loss: 5.10744
2025-03-09 08:21:26,689 - INFO - Batch 8676, Running Avg Loss: 5.10610
2025-03-09 08:21:44,225 - INFO - Batch 8701, Running Avg Loss: 5.10488
2025-03-09 08:21:44,241 - INFO - Batch 8700 finished
2025-03-09 08:21:44,241 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:22:01,821 - INFO - Batch 8726, Running Avg Loss: 5.10359
2025-03-09 08:22:19,288 - INFO - Batch 8751, Running Avg Loss: 5.10217
2025-03-09 08:22:36,717 - INFO - Batch 8776, Running Avg Loss: 5.10081
2025-03-09 08:22:54,169 - INFO - Batch 8801, Running Avg Loss: 5.09938
2025-03-09 08:22:54,186 - INFO - Batch 8800 finished
2025-03-09 08:22:54,187 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:23:11,514 - INFO - Batch 8826, Running Avg Loss: 5.09826
2025-03-09 08:23:28,906 - INFO - Batch 8851, Running Avg Loss: 5.09697
2025-03-09 08:23:46,270 - INFO - Batch 8876, Running Avg Loss: 5.09552
2025-03-09 08:24:03,822 - INFO - Batch 8901, Running Avg Loss: 5.09418
2025-03-09 08:24:03,838 - INFO - Batch 8900 finished
2025-03-09 08:24:03,839 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:24:21,599 - INFO - Batch 8926, Running Avg Loss: 5.09303
2025-03-09 08:24:39,060 - INFO - Batch 8951, Running Avg Loss: 5.09170
2025-03-09 08:24:56,485 - INFO - Batch 8976, Running Avg Loss: 5.09033
2025-03-09 08:25:13,938 - INFO - Batch 9001, Running Avg Loss: 5.08903
2025-03-09 08:25:13,957 - INFO - Batch 9000 finished
2025-03-09 08:25:13,958 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:25:31,490 - INFO - Batch 9026, Running Avg Loss: 5.08787
2025-03-09 08:25:49,035 - INFO - Batch 9051, Running Avg Loss: 5.08664
2025-03-09 08:26:06,636 - INFO - Batch 9076, Running Avg Loss: 5.08538
2025-03-09 08:26:24,214 - INFO - Batch 9101, Running Avg Loss: 5.08418
2025-03-09 08:26:24,233 - INFO - Batch 9100 finished
2025-03-09 08:26:24,233 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:26:41,721 - INFO - Batch 9126, Running Avg Loss: 5.08275
2025-03-09 08:26:59,142 - INFO - Batch 9151, Running Avg Loss: 5.08156
2025-03-09 08:27:16,547 - INFO - Batch 9176, Running Avg Loss: 5.08034
2025-03-09 08:27:33,915 - INFO - Batch 9201, Running Avg Loss: 5.07911
2025-03-09 08:27:33,929 - INFO - Batch 9200 finished
2025-03-09 08:27:33,929 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:27:51,599 - INFO - Batch 9226, Running Avg Loss: 5.07795
2025-03-09 08:28:09,167 - INFO - Batch 9251, Running Avg Loss: 5.07668
2025-03-09 08:28:26,706 - INFO - Batch 9276, Running Avg Loss: 5.07535
2025-03-09 08:28:44,103 - INFO - Batch 9301, Running Avg Loss: 5.07421
2025-03-09 08:28:44,120 - INFO - Batch 9300 finished
2025-03-09 08:28:44,120 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:29:01,667 - INFO - Batch 9326, Running Avg Loss: 5.07317
2025-03-09 08:29:19,329 - INFO - Batch 9351, Running Avg Loss: 5.07206
2025-03-09 08:29:36,925 - INFO - Batch 9376, Running Avg Loss: 5.07091
2025-03-09 08:29:54,425 - INFO - Batch 9401, Running Avg Loss: 5.06943
2025-03-09 08:29:54,441 - INFO - Batch 9400 finished
2025-03-09 08:29:54,442 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:30:11,979 - INFO - Batch 9426, Running Avg Loss: 5.06835
2025-03-09 08:30:29,625 - INFO - Batch 9451, Running Avg Loss: 5.06730
2025-03-09 08:30:47,164 - INFO - Batch 9476, Running Avg Loss: 5.06613
2025-03-09 08:31:04,464 - INFO - Batch 9501, Running Avg Loss: 5.06461
2025-03-09 08:31:04,477 - INFO - Batch 9500 finished
2025-03-09 08:31:04,477 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:31:21,797 - INFO - Batch 9526, Running Avg Loss: 5.06363
2025-03-09 08:31:39,396 - INFO - Batch 9551, Running Avg Loss: 5.06244
2025-03-09 08:31:57,009 - INFO - Batch 9576, Running Avg Loss: 5.06146
2025-03-09 08:32:14,601 - INFO - Batch 9601, Running Avg Loss: 5.06030
2025-03-09 08:32:14,620 - INFO - Batch 9600 finished
2025-03-09 08:32:14,621 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:32:32,162 - INFO - Batch 9626, Running Avg Loss: 5.05904
2025-03-09 08:32:49,892 - INFO - Batch 9651, Running Avg Loss: 5.05799
2025-03-09 08:33:07,380 - INFO - Batch 9676, Running Avg Loss: 5.05673
2025-03-09 08:33:24,833 - INFO - Batch 9701, Running Avg Loss: 5.05593
2025-03-09 08:33:24,852 - INFO - Batch 9700 finished
2025-03-09 08:33:24,852 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:33:42,192 - INFO - Batch 9726, Running Avg Loss: 5.05507
2025-03-09 08:33:59,433 - INFO - Batch 9751, Running Avg Loss: 5.05401
2025-03-09 08:34:16,930 - INFO - Batch 9776, Running Avg Loss: 5.05290
2025-03-09 08:34:34,416 - INFO - Batch 9801, Running Avg Loss: 5.05165
2025-03-09 08:34:34,433 - INFO - Batch 9800 finished
2025-03-09 08:34:34,434 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:34:52,003 - INFO - Batch 9826, Running Avg Loss: 5.05069
2025-03-09 08:35:09,639 - INFO - Batch 9851, Running Avg Loss: 5.04920
2025-03-09 08:35:27,125 - INFO - Batch 9876, Running Avg Loss: 5.04800
2025-03-09 08:35:44,550 - INFO - Batch 9901, Running Avg Loss: 5.04692
2025-03-09 08:35:44,570 - INFO - Batch 9900 finished
2025-03-09 08:35:44,570 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:36:01,907 - INFO - Batch 9926, Running Avg Loss: 5.04595
2025-03-09 08:36:19,299 - INFO - Batch 9951, Running Avg Loss: 5.04500
2025-03-09 08:36:36,724 - INFO - Batch 9976, Running Avg Loss: 5.04381
2025-03-09 08:36:54,205 - INFO - Batch 10001, Running Avg Loss: 5.04277
2025-03-09 08:36:54,221 - INFO - 
GPU Memory Stats at step 10000:
2025-03-09 08:36:54,221 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 08:36:54,222 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 08:36:54,222 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 08:36:54,222 - INFO - learning rate: 0.00000009
2025-03-09 08:36:54,222 - INFO - Ep 1 (Step 010000): Avg loss 5.043 | 40964096 tokens seen
2025-03-09 08:36:54,222 - INFO - optimizer lr: 0.00000009
2025-03-09 08:36:54,222 - INFO - scheduler lr: 0.00000009
2025-03-09 08:36:54,222 - INFO - Selected prompt: Introduction: The Art of Crafting Vegan Sandwich Delights Sandwiches occupy a unique space in
2025-03-09 08:36:54,222 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:36:54,222 - INFO - random_topk: 1
2025-03-09 08:36:54,223 - INFO - random_temperature: 0.7949885823596805
2025-03-09 08:36:54,223 - INFO - global step 10000 , batch_idx 10000 => generating text
2025-03-09 08:36:54,223 - INFO - Generating on device cuda
2025-03-09 08:37:27,298 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:37:27,298 - INFO - Introduction: The Art of Crafting Vegan Sandwich Delights Sandwiches occupy a unique space in the world. This chapter will delve into the fascinating world of art, specifically focusing on the art of art and music. We will explore how these techniques can be used to create a unique blend of art and music.

I. Understanding the Basics of the Giraan
A. Historical context
B. Historical context
1. Historical context
2. Historical context
1. Historical context
2. Historical context
1. Early civilizations
2. Early civilizations and civilizations
2. Early civilizations such as the 19th century

II. Historical context and evolution of the Middle East
A. Historical context
1. Early civilizations such as the 19th century
2. Early civilizations such as the 19th century

II. The History of the 19th century

The origins of the Middle East was the first American American and early 19th centuries, which had been passed down through generations. As such, the British Empire began to the 19th century, which led to the British colonial era.

B. The History of the 19th century

A. Early 19th century
2. Early civilizations such as the 19th century


2025-03-09 08:37:27,298 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:37:55,342 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_10000_steps_avg_loss_5.04277_optimizer_lr_0.00000009.pth
2025-03-09 08:37:55,562 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 08:37:55,562 - INFO - Batch 10000 finished
2025-03-09 08:37:55,562 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:38:12,833 - INFO - Batch 10026, Running Avg Loss: 5.04172
2025-03-09 08:38:30,195 - INFO - Batch 10051, Running Avg Loss: 5.04067
2025-03-09 08:38:47,732 - INFO - Batch 10076, Running Avg Loss: 5.03972
2025-03-09 08:39:05,324 - INFO - Batch 10101, Running Avg Loss: 5.03870
2025-03-09 08:39:05,339 - INFO - Batch 10100 finished
2025-03-09 08:39:05,340 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:39:23,002 - INFO - Batch 10126, Running Avg Loss: 5.03734
2025-03-09 08:39:40,493 - INFO - Batch 10151, Running Avg Loss: 5.03614
2025-03-09 08:39:58,025 - INFO - Batch 10176, Running Avg Loss: 5.03524
2025-03-09 08:40:15,615 - INFO - Batch 10201, Running Avg Loss: 5.03417
2025-03-09 08:40:15,633 - INFO - Batch 10200 finished
2025-03-09 08:40:15,634 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:40:33,101 - INFO - Batch 10226, Running Avg Loss: 5.03292
2025-03-09 08:40:50,214 - INFO - Batch 10251, Running Avg Loss: 5.03186
2025-03-09 08:41:07,323 - INFO - Batch 10276, Running Avg Loss: 5.03086
2025-03-09 08:41:24,747 - INFO - Batch 10301, Running Avg Loss: 5.02976
2025-03-09 08:41:24,763 - INFO - Batch 10300 finished
2025-03-09 08:41:24,764 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:41:42,278 - INFO - Batch 10326, Running Avg Loss: 5.02879
2025-03-09 08:41:59,721 - INFO - Batch 10351, Running Avg Loss: 5.02783
2025-03-09 08:42:17,134 - INFO - Batch 10376, Running Avg Loss: 5.02683
2025-03-09 08:42:34,548 - INFO - Batch 10401, Running Avg Loss: 5.02582
2025-03-09 08:42:34,564 - INFO - Batch 10400 finished
2025-03-09 08:42:34,564 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:42:52,013 - INFO - Batch 10426, Running Avg Loss: 5.02487
2025-03-09 08:43:09,556 - INFO - Batch 10451, Running Avg Loss: 5.02392
2025-03-09 08:43:27,182 - INFO - Batch 10476, Running Avg Loss: 5.02303
2025-03-09 08:43:44,708 - INFO - Batch 10501, Running Avg Loss: 5.02196
2025-03-09 08:43:44,725 - INFO - Batch 10500 finished
2025-03-09 08:43:44,726 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:44:02,278 - INFO - Batch 10526, Running Avg Loss: 5.02113
2025-03-09 08:44:19,627 - INFO - Batch 10551, Running Avg Loss: 5.01992
2025-03-09 08:44:37,037 - INFO - Batch 10576, Running Avg Loss: 5.01893
2025-03-09 08:44:54,542 - INFO - Batch 10601, Running Avg Loss: 5.01780
2025-03-09 08:44:54,559 - INFO - Batch 10600 finished
2025-03-09 08:44:54,559 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:45:12,049 - INFO - Batch 10626, Running Avg Loss: 5.01680
2025-03-09 08:45:29,631 - INFO - Batch 10651, Running Avg Loss: 5.01582
2025-03-09 08:45:47,191 - INFO - Batch 10676, Running Avg Loss: 5.01510
2025-03-09 08:46:04,709 - INFO - Batch 10701, Running Avg Loss: 5.01421
2025-03-09 08:46:04,725 - INFO - Batch 10700 finished
2025-03-09 08:46:04,725 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:46:22,207 - INFO - Batch 10726, Running Avg Loss: 5.01333
2025-03-09 08:46:39,642 - INFO - Batch 10751, Running Avg Loss: 5.01238
2025-03-09 08:46:57,013 - INFO - Batch 10776, Running Avg Loss: 5.01141
2025-03-09 08:47:14,624 - INFO - Batch 10801, Running Avg Loss: 5.01045
2025-03-09 08:47:14,642 - INFO - Batch 10800 finished
2025-03-09 08:47:14,642 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:47:32,227 - INFO - Batch 10826, Running Avg Loss: 5.00954
2025-03-09 08:47:49,885 - INFO - Batch 10851, Running Avg Loss: 5.00852
2025-03-09 08:48:07,523 - INFO - Batch 10876, Running Avg Loss: 5.00760
2025-03-09 08:48:25,091 - INFO - Batch 10901, Running Avg Loss: 5.00674
2025-03-09 08:48:25,106 - INFO - Batch 10900 finished
2025-03-09 08:48:25,107 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:48:42,635 - INFO - Batch 10926, Running Avg Loss: 5.00586
2025-03-09 08:49:00,064 - INFO - Batch 10951, Running Avg Loss: 5.00502
2025-03-09 08:49:17,727 - INFO - Batch 10976, Running Avg Loss: 5.00393
2025-03-09 08:49:35,324 - INFO - Batch 11001, Running Avg Loss: 5.00302
2025-03-09 08:49:35,341 - INFO - Batch 11000 finished
2025-03-09 08:49:35,341 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:49:52,811 - INFO - Batch 11026, Running Avg Loss: 5.00234
2025-03-09 08:50:10,296 - INFO - Batch 11051, Running Avg Loss: 5.00142
2025-03-09 08:50:27,729 - INFO - Batch 11076, Running Avg Loss: 5.00052
2025-03-09 08:50:45,047 - INFO - Batch 11101, Running Avg Loss: 4.99959
2025-03-09 08:50:45,059 - INFO - Batch 11100 finished
2025-03-09 08:50:45,060 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:51:02,236 - INFO - Batch 11126, Running Avg Loss: 4.99871
2025-03-09 08:51:19,567 - INFO - Batch 11151, Running Avg Loss: 4.99796
2025-03-09 08:51:37,020 - INFO - Batch 11176, Running Avg Loss: 4.99714
2025-03-09 08:51:54,493 - INFO - Batch 11201, Running Avg Loss: 4.99625
2025-03-09 08:51:54,511 - INFO - Batch 11200 finished
2025-03-09 08:51:54,511 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:52:11,918 - INFO - Batch 11226, Running Avg Loss: 4.99554
2025-03-09 08:52:29,669 - INFO - Batch 11251, Running Avg Loss: 4.99466
2025-03-09 08:52:47,161 - INFO - Batch 11276, Running Avg Loss: 4.99398
2025-03-09 08:53:04,675 - INFO - Batch 11301, Running Avg Loss: 4.99332
2025-03-09 08:53:04,690 - INFO - Batch 11300 finished
2025-03-09 08:53:04,690 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:53:22,189 - INFO - Batch 11326, Running Avg Loss: 4.99249
2025-03-09 08:53:39,823 - INFO - Batch 11351, Running Avg Loss: 4.99147
2025-03-09 08:53:57,167 - INFO - Batch 11376, Running Avg Loss: 4.99053
2025-03-09 08:54:14,525 - INFO - Batch 11401, Running Avg Loss: 4.98981
2025-03-09 08:54:14,542 - INFO - Batch 11400 finished
2025-03-09 08:54:14,543 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:54:32,010 - INFO - Batch 11426, Running Avg Loss: 4.98883
2025-03-09 08:54:49,392 - INFO - Batch 11451, Running Avg Loss: 4.98806
2025-03-09 08:55:06,545 - INFO - Batch 11476, Running Avg Loss: 4.98728
2025-03-09 08:55:23,700 - INFO - Batch 11501, Running Avg Loss: 4.98636
2025-03-09 08:55:23,721 - INFO - Batch 11500 finished
2025-03-09 08:55:23,722 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:55:41,104 - INFO - Batch 11526, Running Avg Loss: 4.98539
2025-03-09 08:55:58,469 - INFO - Batch 11551, Running Avg Loss: 4.98461
2025-03-09 08:56:15,870 - INFO - Batch 11576, Running Avg Loss: 4.98364
2025-03-09 08:56:33,287 - INFO - Batch 11601, Running Avg Loss: 4.98288
2025-03-09 08:56:33,302 - INFO - Batch 11600 finished
2025-03-09 08:56:33,302 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:56:50,889 - INFO - Batch 11626, Running Avg Loss: 4.98214
2025-03-09 08:57:08,255 - INFO - Batch 11651, Running Avg Loss: 4.98135
2025-03-09 08:57:25,572 - INFO - Batch 11676, Running Avg Loss: 4.98054
2025-03-09 08:57:42,917 - INFO - Batch 11701, Running Avg Loss: 4.97977
2025-03-09 08:57:42,936 - INFO - Batch 11700 finished
2025-03-09 08:57:42,936 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:58:00,296 - INFO - Batch 11726, Running Avg Loss: 4.97890
2025-03-09 08:58:17,797 - INFO - Batch 11751, Running Avg Loss: 4.97797
2025-03-09 08:58:35,403 - INFO - Batch 11776, Running Avg Loss: 4.97737
2025-03-09 08:58:52,882 - INFO - Batch 11801, Running Avg Loss: 4.97654
2025-03-09 08:58:52,898 - INFO - Batch 11800 finished
2025-03-09 08:58:52,898 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 08:59:10,362 - INFO - Batch 11826, Running Avg Loss: 4.97571
2025-03-09 08:59:27,913 - INFO - Batch 11851, Running Avg Loss: 4.97489
2025-03-09 08:59:45,486 - INFO - Batch 11876, Running Avg Loss: 4.97414
2025-03-09 09:00:02,882 - INFO - Batch 11901, Running Avg Loss: 4.97350
2025-03-09 09:00:02,899 - INFO - Batch 11900 finished
2025-03-09 09:00:02,900 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:00:20,412 - INFO - Batch 11926, Running Avg Loss: 4.97279
2025-03-09 09:00:37,799 - INFO - Batch 11951, Running Avg Loss: 4.97199
2025-03-09 09:00:55,037 - INFO - Batch 11976, Running Avg Loss: 4.97108
2025-03-09 09:01:12,226 - INFO - Batch 12001, Running Avg Loss: 4.97028
2025-03-09 09:01:12,241 - INFO - Batch 12000 finished
2025-03-09 09:01:12,241 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:01:29,455 - INFO - Batch 12026, Running Avg Loss: 4.96945
2025-03-09 09:01:46,885 - INFO - Batch 12051, Running Avg Loss: 4.96873
2025-03-09 09:02:04,322 - INFO - Batch 12076, Running Avg Loss: 4.96822
2025-03-09 09:02:21,771 - INFO - Batch 12101, Running Avg Loss: 4.96750
2025-03-09 09:02:21,787 - INFO - Batch 12100 finished
2025-03-09 09:02:21,787 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:02:39,252 - INFO - Batch 12126, Running Avg Loss: 4.96660
2025-03-09 09:02:56,738 - INFO - Batch 12151, Running Avg Loss: 4.96570
2025-03-09 09:03:14,177 - INFO - Batch 12176, Running Avg Loss: 4.96498
2025-03-09 09:03:31,828 - INFO - Batch 12201, Running Avg Loss: 4.96418
2025-03-09 09:03:31,845 - INFO - Batch 12200 finished
2025-03-09 09:03:31,845 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:03:49,297 - INFO - Batch 12226, Running Avg Loss: 4.96349
2025-03-09 09:04:06,702 - INFO - Batch 12251, Running Avg Loss: 4.96287
2025-03-09 09:04:24,102 - INFO - Batch 12276, Running Avg Loss: 4.96227
2025-03-09 09:04:41,523 - INFO - Batch 12301, Running Avg Loss: 4.96155
2025-03-09 09:04:41,540 - INFO - Batch 12300 finished
2025-03-09 09:04:41,540 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:04:59,008 - INFO - Batch 12326, Running Avg Loss: 4.96086
2025-03-09 09:05:16,391 - INFO - Batch 12351, Running Avg Loss: 4.96023
2025-03-09 09:05:33,913 - INFO - Batch 12376, Running Avg Loss: 4.95953
2025-03-09 09:05:51,247 - INFO - Batch 12401, Running Avg Loss: 4.95901
2025-03-09 09:05:51,266 - INFO - Batch 12400 finished
2025-03-09 09:05:51,266 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:06:08,683 - INFO - Batch 12426, Running Avg Loss: 4.95822
2025-03-09 09:06:26,278 - INFO - Batch 12451, Running Avg Loss: 4.95757
2025-03-09 09:06:43,908 - INFO - Batch 12476, Running Avg Loss: 4.95675
2025-03-09 09:07:01,378 - INFO - Batch 12501, Running Avg Loss: 4.95619
2025-03-09 09:07:01,396 - INFO - 
GPU Memory Stats at step 12500:
2025-03-09 09:07:01,397 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 09:07:01,397 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 09:07:01,397 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 09:07:01,397 - INFO - learning rate: 0.00000008
2025-03-09 09:07:01,397 - INFO - Ep 1 (Step 012500): Avg loss 4.956 | 51204096 tokens seen
2025-03-09 09:07:01,397 - INFO - optimizer lr: 0.00000008
2025-03-09 09:07:01,397 - INFO - scheduler lr: 0.00000008
2025-03-09 09:07:01,398 - INFO - Selected prompt: Once upon a time, in a colorful town called Popville, 
2025-03-09 09:07:01,398 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:07:01,398 - INFO - random_topk: 1
2025-03-09 09:07:01,398 - INFO - random_temperature: 0.8598255093392344
2025-03-09 09:07:01,398 - INFO - global step 12500 , batch_idx 12500 => generating text
2025-03-09 09:07:01,398 - INFO - Generating on device cuda
2025-03-09 09:07:34,409 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:07:34,409 - INFO - Once upon a time, in a colorful town called Popville, 1914, there lived many different types of animals who lived in the world. One day, while walking through the river, they found a special place called "The Gira," which was known for its unique properties.

One day, while walking through the river, they found a special place called "The Gira," which was known for its vibrant colors. The people of the people were made up of the water, which was called "to," which was known as the "The Gira" in the 19th century.

One day, while walking through the river, they found a special place called "The Gira" in the 18th century. This made them feel like a big, beautiful, and beautiful!

One day, while walking through the river, they found a special place called "The Gira" in the 18th century. He was known for his unique customs and traditions, but he had a special place called "The Gira".

One day, while walking through the river, he noticed something unusual - the water was made of a special place called "The Gira" and "The Gira" in the 18th century. He was known
2025-03-09 09:07:34,409 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:08:01,877 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_12500_steps_avg_loss_4.95619_optimizer_lr_0.00000008.pth
2025-03-09 09:08:02,074 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 09:08:02,074 - INFO - Batch 12500 finished
2025-03-09 09:08:02,074 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:08:19,633 - INFO - Batch 12526, Running Avg Loss: 4.95553
2025-03-09 09:08:37,262 - INFO - Batch 12551, Running Avg Loss: 4.95492
2025-03-09 09:08:54,747 - INFO - Batch 12576, Running Avg Loss: 4.95425
2025-03-09 09:09:12,263 - INFO - Batch 12601, Running Avg Loss: 4.95353
2025-03-09 09:09:12,277 - INFO - Batch 12600 finished
2025-03-09 09:09:12,278 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:09:29,783 - INFO - Batch 12626, Running Avg Loss: 4.95297
2025-03-09 09:09:47,301 - INFO - Batch 12651, Running Avg Loss: 4.95241
2025-03-09 09:10:04,818 - INFO - Batch 12676, Running Avg Loss: 4.95177
2025-03-09 09:10:22,329 - INFO - Batch 12701, Running Avg Loss: 4.95119
2025-03-09 09:10:22,345 - INFO - Batch 12700 finished
2025-03-09 09:10:22,345 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:10:39,809 - INFO - Batch 12726, Running Avg Loss: 4.95052
2025-03-09 09:10:57,036 - INFO - Batch 12751, Running Avg Loss: 4.94994
2025-03-09 09:11:14,345 - INFO - Batch 12776, Running Avg Loss: 4.94929
2025-03-09 09:11:31,812 - INFO - Batch 12801, Running Avg Loss: 4.94866
2025-03-09 09:11:31,827 - INFO - Batch 12800 finished
2025-03-09 09:11:31,828 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:11:49,333 - INFO - Batch 12826, Running Avg Loss: 4.94797
2025-03-09 09:12:06,957 - INFO - Batch 12851, Running Avg Loss: 4.94728
2025-03-09 09:12:24,519 - INFO - Batch 12876, Running Avg Loss: 4.94673
2025-03-09 09:12:41,988 - INFO - Batch 12901, Running Avg Loss: 4.94592
2025-03-09 09:12:42,004 - INFO - Batch 12900 finished
2025-03-09 09:12:42,004 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:12:59,437 - INFO - Batch 12926, Running Avg Loss: 4.94545
2025-03-09 09:13:16,901 - INFO - Batch 12951, Running Avg Loss: 4.94488
2025-03-09 09:13:34,405 - INFO - Batch 12976, Running Avg Loss: 4.94419
2025-03-09 09:13:52,027 - INFO - Batch 13001, Running Avg Loss: 4.94360
2025-03-09 09:13:52,043 - INFO - Batch 13000 finished
2025-03-09 09:13:52,043 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:14:09,561 - INFO - Batch 13026, Running Avg Loss: 4.94306
2025-03-09 09:14:27,081 - INFO - Batch 13051, Running Avg Loss: 4.94220
2025-03-09 09:14:44,626 - INFO - Batch 13076, Running Avg Loss: 4.94182
2025-03-09 09:15:02,121 - INFO - Batch 13101, Running Avg Loss: 4.94113
2025-03-09 09:15:02,137 - INFO - Batch 13100 finished
2025-03-09 09:15:02,138 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:15:19,770 - INFO - Batch 13126, Running Avg Loss: 4.94062
2025-03-09 09:15:37,263 - INFO - Batch 13151, Running Avg Loss: 4.94008
2025-03-09 09:15:54,789 - INFO - Batch 13176, Running Avg Loss: 4.93937
2025-03-09 09:16:12,318 - INFO - Batch 13201, Running Avg Loss: 4.93880
2025-03-09 09:16:12,335 - INFO - Batch 13200 finished
2025-03-09 09:16:12,335 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:16:29,793 - INFO - Batch 13226, Running Avg Loss: 4.93829
2025-03-09 09:16:47,256 - INFO - Batch 13251, Running Avg Loss: 4.93767
2025-03-09 09:17:04,755 - INFO - Batch 13276, Running Avg Loss: 4.93717
2025-03-09 09:17:22,198 - INFO - Batch 13301, Running Avg Loss: 4.93651
2025-03-09 09:17:22,216 - INFO - Batch 13300 finished
2025-03-09 09:17:22,217 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:17:39,660 - INFO - Batch 13326, Running Avg Loss: 4.93591
2025-03-09 09:17:57,134 - INFO - Batch 13351, Running Avg Loss: 4.93528
2025-03-09 09:18:14,611 - INFO - Batch 13376, Running Avg Loss: 4.93455
2025-03-09 09:18:32,180 - INFO - Batch 13401, Running Avg Loss: 4.93380
2025-03-09 09:18:32,197 - INFO - Batch 13400 finished
2025-03-09 09:18:32,197 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:18:49,866 - INFO - Batch 13426, Running Avg Loss: 4.93329
2025-03-09 09:19:07,328 - INFO - Batch 13451, Running Avg Loss: 4.93269
2025-03-09 09:19:24,822 - INFO - Batch 13476, Running Avg Loss: 4.93210
2025-03-09 09:19:42,254 - INFO - Batch 13501, Running Avg Loss: 4.93148
2025-03-09 09:19:42,269 - INFO - Batch 13500 finished
2025-03-09 09:19:42,270 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:19:59,747 - INFO - Batch 13526, Running Avg Loss: 4.93089
2025-03-09 09:20:17,260 - INFO - Batch 13551, Running Avg Loss: 4.93023
2025-03-09 09:20:34,874 - INFO - Batch 13576, Running Avg Loss: 4.92962
2025-03-09 09:20:52,280 - INFO - Batch 13601, Running Avg Loss: 4.92921
2025-03-09 09:20:52,294 - INFO - Batch 13600 finished
2025-03-09 09:20:52,294 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:21:09,425 - INFO - Batch 13626, Running Avg Loss: 4.92876
2025-03-09 09:21:26,839 - INFO - Batch 13651, Running Avg Loss: 4.92824
2025-03-09 09:21:44,566 - INFO - Batch 13676, Running Avg Loss: 4.92770
2025-03-09 09:22:02,095 - INFO - Batch 13701, Running Avg Loss: 4.92704
2025-03-09 09:22:02,111 - INFO - Batch 13700 finished
2025-03-09 09:22:02,111 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:22:19,583 - INFO - Batch 13726, Running Avg Loss: 4.92636
2025-03-09 09:22:37,087 - INFO - Batch 13751, Running Avg Loss: 4.92582
2025-03-09 09:22:54,614 - INFO - Batch 13776, Running Avg Loss: 4.92532
2025-03-09 09:23:12,169 - INFO - Batch 13801, Running Avg Loss: 4.92474
2025-03-09 09:23:12,185 - INFO - Batch 13800 finished
2025-03-09 09:23:12,186 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:23:29,690 - INFO - Batch 13826, Running Avg Loss: 4.92421
2025-03-09 09:23:47,233 - INFO - Batch 13851, Running Avg Loss: 4.92365
2025-03-09 09:24:04,706 - INFO - Batch 13876, Running Avg Loss: 4.92292
2025-03-09 09:24:22,135 - INFO - Batch 13901, Running Avg Loss: 4.92235
2025-03-09 09:24:22,150 - INFO - Batch 13900 finished
2025-03-09 09:24:22,151 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:24:39,563 - INFO - Batch 13926, Running Avg Loss: 4.92173
2025-03-09 09:24:57,039 - INFO - Batch 13951, Running Avg Loss: 4.92121
2025-03-09 09:25:14,622 - INFO - Batch 13976, Running Avg Loss: 4.92062
2025-03-09 09:25:32,054 - INFO - Batch 14001, Running Avg Loss: 4.92003
2025-03-09 09:25:32,069 - INFO - Batch 14000 finished
2025-03-09 09:25:32,069 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:25:49,506 - INFO - Batch 14026, Running Avg Loss: 4.91937
2025-03-09 09:26:07,001 - INFO - Batch 14051, Running Avg Loss: 4.91884
2025-03-09 09:26:24,482 - INFO - Batch 14076, Running Avg Loss: 4.91835
2025-03-09 09:26:42,037 - INFO - Batch 14101, Running Avg Loss: 4.91790
2025-03-09 09:26:42,053 - INFO - Batch 14100 finished
2025-03-09 09:26:42,054 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:26:59,528 - INFO - Batch 14126, Running Avg Loss: 4.91726
2025-03-09 09:27:16,995 - INFO - Batch 14151, Running Avg Loss: 4.91663
2025-03-09 09:27:34,437 - INFO - Batch 14176, Running Avg Loss: 4.91611
2025-03-09 09:27:51,882 - INFO - Batch 14201, Running Avg Loss: 4.91556
2025-03-09 09:27:51,898 - INFO - Batch 14200 finished
2025-03-09 09:27:51,898 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:28:09,350 - INFO - Batch 14226, Running Avg Loss: 4.91492
2025-03-09 09:28:27,014 - INFO - Batch 14251, Running Avg Loss: 4.91448
2025-03-09 09:28:44,448 - INFO - Batch 14276, Running Avg Loss: 4.91388
2025-03-09 09:29:01,869 - INFO - Batch 14301, Running Avg Loss: 4.91343
2025-03-09 09:29:01,884 - INFO - Batch 14300 finished
2025-03-09 09:29:01,884 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:29:19,288 - INFO - Batch 14326, Running Avg Loss: 4.91288
2025-03-09 09:29:36,755 - INFO - Batch 14351, Running Avg Loss: 4.91233
2025-03-09 09:29:54,160 - INFO - Batch 14376, Running Avg Loss: 4.91177
2025-03-09 09:30:11,560 - INFO - Batch 14401, Running Avg Loss: 4.91132
2025-03-09 09:30:11,576 - INFO - Batch 14400 finished
2025-03-09 09:30:11,577 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:30:28,984 - INFO - Batch 14426, Running Avg Loss: 4.91077
2025-03-09 09:30:46,200 - INFO - Batch 14451, Running Avg Loss: 4.91034
2025-03-09 09:31:03,354 - INFO - Batch 14476, Running Avg Loss: 4.90977
2025-03-09 09:31:20,569 - INFO - Batch 14501, Running Avg Loss: 4.90915
2025-03-09 09:31:20,585 - INFO - Batch 14500 finished
2025-03-09 09:31:20,586 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:31:38,103 - INFO - Batch 14526, Running Avg Loss: 4.90868
2025-03-09 09:31:55,717 - INFO - Batch 14551, Running Avg Loss: 4.90811
2025-03-09 09:32:13,158 - INFO - Batch 14576, Running Avg Loss: 4.90746
2025-03-09 09:32:30,684 - INFO - Batch 14601, Running Avg Loss: 4.90706
2025-03-09 09:32:30,700 - INFO - Batch 14600 finished
2025-03-09 09:32:30,700 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:32:48,185 - INFO - Batch 14626, Running Avg Loss: 4.90658
2025-03-09 09:33:05,671 - INFO - Batch 14651, Running Avg Loss: 4.90613
2025-03-09 09:33:23,207 - INFO - Batch 14676, Running Avg Loss: 4.90544
2025-03-09 09:33:40,911 - INFO - Batch 14701, Running Avg Loss: 4.90480
2025-03-09 09:33:40,926 - INFO - Batch 14700 finished
2025-03-09 09:33:40,926 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:33:58,401 - INFO - Batch 14726, Running Avg Loss: 4.90420
2025-03-09 09:34:15,940 - INFO - Batch 14751, Running Avg Loss: 4.90367
2025-03-09 09:34:33,450 - INFO - Batch 14776, Running Avg Loss: 4.90296
2025-03-09 09:34:51,082 - INFO - Batch 14801, Running Avg Loss: 4.90247
2025-03-09 09:34:51,097 - INFO - Batch 14800 finished
2025-03-09 09:34:51,098 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:35:08,506 - INFO - Batch 14826, Running Avg Loss: 4.90207
2025-03-09 09:35:25,957 - INFO - Batch 14851, Running Avg Loss: 4.90144
2025-03-09 09:35:43,424 - INFO - Batch 14876, Running Avg Loss: 4.90087
2025-03-09 09:36:00,892 - INFO - Batch 14901, Running Avg Loss: 4.90045
2025-03-09 09:36:00,908 - INFO - Batch 14900 finished
2025-03-09 09:36:00,908 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:36:18,345 - INFO - Batch 14926, Running Avg Loss: 4.89997
2025-03-09 09:36:35,803 - INFO - Batch 14951, Running Avg Loss: 4.89948
2025-03-09 09:36:53,296 - INFO - Batch 14976, Running Avg Loss: 4.89897
2025-03-09 09:37:10,725 - INFO - Batch 15001, Running Avg Loss: 4.89849
2025-03-09 09:37:10,741 - INFO - 
GPU Memory Stats at step 15000:
2025-03-09 09:37:10,741 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 09:37:10,741 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 09:37:10,742 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 09:37:10,742 - INFO - learning rate: 0.00000008
2025-03-09 09:37:10,742 - INFO - Ep 1 (Step 015000): Avg loss 4.898 | 61444096 tokens seen
2025-03-09 09:37:10,742 - INFO - optimizer lr: 0.00000008
2025-03-09 09:37:10,742 - INFO - scheduler lr: 0.00000008
2025-03-09 09:37:10,742 - INFO - Selected prompt: Just as there are many variants and forms of electronic malware and Internet-based 
2025-03-09 09:37:10,742 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:37:10,743 - INFO - random_topk: 1
2025-03-09 09:37:10,743 - INFO - random_temperature: 0.7690243909419825
2025-03-09 09:37:10,743 - INFO - global step 15000 , batch_idx 15000 => generating text
2025-03-09 09:37:10,743 - INFO - Generating on device cuda
2025-03-09 09:37:42,941 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:37:42,942 - INFO - Just as there are many variants and forms of electronic malware and Internet-based 2. These tools can be used for various types of data, including the data, data visualization, and data visualization. In this section, we will delve into the concept of data science, its significance in modern computing, and the following code.

### What is a Data?

A computer is a type of data that allows us to create and manipulate data. It is a way to create a model that allows us to perform complex numbers and concepts. For example, we can use a library called `X` to create a list of data.

### What is a Data?

Before we dive into the code, we need to install the necessary libraries. We will use a function called `X` to create a list of data.

### What is a Data?

Before we dive into the code, we need to install the necessary libraries. We will use a function called `X` to create a list of data.

```python
import numpy as np
import numpy as np
```

### What is a Data?

The `np.pyplot is a list of values that we can use to create a function called `X` and `X` that we can use.

### What is
2025-03-09 09:37:42,942 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:38:11,168 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_15000_steps_avg_loss_4.89849_optimizer_lr_0.00000008.pth
2025-03-09 09:38:11,342 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 09:38:11,342 - INFO - Batch 15000 finished
2025-03-09 09:38:11,342 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:38:28,640 - INFO - Batch 15026, Running Avg Loss: 4.89807
2025-03-09 09:38:46,101 - INFO - Batch 15051, Running Avg Loss: 4.89754
2025-03-09 09:39:03,531 - INFO - Batch 15076, Running Avg Loss: 4.89696
2025-03-09 09:39:20,982 - INFO - Batch 15101, Running Avg Loss: 4.89631
2025-03-09 09:39:20,998 - INFO - Batch 15100 finished
2025-03-09 09:39:20,999 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:39:38,367 - INFO - Batch 15126, Running Avg Loss: 4.89573
2025-03-09 09:39:55,986 - INFO - Batch 15151, Running Avg Loss: 4.89505
2025-03-09 09:40:13,482 - INFO - Batch 15176, Running Avg Loss: 4.89461
2025-03-09 09:40:30,946 - INFO - Batch 15201, Running Avg Loss: 4.89416
2025-03-09 09:40:30,964 - INFO - Batch 15200 finished
2025-03-09 09:40:30,964 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:40:48,346 - INFO - Batch 15226, Running Avg Loss: 4.89358
2025-03-09 09:41:05,854 - INFO - Batch 15251, Running Avg Loss: 4.89316
2025-03-09 09:41:23,303 - INFO - Batch 15276, Running Avg Loss: 4.89257
2025-03-09 09:41:40,796 - INFO - Batch 15301, Running Avg Loss: 4.89215
2025-03-09 09:41:40,813 - INFO - Batch 15300 finished
2025-03-09 09:41:40,814 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:41:58,284 - INFO - Batch 15326, Running Avg Loss: 4.89156
2025-03-09 09:42:15,760 - INFO - Batch 15351, Running Avg Loss: 4.89116
2025-03-09 09:42:33,269 - INFO - Batch 15376, Running Avg Loss: 4.89078
2025-03-09 09:42:50,740 - INFO - Batch 15401, Running Avg Loss: 4.89034
2025-03-09 09:42:50,756 - INFO - Batch 15400 finished
2025-03-09 09:42:50,757 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:43:08,171 - INFO - Batch 15426, Running Avg Loss: 4.88984
2025-03-09 09:43:25,657 - INFO - Batch 15451, Running Avg Loss: 4.88955
2025-03-09 09:43:43,180 - INFO - Batch 15476, Running Avg Loss: 4.88908
2025-03-09 09:44:00,604 - INFO - Batch 15501, Running Avg Loss: 4.88870
2025-03-09 09:44:00,618 - INFO - Batch 15500 finished
2025-03-09 09:44:00,618 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:44:18,061 - INFO - Batch 15526, Running Avg Loss: 4.88836
2025-03-09 09:44:35,755 - INFO - Batch 15551, Running Avg Loss: 4.88790
2025-03-09 09:44:53,250 - INFO - Batch 15576, Running Avg Loss: 4.88741
2025-03-09 09:45:10,753 - INFO - Batch 15601, Running Avg Loss: 4.88688
2025-03-09 09:45:10,774 - INFO - Batch 15600 finished
2025-03-09 09:45:10,775 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:45:28,263 - INFO - Batch 15626, Running Avg Loss: 4.88653
2025-03-09 09:45:45,678 - INFO - Batch 15651, Running Avg Loss: 4.88596
2025-03-09 09:46:03,175 - INFO - Batch 15676, Running Avg Loss: 4.88540
2025-03-09 09:46:20,611 - INFO - Batch 15701, Running Avg Loss: 4.88499
2025-03-09 09:46:20,627 - INFO - Batch 15700 finished
2025-03-09 09:46:20,628 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:46:38,109 - INFO - Batch 15726, Running Avg Loss: 4.88451
2025-03-09 09:46:55,576 - INFO - Batch 15751, Running Avg Loss: 4.88409
2025-03-09 09:47:12,986 - INFO - Batch 15776, Running Avg Loss: 4.88365
2025-03-09 09:47:30,405 - INFO - Batch 15801, Running Avg Loss: 4.88313
2025-03-09 09:47:30,422 - INFO - Batch 15800 finished
2025-03-09 09:47:30,423 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:47:48,010 - INFO - Batch 15826, Running Avg Loss: 4.88255
2025-03-09 09:48:05,461 - INFO - Batch 15851, Running Avg Loss: 4.88209
2025-03-09 09:48:22,993 - INFO - Batch 15876, Running Avg Loss: 4.88166
2025-03-09 09:48:40,512 - INFO - Batch 15901, Running Avg Loss: 4.88118
2025-03-09 09:48:40,529 - INFO - Batch 15900 finished
2025-03-09 09:48:40,530 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:48:57,983 - INFO - Batch 15926, Running Avg Loss: 4.88076
2025-03-09 09:49:15,398 - INFO - Batch 15951, Running Avg Loss: 4.88021
2025-03-09 09:49:32,796 - INFO - Batch 15976, Running Avg Loss: 4.87986
2025-03-09 09:49:50,230 - INFO - Batch 16001, Running Avg Loss: 4.87949
2025-03-09 09:49:50,244 - INFO - Batch 16000 finished
2025-03-09 09:49:50,245 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:50:07,650 - INFO - Batch 16026, Running Avg Loss: 4.87912
2025-03-09 09:50:25,033 - INFO - Batch 16051, Running Avg Loss: 4.87884
2025-03-09 09:50:42,444 - INFO - Batch 16076, Running Avg Loss: 4.87844
2025-03-09 09:50:59,745 - INFO - Batch 16101, Running Avg Loss: 4.87804
2025-03-09 09:50:59,758 - INFO - Batch 16100 finished
2025-03-09 09:50:59,758 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:51:17,078 - INFO - Batch 16126, Running Avg Loss: 4.87761
2025-03-09 09:51:34,578 - INFO - Batch 16151, Running Avg Loss: 4.87725
2025-03-09 09:51:51,989 - INFO - Batch 16176, Running Avg Loss: 4.87691
2025-03-09 09:52:09,420 - INFO - Batch 16201, Running Avg Loss: 4.87647
2025-03-09 09:52:09,436 - INFO - Batch 16200 finished
2025-03-09 09:52:09,436 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:52:26,945 - INFO - Batch 16226, Running Avg Loss: 4.87614
2025-03-09 09:52:44,433 - INFO - Batch 16251, Running Avg Loss: 4.87575
2025-03-09 09:53:02,039 - INFO - Batch 16276, Running Avg Loss: 4.87528
2025-03-09 09:53:19,522 - INFO - Batch 16301, Running Avg Loss: 4.87484
2025-03-09 09:53:19,537 - INFO - Batch 16300 finished
2025-03-09 09:53:19,538 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:53:36,972 - INFO - Batch 16326, Running Avg Loss: 4.87431
2025-03-09 09:53:54,423 - INFO - Batch 16351, Running Avg Loss: 4.87395
2025-03-09 09:54:12,018 - INFO - Batch 16376, Running Avg Loss: 4.87349
2025-03-09 09:54:29,459 - INFO - Batch 16401, Running Avg Loss: 4.87290
2025-03-09 09:54:29,476 - INFO - Batch 16400 finished
2025-03-09 09:54:29,477 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:54:46,903 - INFO - Batch 16426, Running Avg Loss: 4.87249
2025-03-09 09:55:04,329 - INFO - Batch 16451, Running Avg Loss: 4.87198
2025-03-09 09:55:21,815 - INFO - Batch 16476, Running Avg Loss: 4.87143
2025-03-09 09:55:39,234 - INFO - Batch 16501, Running Avg Loss: 4.87105
2025-03-09 09:55:39,248 - INFO - Batch 16500 finished
2025-03-09 09:55:39,249 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:55:56,737 - INFO - Batch 16526, Running Avg Loss: 4.87059
2025-03-09 09:56:14,169 - INFO - Batch 16551, Running Avg Loss: 4.87023
2025-03-09 09:56:31,627 - INFO - Batch 16576, Running Avg Loss: 4.86983
2025-03-09 09:56:49,067 - INFO - Batch 16601, Running Avg Loss: 4.86930
2025-03-09 09:56:49,082 - INFO - Batch 16600 finished
2025-03-09 09:56:49,082 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:57:06,567 - INFO - Batch 16626, Running Avg Loss: 4.86894
2025-03-09 09:57:24,176 - INFO - Batch 16651, Running Avg Loss: 4.86856
2025-03-09 09:57:41,589 - INFO - Batch 16676, Running Avg Loss: 4.86830
2025-03-09 09:57:59,002 - INFO - Batch 16701, Running Avg Loss: 4.86782
2025-03-09 09:57:59,018 - INFO - Batch 16700 finished
2025-03-09 09:57:59,019 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:58:16,437 - INFO - Batch 16726, Running Avg Loss: 4.86749
2025-03-09 09:58:33,879 - INFO - Batch 16751, Running Avg Loss: 4.86720
2025-03-09 09:58:51,320 - INFO - Batch 16776, Running Avg Loss: 4.86686
2025-03-09 09:59:08,847 - INFO - Batch 16801, Running Avg Loss: 4.86640
2025-03-09 09:59:08,864 - INFO - Batch 16800 finished
2025-03-09 09:59:08,864 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 09:59:26,314 - INFO - Batch 16826, Running Avg Loss: 4.86610
2025-03-09 09:59:43,760 - INFO - Batch 16851, Running Avg Loss: 4.86573
2025-03-09 10:00:01,198 - INFO - Batch 16876, Running Avg Loss: 4.86532
2025-03-09 10:00:18,646 - INFO - Batch 16901, Running Avg Loss: 4.86496
2025-03-09 10:00:18,660 - INFO - Batch 16900 finished
2025-03-09 10:00:18,661 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:00:35,786 - INFO - Batch 16926, Running Avg Loss: 4.86442
2025-03-09 10:00:53,039 - INFO - Batch 16951, Running Avg Loss: 4.86404
2025-03-09 10:01:10,174 - INFO - Batch 16976, Running Avg Loss: 4.86367
2025-03-09 10:01:27,378 - INFO - Batch 17001, Running Avg Loss: 4.86327
2025-03-09 10:01:27,391 - INFO - Batch 17000 finished
2025-03-09 10:01:27,391 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:01:44,664 - INFO - Batch 17026, Running Avg Loss: 4.86292
2025-03-09 10:02:02,013 - INFO - Batch 17051, Running Avg Loss: 4.86249
2025-03-09 10:02:19,454 - INFO - Batch 17076, Running Avg Loss: 4.86203
2025-03-09 10:02:36,893 - INFO - Batch 17101, Running Avg Loss: 4.86169
2025-03-09 10:02:36,908 - INFO - Batch 17100 finished
2025-03-09 10:02:36,909 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:02:54,332 - INFO - Batch 17126, Running Avg Loss: 4.86139
2025-03-09 10:03:11,727 - INFO - Batch 17151, Running Avg Loss: 4.86104
2025-03-09 10:03:29,175 - INFO - Batch 17176, Running Avg Loss: 4.86067
2025-03-09 10:03:46,606 - INFO - Batch 17201, Running Avg Loss: 4.86036
2025-03-09 10:03:46,622 - INFO - Batch 17200 finished
2025-03-09 10:03:46,623 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:04:04,266 - INFO - Batch 17226, Running Avg Loss: 4.86005
2025-03-09 10:04:21,677 - INFO - Batch 17251, Running Avg Loss: 4.85969
2025-03-09 10:04:39,116 - INFO - Batch 17276, Running Avg Loss: 4.85937
2025-03-09 10:04:56,553 - INFO - Batch 17301, Running Avg Loss: 4.85899
2025-03-09 10:04:56,568 - INFO - Batch 17300 finished
2025-03-09 10:04:56,569 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:05:14,024 - INFO - Batch 17326, Running Avg Loss: 4.85868
2025-03-09 10:05:31,559 - INFO - Batch 17351, Running Avg Loss: 4.85836
2025-03-09 10:05:49,010 - INFO - Batch 17376, Running Avg Loss: 4.85805
2025-03-09 10:06:06,640 - INFO - Batch 17401, Running Avg Loss: 4.85766
2025-03-09 10:06:06,655 - INFO - Batch 17400 finished
2025-03-09 10:06:06,656 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:06:24,090 - INFO - Batch 17426, Running Avg Loss: 4.85735
2025-03-09 10:06:41,513 - INFO - Batch 17451, Running Avg Loss: 4.85697
2025-03-09 10:06:58,909 - INFO - Batch 17476, Running Avg Loss: 4.85661
2025-03-09 10:07:16,516 - INFO - Batch 17501, Running Avg Loss: 4.85627
2025-03-09 10:07:16,532 - INFO - 
GPU Memory Stats at step 17500:
2025-03-09 10:07:16,532 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 10:07:16,532 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 10:07:16,532 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 10:07:16,533 - INFO - learning rate: 0.00000008
2025-03-09 10:07:16,533 - INFO - Ep 1 (Step 017500): Avg loss 4.856 | 71684096 tokens seen
2025-03-09 10:07:16,533 - INFO - optimizer lr: 0.00000008
2025-03-09 10:07:16,533 - INFO - scheduler lr: 0.00000008
2025-03-09 10:07:16,533 - INFO - Selected prompt: Feeling Alone Together: Exploring Alienation and Isolation in Literature
2025-03-09 10:07:16,533 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:07:16,533 - INFO - random_topk: 6
2025-03-09 10:07:16,533 - INFO - random_temperature: 0.8016913878035201
2025-03-09 10:07:16,533 - INFO - global step 17500 , batch_idx 17500 => generating text
2025-03-09 10:07:16,534 - INFO - Generating on device cuda
2025-03-09 10:07:49,787 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:07:49,788 - INFO - Feeling Alone Together: Exploring Alienation and Isolation in Literature

Introduction

The world of science and technology have been a fascinating field in recent years, often referred to as the "the "Gol," a significant place that has been used to be a significant issue in shaping the way we learn about and interact within our lives. In this chapter, we will delve into how these two types of medical technology have shaped the way we now know and consider when it comes to contemporary discussions.

Section 1: What is the National Park

Before diving into the specifics of a specific study, let us first understand what constitutes a "the National Center" refers to the idea of understanding how people can work in the field to understand how the system works. By examining the historical context, we can gain insight into the role of human interactions and the environment.

Section 1: What is the "A" and "The Powerine"

* "A" refers to the "The term "t" of a system, including the "the term 'an.' It allows for the same language to create a more complex network of information.
* **Minks**: This is a type of study that allows them to understand their thoughts and feelings, regardless of the same language or language.
* **
2025-03-09 10:07:49,788 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:08:13,106 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_17500_steps_avg_loss_4.85627_optimizer_lr_0.00000008.pth
2025-03-09 10:08:13,310 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 10:08:13,310 - INFO - Batch 17500 finished
2025-03-09 10:08:13,310 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:08:30,661 - INFO - Batch 17526, Running Avg Loss: 4.85585
2025-03-09 10:08:48,287 - INFO - Batch 17551, Running Avg Loss: 4.85553
2025-03-09 10:09:06,059 - INFO - Batch 17576, Running Avg Loss: 4.85513
2025-03-09 10:09:23,648 - INFO - Batch 17601, Running Avg Loss: 4.85477
2025-03-09 10:09:23,665 - INFO - Batch 17600 finished
2025-03-09 10:09:23,665 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:09:41,186 - INFO - Batch 17626, Running Avg Loss: 4.85439
2025-03-09 10:09:58,676 - INFO - Batch 17651, Running Avg Loss: 4.85404
2025-03-09 10:10:16,144 - INFO - Batch 17676, Running Avg Loss: 4.85372
2025-03-09 10:10:33,635 - INFO - Batch 17701, Running Avg Loss: 4.85329
2025-03-09 10:10:33,651 - INFO - Batch 17700 finished
2025-03-09 10:10:33,651 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:10:51,140 - INFO - Batch 17726, Running Avg Loss: 4.85291
2025-03-09 10:11:08,626 - INFO - Batch 17751, Running Avg Loss: 4.85253
2025-03-09 10:11:26,183 - INFO - Batch 17776, Running Avg Loss: 4.85221
2025-03-09 10:11:43,621 - INFO - Batch 17801, Running Avg Loss: 4.85180
2025-03-09 10:11:43,637 - INFO - Batch 17800 finished
2025-03-09 10:11:43,638 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:12:01,069 - INFO - Batch 17826, Running Avg Loss: 4.85151
2025-03-09 10:12:18,653 - INFO - Batch 17851, Running Avg Loss: 4.85106
2025-03-09 10:12:36,162 - INFO - Batch 17876, Running Avg Loss: 4.85067
2025-03-09 10:12:53,634 - INFO - Batch 17901, Running Avg Loss: 4.85026
2025-03-09 10:12:53,651 - INFO - Batch 17900 finished
2025-03-09 10:12:53,651 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:13:10,981 - INFO - Batch 17926, Running Avg Loss: 4.84995
2025-03-09 10:13:28,322 - INFO - Batch 17951, Running Avg Loss: 4.84963
2025-03-09 10:13:45,744 - INFO - Batch 17976, Running Avg Loss: 4.84935
2025-03-09 10:14:03,294 - INFO - Batch 18001, Running Avg Loss: 4.84898
2025-03-09 10:14:03,311 - INFO - Batch 18000 finished
2025-03-09 10:14:03,312 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:14:20,784 - INFO - Batch 18026, Running Avg Loss: 4.84871
2025-03-09 10:14:38,245 - INFO - Batch 18051, Running Avg Loss: 4.84822
2025-03-09 10:14:55,615 - INFO - Batch 18076, Running Avg Loss: 4.84783
2025-03-09 10:15:12,988 - INFO - Batch 18101, Running Avg Loss: 4.84751
2025-03-09 10:15:13,004 - INFO - Batch 18100 finished
2025-03-09 10:15:13,005 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:15:30,404 - INFO - Batch 18126, Running Avg Loss: 4.84719
2025-03-09 10:15:48,022 - INFO - Batch 18151, Running Avg Loss: 4.84689
2025-03-09 10:16:05,479 - INFO - Batch 18176, Running Avg Loss: 4.84666
2025-03-09 10:16:22,993 - INFO - Batch 18201, Running Avg Loss: 4.84632
2025-03-09 10:16:23,010 - INFO - Batch 18200 finished
2025-03-09 10:16:23,010 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:16:40,545 - INFO - Batch 18226, Running Avg Loss: 4.84603
2025-03-09 10:16:58,055 - INFO - Batch 18251, Running Avg Loss: 4.84571
2025-03-09 10:17:15,582 - INFO - Batch 18276, Running Avg Loss: 4.84539
2025-03-09 10:17:33,127 - INFO - Batch 18301, Running Avg Loss: 4.84506
2025-03-09 10:17:33,144 - INFO - Batch 18300 finished
2025-03-09 10:17:33,144 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:17:50,623 - INFO - Batch 18326, Running Avg Loss: 4.84464
2025-03-09 10:18:08,077 - INFO - Batch 18351, Running Avg Loss: 4.84427
2025-03-09 10:18:25,519 - INFO - Batch 18376, Running Avg Loss: 4.84383
2025-03-09 10:18:42,949 - INFO - Batch 18401, Running Avg Loss: 4.84351
2025-03-09 10:18:42,965 - INFO - Batch 18400 finished
2025-03-09 10:18:42,965 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:19:00,640 - INFO - Batch 18426, Running Avg Loss: 4.84318
2025-03-09 10:19:18,146 - INFO - Batch 18451, Running Avg Loss: 4.84288
2025-03-09 10:19:35,667 - INFO - Batch 18476, Running Avg Loss: 4.84250
2025-03-09 10:19:53,195 - INFO - Batch 18501, Running Avg Loss: 4.84208
2025-03-09 10:19:53,213 - INFO - Batch 18500 finished
2025-03-09 10:19:53,213 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:20:10,736 - INFO - Batch 18526, Running Avg Loss: 4.84174
2025-03-09 10:20:28,347 - INFO - Batch 18551, Running Avg Loss: 4.84148
2025-03-09 10:20:45,976 - INFO - Batch 18576, Running Avg Loss: 4.84109
2025-03-09 10:21:03,498 - INFO - Batch 18601, Running Avg Loss: 4.84083
2025-03-09 10:21:03,513 - INFO - Batch 18600 finished
2025-03-09 10:21:03,513 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:21:21,017 - INFO - Batch 18626, Running Avg Loss: 4.84043
2025-03-09 10:21:38,500 - INFO - Batch 18651, Running Avg Loss: 4.84000
2025-03-09 10:21:56,044 - INFO - Batch 18676, Running Avg Loss: 4.83964
2025-03-09 10:22:13,783 - INFO - Batch 18701, Running Avg Loss: 4.83931
2025-03-09 10:22:13,802 - INFO - Batch 18700 finished
2025-03-09 10:22:13,802 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:22:31,413 - INFO - Batch 18726, Running Avg Loss: 4.83899
2025-03-09 10:22:49,090 - INFO - Batch 18751, Running Avg Loss: 4.83870
2025-03-09 10:23:06,734 - INFO - Batch 18776, Running Avg Loss: 4.83848
2025-03-09 10:23:24,316 - INFO - Batch 18801, Running Avg Loss: 4.83815
2025-03-09 10:23:24,333 - INFO - Batch 18800 finished
2025-03-09 10:23:24,334 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:23:41,945 - INFO - Batch 18826, Running Avg Loss: 4.83777
2025-03-09 10:23:59,476 - INFO - Batch 18851, Running Avg Loss: 4.83749
2025-03-09 10:24:16,913 - INFO - Batch 18876, Running Avg Loss: 4.83719
2025-03-09 10:24:34,479 - INFO - Batch 18901, Running Avg Loss: 4.83678
2025-03-09 10:24:34,498 - INFO - Batch 18900 finished
2025-03-09 10:24:34,498 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:24:52,047 - INFO - Batch 18926, Running Avg Loss: 4.83659
2025-03-09 10:25:09,580 - INFO - Batch 18951, Running Avg Loss: 4.83621
2025-03-09 10:25:27,321 - INFO - Batch 18976, Running Avg Loss: 4.83598
2025-03-09 10:25:44,856 - INFO - Batch 19001, Running Avg Loss: 4.83569
2025-03-09 10:25:44,871 - INFO - Batch 19000 finished
2025-03-09 10:25:44,872 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:26:02,481 - INFO - Batch 19026, Running Avg Loss: 4.83526
2025-03-09 10:26:19,995 - INFO - Batch 19051, Running Avg Loss: 4.83499
2025-03-09 10:26:37,623 - INFO - Batch 19076, Running Avg Loss: 4.83466
2025-03-09 10:26:55,238 - INFO - Batch 19101, Running Avg Loss: 4.83435
2025-03-09 10:26:55,253 - INFO - Batch 19100 finished
2025-03-09 10:26:55,253 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:27:12,841 - INFO - Batch 19126, Running Avg Loss: 4.83403
2025-03-09 10:27:30,373 - INFO - Batch 19151, Running Avg Loss: 4.83379
2025-03-09 10:27:47,950 - INFO - Batch 19176, Running Avg Loss: 4.83342
2025-03-09 10:28:05,518 - INFO - Batch 19201, Running Avg Loss: 4.83318
2025-03-09 10:28:05,536 - INFO - Batch 19200 finished
2025-03-09 10:28:05,536 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:28:23,075 - INFO - Batch 19226, Running Avg Loss: 4.83283
2025-03-09 10:28:40,648 - INFO - Batch 19251, Running Avg Loss: 4.83254
2025-03-09 10:28:58,438 - INFO - Batch 19276, Running Avg Loss: 4.83229
2025-03-09 10:29:16,001 - INFO - Batch 19301, Running Avg Loss: 4.83197
2025-03-09 10:29:16,019 - INFO - Batch 19300 finished
2025-03-09 10:29:16,019 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:29:33,470 - INFO - Batch 19326, Running Avg Loss: 4.83178
2025-03-09 10:29:51,045 - INFO - Batch 19351, Running Avg Loss: 4.83152
2025-03-09 10:30:08,615 - INFO - Batch 19376, Running Avg Loss: 4.83121
2025-03-09 10:30:26,097 - INFO - Batch 19401, Running Avg Loss: 4.83084
2025-03-09 10:30:26,114 - INFO - Batch 19400 finished
2025-03-09 10:30:26,114 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:30:43,554 - INFO - Batch 19426, Running Avg Loss: 4.83051
2025-03-09 10:31:00,637 - INFO - Batch 19451, Running Avg Loss: 4.83028
2025-03-09 10:31:17,957 - INFO - Batch 19476, Running Avg Loss: 4.82997
2025-03-09 10:31:35,435 - INFO - Batch 19501, Running Avg Loss: 4.82960
2025-03-09 10:31:35,451 - INFO - Batch 19500 finished
2025-03-09 10:31:35,452 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:31:52,867 - INFO - Batch 19526, Running Avg Loss: 4.82922
2025-03-09 10:32:10,531 - INFO - Batch 19551, Running Avg Loss: 4.82895
2025-03-09 10:32:28,061 - INFO - Batch 19576, Running Avg Loss: 4.82877
2025-03-09 10:32:45,485 - INFO - Batch 19601, Running Avg Loss: 4.82850
2025-03-09 10:32:45,501 - INFO - Batch 19600 finished
2025-03-09 10:32:45,502 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:33:02,994 - INFO - Batch 19626, Running Avg Loss: 4.82816
2025-03-09 10:33:20,507 - INFO - Batch 19651, Running Avg Loss: 4.82772
2025-03-09 10:33:38,051 - INFO - Batch 19676, Running Avg Loss: 4.82741
2025-03-09 10:33:55,681 - INFO - Batch 19701, Running Avg Loss: 4.82710
2025-03-09 10:33:55,696 - INFO - Batch 19700 finished
2025-03-09 10:33:55,696 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:34:13,199 - INFO - Batch 19726, Running Avg Loss: 4.82669
2025-03-09 10:34:30,633 - INFO - Batch 19751, Running Avg Loss: 4.82640
2025-03-09 10:34:48,128 - INFO - Batch 19776, Running Avg Loss: 4.82608
2025-03-09 10:35:05,795 - INFO - Batch 19801, Running Avg Loss: 4.82581
2025-03-09 10:35:05,813 - INFO - Batch 19800 finished
2025-03-09 10:35:05,813 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:35:23,300 - INFO - Batch 19826, Running Avg Loss: 4.82553
2025-03-09 10:35:40,835 - INFO - Batch 19851, Running Avg Loss: 4.82527
2025-03-09 10:35:58,348 - INFO - Batch 19876, Running Avg Loss: 4.82490
2025-03-09 10:36:15,866 - INFO - Batch 19901, Running Avg Loss: 4.82458
2025-03-09 10:36:15,886 - INFO - Batch 19900 finished
2025-03-09 10:36:15,886 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:36:33,446 - INFO - Batch 19926, Running Avg Loss: 4.82424
2025-03-09 10:36:50,927 - INFO - Batch 19951, Running Avg Loss: 4.82399
2025-03-09 10:37:08,462 - INFO - Batch 19976, Running Avg Loss: 4.82368
2025-03-09 10:37:25,887 - INFO - Batch 20001, Running Avg Loss: 4.82343
2025-03-09 10:37:25,903 - INFO - 
GPU Memory Stats at step 20000:
2025-03-09 10:37:25,904 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 10:37:25,904 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 10:37:25,904 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 10:37:25,904 - INFO - learning rate: 0.00000007
2025-03-09 10:37:25,904 - INFO - Ep 1 (Step 020000): Avg loss 4.823 | 81924096 tokens seen
2025-03-09 10:37:25,904 - INFO - optimizer lr: 0.00000007
2025-03-09 10:37:25,904 - INFO - scheduler lr: 0.00000007
2025-03-09 10:37:25,904 - INFO - Selected prompt: Correctly identifying what is causing a problem is the most important step in pest control.
2025-03-09 10:37:25,905 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:37:25,905 - INFO - random_topk: 4
2025-03-09 10:37:25,905 - INFO - random_temperature: 0.8064469367794592
2025-03-09 10:37:25,905 - INFO - global step 20000 , batch_idx 20000 => generating text
2025-03-09 10:37:25,905 - INFO - Generating on device cuda
2025-03-09 10:37:59,717 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:37:59,719 - INFO - Correctly identifying what is causing a problem is the most important step in pest control. This can be challenging to understand and manage your thoughts, feelings, and emotions through your body, and it's essential to understand the basics. By following these techniques, you can ensure that your body is safe and strong and enjoyable, and can help you feel better at your heart.

Imagine you're playing soccer during your school year, and you have a big, and you're not feeling sad. Now imagine if you were feeling overwhelmed and anxious about your body. That's what happens when someone feels uncomfortable, like you are feeling overwhelmed, and then you can't feel uncomfortable. But there are many reasons why you feel like, "I don't know what you're going through, but I feel sad." This way, we are using ourselves and others to make decisions about ourselves.

One way to understand this concept is by understanding what we're thinking about. When we talk about the same language, we often feel like they are feeling overwhelmed and valued. When we grow older, we may find it challenging to learn about ourselves and others, just like our friend or family. But what if someone has a good idea, then they're not always feeling uncomfortable or anxious.

Now let's talk about what a good person is. When we say something is wrong
2025-03-09 10:37:59,719 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:38:26,783 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_20000_steps_avg_loss_4.82343_optimizer_lr_0.00000007.pth
2025-03-09 10:38:26,969 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 10:38:26,969 - INFO - Batch 20000 finished
2025-03-09 10:38:26,969 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:38:44,334 - INFO - Batch 20026, Running Avg Loss: 4.82314
2025-03-09 10:39:01,796 - INFO - Batch 20051, Running Avg Loss: 4.82282
2025-03-09 10:39:19,229 - INFO - Batch 20076, Running Avg Loss: 4.82257
2025-03-09 10:39:36,701 - INFO - Batch 20101, Running Avg Loss: 4.82227
2025-03-09 10:39:36,717 - INFO - Batch 20100 finished
2025-03-09 10:39:36,717 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:39:54,210 - INFO - Batch 20126, Running Avg Loss: 4.82197
2025-03-09 10:40:11,966 - INFO - Batch 20151, Running Avg Loss: 4.82166
2025-03-09 10:40:29,188 - INFO - Batch 20176, Running Avg Loss: 4.82131
2025-03-09 10:40:46,354 - INFO - Batch 20201, Running Avg Loss: 4.82094
2025-03-09 10:40:46,366 - INFO - Batch 20200 finished
2025-03-09 10:40:46,367 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:41:03,565 - INFO - Batch 20226, Running Avg Loss: 4.82063
2025-03-09 10:41:21,238 - INFO - Batch 20251, Running Avg Loss: 4.82038
2025-03-09 10:41:38,721 - INFO - Batch 20276, Running Avg Loss: 4.82017
2025-03-09 10:41:56,271 - INFO - Batch 20301, Running Avg Loss: 4.81994
2025-03-09 10:41:56,287 - INFO - Batch 20300 finished
2025-03-09 10:41:56,288 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:42:13,772 - INFO - Batch 20326, Running Avg Loss: 4.81970
2025-03-09 10:42:31,307 - INFO - Batch 20351, Running Avg Loss: 4.81944
2025-03-09 10:42:48,829 - INFO - Batch 20376, Running Avg Loss: 4.81921
2025-03-09 10:43:06,389 - INFO - Batch 20401, Running Avg Loss: 4.81901
2025-03-09 10:43:06,406 - INFO - Batch 20400 finished
2025-03-09 10:43:06,407 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:43:24,059 - INFO - Batch 20426, Running Avg Loss: 4.81871
2025-03-09 10:43:41,630 - INFO - Batch 20451, Running Avg Loss: 4.81841
2025-03-09 10:43:59,108 - INFO - Batch 20476, Running Avg Loss: 4.81813
2025-03-09 10:44:16,655 - INFO - Batch 20501, Running Avg Loss: 4.81776
2025-03-09 10:44:16,671 - INFO - Batch 20500 finished
2025-03-09 10:44:16,672 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:44:34,215 - INFO - Batch 20526, Running Avg Loss: 4.81742
2025-03-09 10:44:51,873 - INFO - Batch 20551, Running Avg Loss: 4.81721
2025-03-09 10:45:09,375 - INFO - Batch 20576, Running Avg Loss: 4.81688
2025-03-09 10:45:26,836 - INFO - Batch 20601, Running Avg Loss: 4.81668
2025-03-09 10:45:26,855 - INFO - Batch 20600 finished
2025-03-09 10:45:26,856 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:45:44,375 - INFO - Batch 20626, Running Avg Loss: 4.81632
2025-03-09 10:46:01,939 - INFO - Batch 20651, Running Avg Loss: 4.81592
2025-03-09 10:46:19,522 - INFO - Batch 20676, Running Avg Loss: 4.81561
2025-03-09 10:46:36,946 - INFO - Batch 20701, Running Avg Loss: 4.81521
2025-03-09 10:46:36,960 - INFO - Batch 20700 finished
2025-03-09 10:46:36,961 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:46:54,477 - INFO - Batch 20726, Running Avg Loss: 4.81493
2025-03-09 10:47:11,861 - INFO - Batch 20751, Running Avg Loss: 4.81476
2025-03-09 10:47:29,349 - INFO - Batch 20776, Running Avg Loss: 4.81449
2025-03-09 10:47:46,796 - INFO - Batch 20801, Running Avg Loss: 4.81418
2025-03-09 10:47:46,810 - INFO - Batch 20800 finished
2025-03-09 10:47:46,811 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:48:04,387 - INFO - Batch 20826, Running Avg Loss: 4.81392
2025-03-09 10:48:21,899 - INFO - Batch 20851, Running Avg Loss: 4.81363
2025-03-09 10:48:39,431 - INFO - Batch 20876, Running Avg Loss: 4.81339
2025-03-09 10:48:57,048 - INFO - Batch 20901, Running Avg Loss: 4.81313
2025-03-09 10:48:57,067 - INFO - Batch 20900 finished
2025-03-09 10:48:57,067 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:49:14,608 - INFO - Batch 20926, Running Avg Loss: 4.81286
2025-03-09 10:49:32,147 - INFO - Batch 20951, Running Avg Loss: 4.81267
2025-03-09 10:49:49,598 - INFO - Batch 20976, Running Avg Loss: 4.81239
2025-03-09 10:50:07,085 - INFO - Batch 21001, Running Avg Loss: 4.81215
2025-03-09 10:50:07,102 - INFO - Batch 21000 finished
2025-03-09 10:50:07,102 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:50:24,491 - INFO - Batch 21026, Running Avg Loss: 4.81182
2025-03-09 10:50:41,928 - INFO - Batch 21051, Running Avg Loss: 4.81157
2025-03-09 10:50:59,133 - INFO - Batch 21076, Running Avg Loss: 4.81132
2025-03-09 10:51:16,304 - INFO - Batch 21101, Running Avg Loss: 4.81109
2025-03-09 10:51:16,319 - INFO - Batch 21100 finished
2025-03-09 10:51:16,320 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:51:34,034 - INFO - Batch 21126, Running Avg Loss: 4.81083
2025-03-09 10:51:51,582 - INFO - Batch 21151, Running Avg Loss: 4.81052
2025-03-09 10:52:09,090 - INFO - Batch 21176, Running Avg Loss: 4.81025
2025-03-09 10:52:26,523 - INFO - Batch 21201, Running Avg Loss: 4.80998
2025-03-09 10:52:26,537 - INFO - Batch 21200 finished
2025-03-09 10:52:26,538 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:52:44,041 - INFO - Batch 21226, Running Avg Loss: 4.80980
2025-03-09 10:53:01,595 - INFO - Batch 21251, Running Avg Loss: 4.80944
2025-03-09 10:53:19,276 - INFO - Batch 21276, Running Avg Loss: 4.80921
2025-03-09 10:53:36,783 - INFO - Batch 21301, Running Avg Loss: 4.80899
2025-03-09 10:53:36,798 - INFO - Batch 21300 finished
2025-03-09 10:53:36,798 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:53:54,316 - INFO - Batch 21326, Running Avg Loss: 4.80881
2025-03-09 10:54:11,754 - INFO - Batch 21351, Running Avg Loss: 4.80863
2025-03-09 10:54:29,341 - INFO - Batch 21376, Running Avg Loss: 4.80847
2025-03-09 10:54:46,850 - INFO - Batch 21401, Running Avg Loss: 4.80822
2025-03-09 10:54:46,866 - INFO - Batch 21400 finished
2025-03-09 10:54:46,866 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:55:04,465 - INFO - Batch 21426, Running Avg Loss: 4.80797
2025-03-09 10:55:22,051 - INFO - Batch 21451, Running Avg Loss: 4.80774
2025-03-09 10:55:39,598 - INFO - Batch 21476, Running Avg Loss: 4.80758
2025-03-09 10:55:57,139 - INFO - Batch 21501, Running Avg Loss: 4.80719
2025-03-09 10:55:57,156 - INFO - Batch 21500 finished
2025-03-09 10:55:57,157 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:56:14,766 - INFO - Batch 21526, Running Avg Loss: 4.80692
2025-03-09 10:56:32,292 - INFO - Batch 21551, Running Avg Loss: 4.80670
2025-03-09 10:56:49,725 - INFO - Batch 21576, Running Avg Loss: 4.80640
2025-03-09 10:57:07,119 - INFO - Batch 21601, Running Avg Loss: 4.80617
2025-03-09 10:57:07,136 - INFO - Batch 21600 finished
2025-03-09 10:57:07,137 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:57:24,526 - INFO - Batch 21626, Running Avg Loss: 4.80598
2025-03-09 10:57:42,186 - INFO - Batch 21651, Running Avg Loss: 4.80576
2025-03-09 10:57:59,779 - INFO - Batch 21676, Running Avg Loss: 4.80543
2025-03-09 10:58:17,201 - INFO - Batch 21701, Running Avg Loss: 4.80520
2025-03-09 10:58:17,218 - INFO - Batch 21700 finished
2025-03-09 10:58:17,218 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:58:34,486 - INFO - Batch 21726, Running Avg Loss: 4.80489
2025-03-09 10:58:51,829 - INFO - Batch 21751, Running Avg Loss: 4.80460
2025-03-09 10:59:09,167 - INFO - Batch 21776, Running Avg Loss: 4.80435
2025-03-09 10:59:26,794 - INFO - Batch 21801, Running Avg Loss: 4.80412
2025-03-09 10:59:26,810 - INFO - Batch 21800 finished
2025-03-09 10:59:26,810 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 10:59:44,283 - INFO - Batch 21826, Running Avg Loss: 4.80376
2025-03-09 11:00:01,690 - INFO - Batch 21851, Running Avg Loss: 4.80354
2025-03-09 11:00:19,189 - INFO - Batch 21876, Running Avg Loss: 4.80322
2025-03-09 11:00:36,596 - INFO - Batch 21901, Running Avg Loss: 4.80297
2025-03-09 11:00:36,613 - INFO - Batch 21900 finished
2025-03-09 11:00:36,613 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:00:54,073 - INFO - Batch 21926, Running Avg Loss: 4.80264
2025-03-09 11:01:11,755 - INFO - Batch 21951, Running Avg Loss: 4.80246
2025-03-09 11:01:29,166 - INFO - Batch 21976, Running Avg Loss: 4.80219
2025-03-09 11:01:46,619 - INFO - Batch 22001, Running Avg Loss: 4.80198
2025-03-09 11:01:46,636 - INFO - Batch 22000 finished
2025-03-09 11:01:46,636 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:02:04,051 - INFO - Batch 22026, Running Avg Loss: 4.80163
2025-03-09 11:02:21,572 - INFO - Batch 22051, Running Avg Loss: 4.80137
2025-03-09 11:02:39,015 - INFO - Batch 22076, Running Avg Loss: 4.80115
2025-03-09 11:02:56,569 - INFO - Batch 22101, Running Avg Loss: 4.80094
2025-03-09 11:02:56,587 - INFO - Batch 22100 finished
2025-03-09 11:02:56,588 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:03:14,102 - INFO - Batch 22126, Running Avg Loss: 4.80072
2025-03-09 11:03:31,657 - INFO - Batch 22151, Running Avg Loss: 4.80050
2025-03-09 11:03:49,110 - INFO - Batch 22176, Running Avg Loss: 4.80038
2025-03-09 11:04:06,322 - INFO - Batch 22201, Running Avg Loss: 4.80020
2025-03-09 11:04:06,335 - INFO - Batch 22200 finished
2025-03-09 11:04:06,335 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:04:23,838 - INFO - Batch 22226, Running Avg Loss: 4.80007
2025-03-09 11:04:41,228 - INFO - Batch 22251, Running Avg Loss: 4.79991
2025-03-09 11:04:58,650 - INFO - Batch 22276, Running Avg Loss: 4.79969
2025-03-09 11:05:15,999 - INFO - Batch 22301, Running Avg Loss: 4.79947
2025-03-09 11:05:16,014 - INFO - Batch 22300 finished
2025-03-09 11:05:16,014 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:05:33,477 - INFO - Batch 22326, Running Avg Loss: 4.79921
2025-03-09 11:05:51,185 - INFO - Batch 22351, Running Avg Loss: 4.79897
2025-03-09 11:06:08,755 - INFO - Batch 22376, Running Avg Loss: 4.79880
2025-03-09 11:06:26,576 - INFO - Batch 22401, Running Avg Loss: 4.79861
2025-03-09 11:06:26,596 - INFO - Batch 22400 finished
2025-03-09 11:06:26,597 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:06:44,214 - INFO - Batch 22426, Running Avg Loss: 4.79832
2025-03-09 11:07:01,814 - INFO - Batch 22451, Running Avg Loss: 4.79805
2025-03-09 11:07:19,286 - INFO - Batch 22476, Running Avg Loss: 4.79785
2025-03-09 11:07:36,927 - INFO - Batch 22501, Running Avg Loss: 4.79762
2025-03-09 11:07:36,945 - INFO - 
GPU Memory Stats at step 22500:
2025-03-09 11:07:36,946 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 11:07:36,946 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 11:07:36,946 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 11:07:36,946 - INFO - learning rate: 0.00000007
2025-03-09 11:07:36,947 - INFO - Ep 1 (Step 022500): Avg loss 4.798 | 92164096 tokens seen
2025-03-09 11:07:36,947 - INFO - optimizer lr: 0.00000007
2025-03-09 11:07:36,947 - INFO - scheduler lr: 0.00000007
2025-03-09 11:07:36,947 - INFO - Selected prompt: Introduction: The Art of Crafting Vegan Sandwich Delights Sandwiches occupy a unique space in
2025-03-09 11:07:36,947 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:07:36,947 - INFO - random_topk: 8
2025-03-09 11:07:36,947 - INFO - random_temperature: 0.7349340641133754
2025-03-09 11:07:36,947 - INFO - global step 22500 , batch_idx 22500 => generating text
2025-03-09 11:07:36,947 - INFO - Generating on device cuda
2025-03-09 11:08:10,896 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:08:10,897 - INFO - Introduction: The Art of Crafting Vegan Sandwich Delights Sandwiches occupy a unique space in the United States. This period saw many challenges, including the US, including the Middle East, China, and South Africa. Today, we will explore how these two groups of women were created in the globe, their history, and how it came together. Let's dive into this incredible journey together!

First, let's talk about what constitutes a 'or-year-old,' which has its unique culture and traditions. At its core, the National Center is not only one-in-white culture, but instead of being an ancient country. Many people believe that the country's unique identities were built over a long time.

Now, why does any of this matter? Well, imagine if every place was made up of a big tree and had to make your body. That wouldn't be quite an incredible experience. It would be like the first one of the most popular forms of art!

But there were no more than a big family member  they didn't know much about the environment. They didn't need to get any great job or even more people! Instead, they were made from their own countries, including the "A's and the United Kingdom," and the other women in the country.

But wait, there was more!
2025-03-09 11:08:10,897 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:08:38,341 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_22500_steps_avg_loss_4.79762_optimizer_lr_0.00000007.pth
2025-03-09 11:08:38,574 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 11:08:38,575 - INFO - Batch 22500 finished
2025-03-09 11:08:38,575 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:08:56,186 - INFO - Batch 22526, Running Avg Loss: 4.79745
2025-03-09 11:09:13,672 - INFO - Batch 22551, Running Avg Loss: 4.79726
2025-03-09 11:09:31,275 - INFO - Batch 22576, Running Avg Loss: 4.79690
2025-03-09 11:09:48,725 - INFO - Batch 22601, Running Avg Loss: 4.79668
2025-03-09 11:09:48,744 - INFO - Batch 22600 finished
2025-03-09 11:09:48,745 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:10:06,263 - INFO - Batch 22626, Running Avg Loss: 4.79646
2025-03-09 11:10:23,811 - INFO - Batch 22651, Running Avg Loss: 4.79620
2025-03-09 11:10:41,274 - INFO - Batch 22676, Running Avg Loss: 4.79592
2025-03-09 11:10:58,714 - INFO - Batch 22701, Running Avg Loss: 4.79576
2025-03-09 11:10:58,731 - INFO - Batch 22700 finished
2025-03-09 11:10:58,731 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:11:16,158 - INFO - Batch 22726, Running Avg Loss: 4.79546
2025-03-09 11:11:33,621 - INFO - Batch 22751, Running Avg Loss: 4.79526
2025-03-09 11:11:51,051 - INFO - Batch 22776, Running Avg Loss: 4.79494
2025-03-09 11:12:08,598 - INFO - Batch 22801, Running Avg Loss: 4.79480
2025-03-09 11:12:08,616 - INFO - Batch 22800 finished
2025-03-09 11:12:08,616 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:12:26,137 - INFO - Batch 22826, Running Avg Loss: 4.79464
2025-03-09 11:12:43,889 - INFO - Batch 22851, Running Avg Loss: 4.79436
2025-03-09 11:13:01,342 - INFO - Batch 22876, Running Avg Loss: 4.79411
2025-03-09 11:13:18,814 - INFO - Batch 22901, Running Avg Loss: 4.79395
2025-03-09 11:13:18,830 - INFO - Batch 22900 finished
2025-03-09 11:13:18,831 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:13:36,387 - INFO - Batch 22926, Running Avg Loss: 4.79366
2025-03-09 11:13:53,877 - INFO - Batch 22951, Running Avg Loss: 4.79351
2025-03-09 11:14:11,229 - INFO - Batch 22976, Running Avg Loss: 4.79322
2025-03-09 11:14:28,798 - INFO - Batch 23001, Running Avg Loss: 4.79288
2025-03-09 11:14:28,813 - INFO - Batch 23000 finished
2025-03-09 11:14:28,813 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:14:46,277 - INFO - Batch 23026, Running Avg Loss: 4.79271
2025-03-09 11:15:03,843 - INFO - Batch 23051, Running Avg Loss: 4.79244
2025-03-09 11:15:21,326 - INFO - Batch 23076, Running Avg Loss: 4.79230
2025-03-09 11:15:38,814 - INFO - Batch 23101, Running Avg Loss: 4.79201
2025-03-09 11:15:38,833 - INFO - Batch 23100 finished
2025-03-09 11:15:38,833 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:15:56,345 - INFO - Batch 23126, Running Avg Loss: 4.79184
2025-03-09 11:16:14,037 - INFO - Batch 23151, Running Avg Loss: 4.79161
2025-03-09 11:16:31,455 - INFO - Batch 23176, Running Avg Loss: 4.79137
2025-03-09 11:16:48,856 - INFO - Batch 23201, Running Avg Loss: 4.79114
2025-03-09 11:16:48,871 - INFO - Batch 23200 finished
2025-03-09 11:16:48,871 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:17:06,336 - INFO - Batch 23226, Running Avg Loss: 4.79091
2025-03-09 11:17:23,891 - INFO - Batch 23251, Running Avg Loss: 4.79060
2025-03-09 11:17:41,317 - INFO - Batch 23276, Running Avg Loss: 4.79025
2025-03-09 11:17:58,822 - INFO - Batch 23301, Running Avg Loss: 4.79000
2025-03-09 11:17:58,839 - INFO - Batch 23300 finished
2025-03-09 11:17:58,840 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:18:16,323 - INFO - Batch 23326, Running Avg Loss: 4.78990
2025-03-09 11:18:33,829 - INFO - Batch 23351, Running Avg Loss: 4.78970
2025-03-09 11:18:51,332 - INFO - Batch 23376, Running Avg Loss: 4.78946
2025-03-09 11:19:08,805 - INFO - Batch 23401, Running Avg Loss: 4.78922
2025-03-09 11:19:08,821 - INFO - Batch 23400 finished
2025-03-09 11:19:08,821 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:19:26,385 - INFO - Batch 23426, Running Avg Loss: 4.78905
2025-03-09 11:19:43,864 - INFO - Batch 23451, Running Avg Loss: 4.78879
2025-03-09 11:20:01,451 - INFO - Batch 23476, Running Avg Loss: 4.78857
2025-03-09 11:20:18,983 - INFO - Batch 23501, Running Avg Loss: 4.78837
2025-03-09 11:20:18,997 - INFO - Batch 23500 finished
2025-03-09 11:20:18,998 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:20:36,406 - INFO - Batch 23526, Running Avg Loss: 4.78814
2025-03-09 11:20:53,784 - INFO - Batch 23551, Running Avg Loss: 4.78793
2025-03-09 11:21:11,299 - INFO - Batch 23576, Running Avg Loss: 4.78780
2025-03-09 11:21:28,718 - INFO - Batch 23601, Running Avg Loss: 4.78755
2025-03-09 11:21:28,734 - INFO - Batch 23600 finished
2025-03-09 11:21:28,735 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:21:46,247 - INFO - Batch 23626, Running Avg Loss: 4.78736
2025-03-09 11:22:03,737 - INFO - Batch 23651, Running Avg Loss: 4.78710
2025-03-09 11:22:21,193 - INFO - Batch 23676, Running Avg Loss: 4.78683
2025-03-09 11:22:38,789 - INFO - Batch 23701, Running Avg Loss: 4.78656
2025-03-09 11:22:38,805 - INFO - Batch 23700 finished
2025-03-09 11:22:38,806 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:22:56,341 - INFO - Batch 23726, Running Avg Loss: 4.78641
2025-03-09 11:23:13,908 - INFO - Batch 23751, Running Avg Loss: 4.78618
2025-03-09 11:23:31,462 - INFO - Batch 23776, Running Avg Loss: 4.78598
2025-03-09 11:23:48,960 - INFO - Batch 23801, Running Avg Loss: 4.78576
2025-03-09 11:23:48,975 - INFO - Batch 23800 finished
2025-03-09 11:23:48,975 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:24:06,361 - INFO - Batch 23826, Running Avg Loss: 4.78559
2025-03-09 11:24:23,803 - INFO - Batch 23851, Running Avg Loss: 4.78545
2025-03-09 11:24:41,284 - INFO - Batch 23876, Running Avg Loss: 4.78526
2025-03-09 11:24:58,751 - INFO - Batch 23901, Running Avg Loss: 4.78508
2025-03-09 11:24:58,767 - INFO - Batch 23900 finished
2025-03-09 11:24:58,767 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:25:16,268 - INFO - Batch 23926, Running Avg Loss: 4.78494
2025-03-09 11:25:33,796 - INFO - Batch 23951, Running Avg Loss: 4.78474
2025-03-09 11:25:51,441 - INFO - Batch 23976, Running Avg Loss: 4.78446
2025-03-09 11:26:08,861 - INFO - Batch 24001, Running Avg Loss: 4.78418
2025-03-09 11:26:08,878 - INFO - Batch 24000 finished
2025-03-09 11:26:08,879 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:26:26,260 - INFO - Batch 24026, Running Avg Loss: 4.78398
2025-03-09 11:26:43,678 - INFO - Batch 24051, Running Avg Loss: 4.78378
2025-03-09 11:27:01,127 - INFO - Batch 24076, Running Avg Loss: 4.78354
2025-03-09 11:27:18,661 - INFO - Batch 24101, Running Avg Loss: 4.78332
2025-03-09 11:27:18,676 - INFO - Batch 24100 finished
2025-03-09 11:27:18,676 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:27:36,239 - INFO - Batch 24126, Running Avg Loss: 4.78314
2025-03-09 11:27:53,716 - INFO - Batch 24151, Running Avg Loss: 4.78290
2025-03-09 11:28:11,130 - INFO - Batch 24176, Running Avg Loss: 4.78275
2025-03-09 11:28:28,489 - INFO - Batch 24201, Running Avg Loss: 4.78258
2025-03-09 11:28:28,507 - INFO - Batch 24200 finished
2025-03-09 11:28:28,508 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:28:45,874 - INFO - Batch 24226, Running Avg Loss: 4.78237
2025-03-09 11:29:03,303 - INFO - Batch 24251, Running Avg Loss: 4.78222
2025-03-09 11:29:20,988 - INFO - Batch 24276, Running Avg Loss: 4.78205
2025-03-09 11:29:38,399 - INFO - Batch 24301, Running Avg Loss: 4.78182
2025-03-09 11:29:38,414 - INFO - Batch 24300 finished
2025-03-09 11:29:38,415 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:29:55,763 - INFO - Batch 24326, Running Avg Loss: 4.78167
2025-03-09 11:30:13,106 - INFO - Batch 24351, Running Avg Loss: 4.78142
2025-03-09 11:30:30,446 - INFO - Batch 24376, Running Avg Loss: 4.78116
2025-03-09 11:30:47,858 - INFO - Batch 24401, Running Avg Loss: 4.78098
2025-03-09 11:30:47,875 - INFO - Batch 24400 finished
2025-03-09 11:30:47,876 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:31:05,321 - INFO - Batch 24426, Running Avg Loss: 4.78080
2025-03-09 11:31:22,760 - INFO - Batch 24451, Running Avg Loss: 4.78068
2025-03-09 11:31:40,268 - INFO - Batch 24476, Running Avg Loss: 4.78050
2025-03-09 11:31:57,758 - INFO - Batch 24501, Running Avg Loss: 4.78034
2025-03-09 11:31:57,774 - INFO - Batch 24500 finished
2025-03-09 11:31:57,774 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:32:15,247 - INFO - Batch 24526, Running Avg Loss: 4.78012
2025-03-09 11:32:32,870 - INFO - Batch 24551, Running Avg Loss: 4.77987
2025-03-09 11:32:50,334 - INFO - Batch 24576, Running Avg Loss: 4.77964
2025-03-09 11:33:07,810 - INFO - Batch 24601, Running Avg Loss: 4.77937
2025-03-09 11:33:07,827 - INFO - Batch 24600 finished
2025-03-09 11:33:07,828 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:33:25,283 - INFO - Batch 24626, Running Avg Loss: 4.77918
2025-03-09 11:33:42,758 - INFO - Batch 24651, Running Avg Loss: 4.77899
2025-03-09 11:34:00,253 - INFO - Batch 24676, Running Avg Loss: 4.77882
2025-03-09 11:34:17,819 - INFO - Batch 24701, Running Avg Loss: 4.77862
2025-03-09 11:34:17,836 - INFO - Batch 24700 finished
2025-03-09 11:34:17,836 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:34:35,233 - INFO - Batch 24726, Running Avg Loss: 4.77845
2025-03-09 11:34:52,629 - INFO - Batch 24751, Running Avg Loss: 4.77832
2025-03-09 11:35:10,059 - INFO - Batch 24776, Running Avg Loss: 4.77815
2025-03-09 11:35:27,728 - INFO - Batch 24801, Running Avg Loss: 4.77805
2025-03-09 11:35:27,743 - INFO - Batch 24800 finished
2025-03-09 11:35:27,743 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:35:45,202 - INFO - Batch 24826, Running Avg Loss: 4.77780
2025-03-09 11:36:02,617 - INFO - Batch 24851, Running Avg Loss: 4.77760
2025-03-09 11:36:19,970 - INFO - Batch 24876, Running Avg Loss: 4.77745
2025-03-09 11:36:37,464 - INFO - Batch 24901, Running Avg Loss: 4.77728
2025-03-09 11:36:37,480 - INFO - Batch 24900 finished
2025-03-09 11:36:37,480 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:36:55,070 - INFO - Batch 24926, Running Avg Loss: 4.77713
2025-03-09 11:37:12,718 - INFO - Batch 24951, Running Avg Loss: 4.77685
2025-03-09 11:37:30,287 - INFO - Batch 24976, Running Avg Loss: 4.77657
2025-03-09 11:37:47,787 - INFO - Batch 25001, Running Avg Loss: 4.77633
2025-03-09 11:37:47,805 - INFO - 
GPU Memory Stats at step 25000:
2025-03-09 11:37:47,805 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 11:37:47,806 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 11:37:47,806 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 11:37:47,806 - INFO - learning rate: 0.00000006
2025-03-09 11:37:47,806 - INFO - Ep 1 (Step 025000): Avg loss 4.776 | 102404096 tokens seen
2025-03-09 11:37:47,806 - INFO - optimizer lr: 0.00000006
2025-03-09 11:37:47,806 - INFO - scheduler lr: 0.00000006
2025-03-09 11:37:47,806 - INFO - Selected prompt: Meet Chris, a superhero of supplies! Just like how Batman protects Gotham City
2025-03-09 11:37:47,806 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:37:47,807 - INFO - random_topk: 5
2025-03-09 11:37:47,807 - INFO - random_temperature: 0.7867030078947775
2025-03-09 11:37:47,807 - INFO - global step 25000 , batch_idx 25000 => generating text
2025-03-09 11:37:47,807 - INFO - Generating on device cuda
2025-03-09 11:38:21,718 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:38:21,719 - INFO - Meet Chris, a superhero of supplies! Just like how Batman protects Gotham City, he had a unique way of learning about the world. She was known for his work, and she had just how to help her understand and grow up with the world.

One day, a young girl named Max, who was born in a small town called "Cikia," a young woman named Max. He asked her mom, "What is this?" Her mom explained that he had to do this by looking at the right side of the group.

As he walked, "I can't know what it's wrong. I'm so happy to make my own lemonade stand out. But I don't know what you're doing here. We must always ask you what you need to do.

"What does 'S' mean, my friend, I think we want to know how to keep our bodies safe."

"Wow!" exclaimed Benny, "What do you need to do with my friend?"

Lily smiled and replied, "But why do we want to do with such a particular situation? That's right! Let me show you a question about what you want to know."

The team continued to find a problem, which made the best part to make a good way to help people. They would choose to know
2025-03-09 11:38:21,719 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:38:45,679 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_25000_steps_avg_loss_4.77633_optimizer_lr_0.00000006.pth
2025-03-09 11:38:45,956 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 11:38:45,956 - INFO - Batch 25000 finished
2025-03-09 11:38:45,956 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:39:03,219 - INFO - Batch 25026, Running Avg Loss: 4.77616
2025-03-09 11:39:20,786 - INFO - Batch 25051, Running Avg Loss: 4.77596
2025-03-09 11:39:38,348 - INFO - Batch 25076, Running Avg Loss: 4.77575
2025-03-09 11:39:55,983 - INFO - Batch 25101, Running Avg Loss: 4.77559
2025-03-09 11:39:56,002 - INFO - Batch 25100 finished
2025-03-09 11:39:56,002 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:40:13,583 - INFO - Batch 25126, Running Avg Loss: 4.77555
2025-03-09 11:40:31,234 - INFO - Batch 25151, Running Avg Loss: 4.77537
2025-03-09 11:40:48,661 - INFO - Batch 25176, Running Avg Loss: 4.77519
2025-03-09 11:41:06,173 - INFO - Batch 25201, Running Avg Loss: 4.77499
2025-03-09 11:41:06,186 - INFO - Batch 25200 finished
2025-03-09 11:41:06,186 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:41:23,473 - INFO - Batch 25226, Running Avg Loss: 4.77484
2025-03-09 11:41:41,112 - INFO - Batch 25251, Running Avg Loss: 4.77471
2025-03-09 11:41:58,573 - INFO - Batch 25276, Running Avg Loss: 4.77453
2025-03-09 11:42:15,994 - INFO - Batch 25301, Running Avg Loss: 4.77439
2025-03-09 11:42:16,010 - INFO - Batch 25300 finished
2025-03-09 11:42:16,010 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:42:33,386 - INFO - Batch 25326, Running Avg Loss: 4.77423
2025-03-09 11:42:50,866 - INFO - Batch 25351, Running Avg Loss: 4.77397
2025-03-09 11:43:08,356 - INFO - Batch 25376, Running Avg Loss: 4.77384
2025-03-09 11:43:25,828 - INFO - Batch 25401, Running Avg Loss: 4.77366
2025-03-09 11:43:25,843 - INFO - Batch 25400 finished
2025-03-09 11:43:25,844 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:43:43,390 - INFO - Batch 25426, Running Avg Loss: 4.77347
2025-03-09 11:44:00,882 - INFO - Batch 25451, Running Avg Loss: 4.77331
2025-03-09 11:44:18,433 - INFO - Batch 25476, Running Avg Loss: 4.77315
2025-03-09 11:44:35,886 - INFO - Batch 25501, Running Avg Loss: 4.77298
2025-03-09 11:44:35,903 - INFO - Batch 25500 finished
2025-03-09 11:44:35,904 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:44:53,335 - INFO - Batch 25526, Running Avg Loss: 4.77278
2025-03-09 11:45:10,892 - INFO - Batch 25551, Running Avg Loss: 4.77262
2025-03-09 11:45:28,448 - INFO - Batch 25576, Running Avg Loss: 4.77250
2025-03-09 11:45:46,110 - INFO - Batch 25601, Running Avg Loss: 4.77238
2025-03-09 11:45:46,130 - INFO - Batch 25600 finished
2025-03-09 11:45:46,130 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:46:03,749 - INFO - Batch 25626, Running Avg Loss: 4.77218
2025-03-09 11:46:21,332 - INFO - Batch 25651, Running Avg Loss: 4.77200
2025-03-09 11:46:39,024 - INFO - Batch 25676, Running Avg Loss: 4.77173
2025-03-09 11:46:56,676 - INFO - Batch 25701, Running Avg Loss: 4.77153
2025-03-09 11:46:56,694 - INFO - Batch 25700 finished
2025-03-09 11:46:56,695 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:47:14,384 - INFO - Batch 25726, Running Avg Loss: 4.77135
2025-03-09 11:47:31,999 - INFO - Batch 25751, Running Avg Loss: 4.77118
2025-03-09 11:47:49,559 - INFO - Batch 25776, Running Avg Loss: 4.77094
2025-03-09 11:48:07,135 - INFO - Batch 25801, Running Avg Loss: 4.77076
2025-03-09 11:48:07,153 - INFO - Batch 25800 finished
2025-03-09 11:48:07,153 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:48:24,798 - INFO - Batch 25826, Running Avg Loss: 4.77065
2025-03-09 11:48:42,356 - INFO - Batch 25851, Running Avg Loss: 4.77047
2025-03-09 11:48:59,865 - INFO - Batch 25876, Running Avg Loss: 4.77023
2025-03-09 11:49:17,333 - INFO - Batch 25901, Running Avg Loss: 4.77005
2025-03-09 11:49:17,352 - INFO - Batch 25900 finished
2025-03-09 11:49:17,353 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:49:34,890 - INFO - Batch 25926, Running Avg Loss: 4.76985
2025-03-09 11:49:52,456 - INFO - Batch 25951, Running Avg Loss: 4.76961
2025-03-09 11:50:10,044 - INFO - Batch 25976, Running Avg Loss: 4.76944
2025-03-09 11:50:27,587 - INFO - Batch 26001, Running Avg Loss: 4.76933
2025-03-09 11:50:27,603 - INFO - Batch 26000 finished
2025-03-09 11:50:27,603 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:50:45,140 - INFO - Batch 26026, Running Avg Loss: 4.76913
2025-03-09 11:51:02,447 - INFO - Batch 26051, Running Avg Loss: 4.76900
2025-03-09 11:51:19,811 - INFO - Batch 26076, Running Avg Loss: 4.76873
2025-03-09 11:51:37,242 - INFO - Batch 26101, Running Avg Loss: 4.76845
2025-03-09 11:51:37,258 - INFO - Batch 26100 finished
2025-03-09 11:51:37,259 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:51:54,869 - INFO - Batch 26126, Running Avg Loss: 4.76838
2025-03-09 11:52:12,306 - INFO - Batch 26151, Running Avg Loss: 4.76811
2025-03-09 11:52:29,654 - INFO - Batch 26176, Running Avg Loss: 4.76786
2025-03-09 11:52:46,969 - INFO - Batch 26201, Running Avg Loss: 4.76770
2025-03-09 11:52:46,988 - INFO - Batch 26200 finished
2025-03-09 11:52:46,988 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:53:04,470 - INFO - Batch 26226, Running Avg Loss: 4.76762
2025-03-09 11:53:22,015 - INFO - Batch 26251, Running Avg Loss: 4.76753
2025-03-09 11:53:39,749 - INFO - Batch 26276, Running Avg Loss: 4.76739
2025-03-09 11:53:57,282 - INFO - Batch 26301, Running Avg Loss: 4.76726
2025-03-09 11:53:57,298 - INFO - Batch 26300 finished
2025-03-09 11:53:57,299 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:54:15,001 - INFO - Batch 26326, Running Avg Loss: 4.76714
2025-03-09 11:54:32,548 - INFO - Batch 26351, Running Avg Loss: 4.76690
2025-03-09 11:54:50,108 - INFO - Batch 26376, Running Avg Loss: 4.76670
2025-03-09 11:55:07,556 - INFO - Batch 26401, Running Avg Loss: 4.76647
2025-03-09 11:55:07,572 - INFO - Batch 26400 finished
2025-03-09 11:55:07,573 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:55:25,046 - INFO - Batch 26426, Running Avg Loss: 4.76627
2025-03-09 11:55:42,691 - INFO - Batch 26451, Running Avg Loss: 4.76608
2025-03-09 11:56:00,184 - INFO - Batch 26476, Running Avg Loss: 4.76593
2025-03-09 11:56:17,749 - INFO - Batch 26501, Running Avg Loss: 4.76574
2025-03-09 11:56:17,766 - INFO - Batch 26500 finished
2025-03-09 11:56:17,766 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:56:35,313 - INFO - Batch 26526, Running Avg Loss: 4.76570
2025-03-09 11:56:52,883 - INFO - Batch 26551, Running Avg Loss: 4.76546
2025-03-09 11:57:10,421 - INFO - Batch 26576, Running Avg Loss: 4.76531
2025-03-09 11:57:27,971 - INFO - Batch 26601, Running Avg Loss: 4.76510
2025-03-09 11:57:27,989 - INFO - Batch 26600 finished
2025-03-09 11:57:27,990 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:57:45,481 - INFO - Batch 26626, Running Avg Loss: 4.76486
2025-03-09 11:58:03,133 - INFO - Batch 26651, Running Avg Loss: 4.76476
2025-03-09 11:58:20,656 - INFO - Batch 26676, Running Avg Loss: 4.76461
2025-03-09 11:58:38,178 - INFO - Batch 26701, Running Avg Loss: 4.76449
2025-03-09 11:58:38,197 - INFO - Batch 26700 finished
2025-03-09 11:58:38,198 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 11:58:55,707 - INFO - Batch 26726, Running Avg Loss: 4.76438
2025-03-09 11:59:13,206 - INFO - Batch 26751, Running Avg Loss: 4.76426
2025-03-09 11:59:30,645 - INFO - Batch 26776, Running Avg Loss: 4.76410
2025-03-09 11:59:48,124 - INFO - Batch 26801, Running Avg Loss: 4.76396
2025-03-09 11:59:48,144 - INFO - Batch 26800 finished
2025-03-09 11:59:48,144 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:00:05,605 - INFO - Batch 26826, Running Avg Loss: 4.76371
2025-03-09 12:00:23,102 - INFO - Batch 26851, Running Avg Loss: 4.76358
2025-03-09 12:00:40,400 - INFO - Batch 26876, Running Avg Loss: 4.76328
2025-03-09 12:00:57,584 - INFO - Batch 26901, Running Avg Loss: 4.76312
2025-03-09 12:00:57,596 - INFO - Batch 26900 finished
2025-03-09 12:00:57,596 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:01:14,802 - INFO - Batch 26926, Running Avg Loss: 4.76297
2025-03-09 12:01:32,255 - INFO - Batch 26951, Running Avg Loss: 4.76273
2025-03-09 12:01:49,678 - INFO - Batch 26976, Running Avg Loss: 4.76261
2025-03-09 12:02:07,297 - INFO - Batch 27001, Running Avg Loss: 4.76249
2025-03-09 12:02:07,315 - INFO - Batch 27000 finished
2025-03-09 12:02:07,315 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:02:25,003 - INFO - Batch 27026, Running Avg Loss: 4.76231
2025-03-09 12:02:42,632 - INFO - Batch 27051, Running Avg Loss: 4.76208
2025-03-09 12:03:00,152 - INFO - Batch 27076, Running Avg Loss: 4.76192
2025-03-09 12:03:17,652 - INFO - Batch 27101, Running Avg Loss: 4.76176
2025-03-09 12:03:17,670 - INFO - Batch 27100 finished
2025-03-09 12:03:17,671 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:03:35,130 - INFO - Batch 27126, Running Avg Loss: 4.76158
2025-03-09 12:03:52,787 - INFO - Batch 27151, Running Avg Loss: 4.76137
2025-03-09 12:04:10,443 - INFO - Batch 27176, Running Avg Loss: 4.76125
2025-03-09 12:04:28,000 - INFO - Batch 27201, Running Avg Loss: 4.76109
2025-03-09 12:04:28,017 - INFO - Batch 27200 finished
2025-03-09 12:04:28,018 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:04:45,698 - INFO - Batch 27226, Running Avg Loss: 4.76091
2025-03-09 12:05:03,186 - INFO - Batch 27251, Running Avg Loss: 4.76073
2025-03-09 12:05:20,718 - INFO - Batch 27276, Running Avg Loss: 4.76063
2025-03-09 12:05:38,200 - INFO - Batch 27301, Running Avg Loss: 4.76045
2025-03-09 12:05:38,216 - INFO - Batch 27300 finished
2025-03-09 12:05:38,217 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:05:55,727 - INFO - Batch 27326, Running Avg Loss: 4.76025
2025-03-09 12:06:13,261 - INFO - Batch 27351, Running Avg Loss: 4.76007
2025-03-09 12:06:30,782 - INFO - Batch 27376, Running Avg Loss: 4.75983
2025-03-09 12:06:48,608 - INFO - Batch 27401, Running Avg Loss: 4.75971
2025-03-09 12:06:48,626 - INFO - Batch 27400 finished
2025-03-09 12:06:48,627 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:07:06,246 - INFO - Batch 27426, Running Avg Loss: 4.75958
2025-03-09 12:07:23,795 - INFO - Batch 27451, Running Avg Loss: 4.75932
2025-03-09 12:07:41,272 - INFO - Batch 27476, Running Avg Loss: 4.75918
2025-03-09 12:07:58,964 - INFO - Batch 27501, Running Avg Loss: 4.75893
2025-03-09 12:07:58,979 - INFO - 
GPU Memory Stats at step 27500:
2025-03-09 12:07:58,979 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 12:07:58,979 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 12:07:58,979 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 12:07:58,979 - INFO - learning rate: 0.00000006
2025-03-09 12:07:58,979 - INFO - Ep 1 (Step 027500): Avg loss 4.759 | 112644096 tokens seen
2025-03-09 12:07:58,979 - INFO - optimizer lr: 0.00000006
2025-03-09 12:07:58,979 - INFO - scheduler lr: 0.00000006
2025-03-09 12:07:58,980 - INFO - Selected prompt: Meet Chris, a superhero of supplies! Just like how Batman protects Gotham City
2025-03-09 12:07:58,980 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:07:58,981 - INFO - random_topk: 4
2025-03-09 12:07:58,982 - INFO - random_temperature: 0.804933716613633
2025-03-09 12:07:58,982 - INFO - global step 27500 , batch_idx 27500 => generating text
2025-03-09 12:07:58,982 - INFO - Generating on device cuda
2025-03-09 12:08:33,129 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:08:33,129 - INFO - Meet Chris, a superhero of supplies! Just like how Batman protects Gotham City, he had a special way of learning about the world around them.

As they continued exploring the world of art and culture, they decided to explore different cultures and their unique culture. Some people had different experiences, like the people of them, while others were very important. One such group of people was the "The Gira" and "Cereap," which took place in the United States.

One day, while walking along the river, they noticed something unusual about the world. They would use the "the first five books" (or "C" (or "The Gira," which was a group of people who wanted to take action and work towards the world around them.

But wait, there were so many things that were so important. They wanted to create a beautiful community where everyone wanted to share their experiences and share their own experiences.

One day, they decided to take action to help people who wanted to share their thoughts and feelings. One day, they decided to create a new book called "The Hira" and "The Bia of the Gest" (or the G.S., 2014).

As they continued exploring and learning new things, they learned about the importance of being
2025-03-09 12:08:33,129 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:08:56,976 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_27500_steps_avg_loss_4.75893_optimizer_lr_0.00000006.pth
2025-03-09 12:08:57,199 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 12:08:57,199 - INFO - Batch 27500 finished
2025-03-09 12:08:57,199 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:09:14,465 - INFO - Batch 27526, Running Avg Loss: 4.75878
2025-03-09 12:09:31,889 - INFO - Batch 27551, Running Avg Loss: 4.75863
2025-03-09 12:09:49,506 - INFO - Batch 27576, Running Avg Loss: 4.75848
2025-03-09 12:10:06,945 - INFO - Batch 27601, Running Avg Loss: 4.75833
2025-03-09 12:10:06,961 - INFO - Batch 27600 finished
2025-03-09 12:10:06,962 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:10:24,423 - INFO - Batch 27626, Running Avg Loss: 4.75823
2025-03-09 12:10:41,938 - INFO - Batch 27651, Running Avg Loss: 4.75806
2025-03-09 12:10:59,480 - INFO - Batch 27676, Running Avg Loss: 4.75795
2025-03-09 12:11:17,069 - INFO - Batch 27701, Running Avg Loss: 4.75776
2025-03-09 12:11:17,085 - INFO - Batch 27700 finished
2025-03-09 12:11:17,086 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:11:34,593 - INFO - Batch 27726, Running Avg Loss: 4.75761
2025-03-09 12:11:52,053 - INFO - Batch 27751, Running Avg Loss: 4.75751
2025-03-09 12:12:09,547 - INFO - Batch 27776, Running Avg Loss: 4.75734
2025-03-09 12:12:27,042 - INFO - Batch 27801, Running Avg Loss: 4.75717
2025-03-09 12:12:27,057 - INFO - Batch 27800 finished
2025-03-09 12:12:27,057 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:12:44,556 - INFO - Batch 27826, Running Avg Loss: 4.75704
2025-03-09 12:13:02,153 - INFO - Batch 27851, Running Avg Loss: 4.75688
2025-03-09 12:13:19,646 - INFO - Batch 27876, Running Avg Loss: 4.75670
2025-03-09 12:13:37,101 - INFO - Batch 27901, Running Avg Loss: 4.75657
2025-03-09 12:13:37,116 - INFO - Batch 27900 finished
2025-03-09 12:13:37,116 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:13:54,562 - INFO - Batch 27926, Running Avg Loss: 4.75641
2025-03-09 12:14:12,028 - INFO - Batch 27951, Running Avg Loss: 4.75621
2025-03-09 12:14:29,488 - INFO - Batch 27976, Running Avg Loss: 4.75608
2025-03-09 12:14:46,995 - INFO - Batch 28001, Running Avg Loss: 4.75598
2025-03-09 12:14:47,011 - INFO - Batch 28000 finished
2025-03-09 12:14:47,012 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:15:04,451 - INFO - Batch 28026, Running Avg Loss: 4.75578
2025-03-09 12:15:21,928 - INFO - Batch 28051, Running Avg Loss: 4.75572
2025-03-09 12:15:39,453 - INFO - Batch 28076, Running Avg Loss: 4.75555
2025-03-09 12:15:57,017 - INFO - Batch 28101, Running Avg Loss: 4.75541
2025-03-09 12:15:57,032 - INFO - Batch 28100 finished
2025-03-09 12:15:57,032 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:16:14,522 - INFO - Batch 28126, Running Avg Loss: 4.75527
2025-03-09 12:16:32,159 - INFO - Batch 28151, Running Avg Loss: 4.75517
2025-03-09 12:16:49,635 - INFO - Batch 28176, Running Avg Loss: 4.75507
2025-03-09 12:17:07,090 - INFO - Batch 28201, Running Avg Loss: 4.75489
2025-03-09 12:17:07,105 - INFO - Batch 28200 finished
2025-03-09 12:17:07,105 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:17:24,605 - INFO - Batch 28226, Running Avg Loss: 4.75475
2025-03-09 12:17:42,075 - INFO - Batch 28251, Running Avg Loss: 4.75453
2025-03-09 12:17:59,446 - INFO - Batch 28276, Running Avg Loss: 4.75441
2025-03-09 12:18:16,788 - INFO - Batch 28301, Running Avg Loss: 4.75425
2025-03-09 12:18:16,803 - INFO - Batch 28300 finished
2025-03-09 12:18:16,803 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:18:34,186 - INFO - Batch 28326, Running Avg Loss: 4.75412
2025-03-09 12:18:51,658 - INFO - Batch 28351, Running Avg Loss: 4.75396
2025-03-09 12:19:09,159 - INFO - Batch 28376, Running Avg Loss: 4.75384
2025-03-09 12:19:26,550 - INFO - Batch 28401, Running Avg Loss: 4.75370
2025-03-09 12:19:26,566 - INFO - Batch 28400 finished
2025-03-09 12:19:26,566 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:19:44,095 - INFO - Batch 28426, Running Avg Loss: 4.75367
2025-03-09 12:20:01,516 - INFO - Batch 28451, Running Avg Loss: 4.75355
2025-03-09 12:20:18,985 - INFO - Batch 28476, Running Avg Loss: 4.75338
2025-03-09 12:20:36,385 - INFO - Batch 28501, Running Avg Loss: 4.75320
2025-03-09 12:20:36,402 - INFO - Batch 28500 finished
2025-03-09 12:20:36,402 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:20:53,811 - INFO - Batch 28526, Running Avg Loss: 4.75307
2025-03-09 12:21:11,168 - INFO - Batch 28551, Running Avg Loss: 4.75289
2025-03-09 12:21:28,672 - INFO - Batch 28576, Running Avg Loss: 4.75273
2025-03-09 12:21:46,157 - INFO - Batch 28601, Running Avg Loss: 4.75258
2025-03-09 12:21:46,173 - INFO - Batch 28600 finished
2025-03-09 12:21:46,174 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:22:03,620 - INFO - Batch 28626, Running Avg Loss: 4.75246
2025-03-09 12:22:21,106 - INFO - Batch 28651, Running Avg Loss: 4.75233
2025-03-09 12:22:38,616 - INFO - Batch 28676, Running Avg Loss: 4.75213
2025-03-09 12:22:56,287 - INFO - Batch 28701, Running Avg Loss: 4.75199
2025-03-09 12:22:56,302 - INFO - Batch 28700 finished
2025-03-09 12:22:56,302 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:23:13,832 - INFO - Batch 28726, Running Avg Loss: 4.75184
2025-03-09 12:23:31,333 - INFO - Batch 28751, Running Avg Loss: 4.75161
2025-03-09 12:23:48,950 - INFO - Batch 28776, Running Avg Loss: 4.75151
2025-03-09 12:24:06,478 - INFO - Batch 28801, Running Avg Loss: 4.75146
2025-03-09 12:24:06,494 - INFO - Batch 28800 finished
2025-03-09 12:24:06,494 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:24:24,029 - INFO - Batch 28826, Running Avg Loss: 4.75127
2025-03-09 12:24:41,592 - INFO - Batch 28851, Running Avg Loss: 4.75116
2025-03-09 12:24:59,106 - INFO - Batch 28876, Running Avg Loss: 4.75094
2025-03-09 12:25:16,543 - INFO - Batch 28901, Running Avg Loss: 4.75079
2025-03-09 12:25:16,559 - INFO - Batch 28900 finished
2025-03-09 12:25:16,559 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:25:34,003 - INFO - Batch 28926, Running Avg Loss: 4.75068
2025-03-09 12:25:51,501 - INFO - Batch 28951, Running Avg Loss: 4.75052
2025-03-09 12:26:09,156 - INFO - Batch 28976, Running Avg Loss: 4.75044
2025-03-09 12:26:26,579 - INFO - Batch 29001, Running Avg Loss: 4.75030
2025-03-09 12:26:26,594 - INFO - Batch 29000 finished
2025-03-09 12:26:26,595 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:26:44,033 - INFO - Batch 29026, Running Avg Loss: 4.75015
2025-03-09 12:27:01,583 - INFO - Batch 29051, Running Avg Loss: 4.75002
2025-03-09 12:27:19,062 - INFO - Batch 29076, Running Avg Loss: 4.74990
2025-03-09 12:27:36,470 - INFO - Batch 29101, Running Avg Loss: 4.74976
2025-03-09 12:27:36,486 - INFO - Batch 29100 finished
2025-03-09 12:27:36,487 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:27:53,957 - INFO - Batch 29126, Running Avg Loss: 4.74963
2025-03-09 12:28:11,458 - INFO - Batch 29151, Running Avg Loss: 4.74941
2025-03-09 12:28:29,026 - INFO - Batch 29176, Running Avg Loss: 4.74929
2025-03-09 12:28:46,568 - INFO - Batch 29201, Running Avg Loss: 4.74926
2025-03-09 12:28:46,584 - INFO - Batch 29200 finished
2025-03-09 12:28:46,584 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:29:03,979 - INFO - Batch 29226, Running Avg Loss: 4.74908
2025-03-09 12:29:21,368 - INFO - Batch 29251, Running Avg Loss: 4.74899
2025-03-09 12:29:38,934 - INFO - Batch 29276, Running Avg Loss: 4.74873
2025-03-09 12:29:56,402 - INFO - Batch 29301, Running Avg Loss: 4.74859
2025-03-09 12:29:56,418 - INFO - Batch 29300 finished
2025-03-09 12:29:56,419 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:30:13,902 - INFO - Batch 29326, Running Avg Loss: 4.74845
2025-03-09 12:30:31,350 - INFO - Batch 29351, Running Avg Loss: 4.74828
2025-03-09 12:30:48,570 - INFO - Batch 29376, Running Avg Loss: 4.74818
2025-03-09 12:31:05,624 - INFO - Batch 29401, Running Avg Loss: 4.74801
2025-03-09 12:31:05,637 - INFO - Batch 29400 finished
2025-03-09 12:31:05,637 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:31:22,796 - INFO - Batch 29426, Running Avg Loss: 4.74781
2025-03-09 12:31:40,324 - INFO - Batch 29451, Running Avg Loss: 4.74765
2025-03-09 12:31:57,758 - INFO - Batch 29476, Running Avg Loss: 4.74749
2025-03-09 12:32:15,227 - INFO - Batch 29501, Running Avg Loss: 4.74738
2025-03-09 12:32:15,243 - INFO - Batch 29500 finished
2025-03-09 12:32:15,244 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:32:32,804 - INFO - Batch 29526, Running Avg Loss: 4.74727
2025-03-09 12:32:50,583 - INFO - Batch 29551, Running Avg Loss: 4.74715
2025-03-09 12:33:08,100 - INFO - Batch 29576, Running Avg Loss: 4.74695
2025-03-09 12:33:25,633 - INFO - Batch 29601, Running Avg Loss: 4.74688
2025-03-09 12:33:25,651 - INFO - Batch 29600 finished
2025-03-09 12:33:25,652 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:33:43,121 - INFO - Batch 29626, Running Avg Loss: 4.74670
2025-03-09 12:34:00,643 - INFO - Batch 29651, Running Avg Loss: 4.74651
2025-03-09 12:34:18,130 - INFO - Batch 29676, Running Avg Loss: 4.74637
2025-03-09 12:34:35,876 - INFO - Batch 29701, Running Avg Loss: 4.74626
2025-03-09 12:34:35,895 - INFO - Batch 29700 finished
2025-03-09 12:34:35,896 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:34:53,326 - INFO - Batch 29726, Running Avg Loss: 4.74613
2025-03-09 12:35:10,718 - INFO - Batch 29751, Running Avg Loss: 4.74600
2025-03-09 12:35:28,140 - INFO - Batch 29776, Running Avg Loss: 4.74591
2025-03-09 12:35:45,820 - INFO - Batch 29801, Running Avg Loss: 4.74578
2025-03-09 12:35:45,840 - INFO - Batch 29800 finished
2025-03-09 12:35:45,841 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:36:03,304 - INFO - Batch 29826, Running Avg Loss: 4.74560
2025-03-09 12:36:20,659 - INFO - Batch 29851, Running Avg Loss: 4.74552
2025-03-09 12:36:37,989 - INFO - Batch 29876, Running Avg Loss: 4.74538
2025-03-09 12:36:55,448 - INFO - Batch 29901, Running Avg Loss: 4.74526
2025-03-09 12:36:55,466 - INFO - Batch 29900 finished
2025-03-09 12:36:55,466 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:37:12,862 - INFO - Batch 29926, Running Avg Loss: 4.74517
2025-03-09 12:37:30,415 - INFO - Batch 29951, Running Avg Loss: 4.74499
2025-03-09 12:37:48,084 - INFO - Batch 29976, Running Avg Loss: 4.74479
2025-03-09 12:38:05,604 - INFO - Batch 30001, Running Avg Loss: 4.74471
2025-03-09 12:38:05,623 - INFO - 
GPU Memory Stats at step 30000:
2025-03-09 12:38:05,624 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 12:38:05,624 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 12:38:05,624 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 12:38:05,624 - INFO - learning rate: 0.00000005
2025-03-09 12:38:05,624 - INFO - Ep 1 (Step 030000): Avg loss 4.745 | 122884096 tokens seen
2025-03-09 12:38:05,624 - INFO - optimizer lr: 0.00000005
2025-03-09 12:38:05,624 - INFO - scheduler lr: 0.00000005
2025-03-09 12:38:05,624 - INFO - Selected prompt: In today's ever-evolving world, technology has become an integral part of our lives
2025-03-09 12:38:05,624 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:38:05,626 - INFO - random_topk: 5
2025-03-09 12:38:05,626 - INFO - random_temperature: 0.7444856141033531
2025-03-09 12:38:05,626 - INFO - global step 30000 , batch_idx 30000 => generating text
2025-03-09 12:38:05,626 - INFO - Generating on device cuda
2025-03-09 12:38:39,864 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:38:39,865 - INFO - In today's ever-evolving world, technology has become an integral part of our lives. With the power of the brain, we can create new technologies that can help us understand how different factors contribute to our health and overall health. This chapter delves into the fascinating world of software development, specifically focusing on the fascinating world of digital science.

First, let's talk about what we mean by "to-O-E-A-1900. It has been used for centuries, but it has a powerful tool for creating new materials. They are often used for large datasets to create and interact with each other, such as the internet, computers, and even computers!

Now, let's talk about some of the basics:

1. **What are the most popular video game-based medicine?**
2. **Bink-based materials**: A special form of energy that has been passed down through the same time. It's often used to find the most common type of software, like the "tap," which allows users to access the same data into one another.
3. **Serr-19**: The process of managing energy, energy, and performance, making it easy for us to see how much money you need to make.
4. **Sap-20-19**: A small
2025-03-09 12:38:39,865 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:39:06,976 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_30000_steps_avg_loss_4.74471_optimizer_lr_0.00000005.pth
2025-03-09 12:39:07,234 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 12:39:07,234 - INFO - Batch 30000 finished
2025-03-09 12:39:07,234 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:39:24,499 - INFO - Batch 30026, Running Avg Loss: 4.74457
2025-03-09 12:39:41,957 - INFO - Batch 30051, Running Avg Loss: 4.74443
2025-03-09 12:39:59,495 - INFO - Batch 30076, Running Avg Loss: 4.74434
2025-03-09 12:40:16,943 - INFO - Batch 30101, Running Avg Loss: 4.74425
2025-03-09 12:40:16,960 - INFO - Batch 30100 finished
2025-03-09 12:40:16,961 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:40:34,358 - INFO - Batch 30126, Running Avg Loss: 4.74403
2025-03-09 12:40:51,881 - INFO - Batch 30151, Running Avg Loss: 4.74386
2025-03-09 12:41:08,898 - INFO - Batch 30176, Running Avg Loss: 4.74373
2025-03-09 12:41:26,060 - INFO - Batch 30201, Running Avg Loss: 4.74351
2025-03-09 12:41:26,073 - INFO - Batch 30200 finished
2025-03-09 12:41:26,074 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:41:43,563 - INFO - Batch 30226, Running Avg Loss: 4.74339
2025-03-09 12:42:01,244 - INFO - Batch 30251, Running Avg Loss: 4.74328
2025-03-09 12:42:18,712 - INFO - Batch 30276, Running Avg Loss: 4.74315
2025-03-09 12:42:36,224 - INFO - Batch 30301, Running Avg Loss: 4.74298
2025-03-09 12:42:36,243 - INFO - Batch 30300 finished
2025-03-09 12:42:36,243 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:42:53,740 - INFO - Batch 30326, Running Avg Loss: 4.74287
2025-03-09 12:43:11,234 - INFO - Batch 30351, Running Avg Loss: 4.74272
2025-03-09 12:43:28,671 - INFO - Batch 30376, Running Avg Loss: 4.74256
2025-03-09 12:43:46,135 - INFO - Batch 30401, Running Avg Loss: 4.74243
2025-03-09 12:43:46,149 - INFO - Batch 30400 finished
2025-03-09 12:43:46,150 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:44:03,664 - INFO - Batch 30426, Running Avg Loss: 4.74233
2025-03-09 12:44:21,189 - INFO - Batch 30451, Running Avg Loss: 4.74225
2025-03-09 12:44:38,698 - INFO - Batch 30476, Running Avg Loss: 4.74216
2025-03-09 12:44:56,184 - INFO - Batch 30501, Running Avg Loss: 4.74201
2025-03-09 12:44:56,200 - INFO - Batch 30500 finished
2025-03-09 12:44:56,201 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:45:13,648 - INFO - Batch 30526, Running Avg Loss: 4.74189
2025-03-09 12:45:31,166 - INFO - Batch 30551, Running Avg Loss: 4.74175
2025-03-09 12:45:48,503 - INFO - Batch 30576, Running Avg Loss: 4.74162
2025-03-09 12:46:05,928 - INFO - Batch 30601, Running Avg Loss: 4.74152
2025-03-09 12:46:05,943 - INFO - Batch 30600 finished
2025-03-09 12:46:05,943 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:46:23,386 - INFO - Batch 30626, Running Avg Loss: 4.74145
2025-03-09 12:46:40,954 - INFO - Batch 30651, Running Avg Loss: 4.74122
2025-03-09 12:46:58,546 - INFO - Batch 30676, Running Avg Loss: 4.74109
2025-03-09 12:47:15,923 - INFO - Batch 30701, Running Avg Loss: 4.74094
2025-03-09 12:47:15,939 - INFO - Batch 30700 finished
2025-03-09 12:47:15,940 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:47:33,254 - INFO - Batch 30726, Running Avg Loss: 4.74085
2025-03-09 12:47:50,619 - INFO - Batch 30751, Running Avg Loss: 4.74067
2025-03-09 12:48:08,147 - INFO - Batch 30776, Running Avg Loss: 4.74052
2025-03-09 12:48:25,703 - INFO - Batch 30801, Running Avg Loss: 4.74040
2025-03-09 12:48:25,719 - INFO - Batch 30800 finished
2025-03-09 12:48:25,719 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:48:43,354 - INFO - Batch 30826, Running Avg Loss: 4.74026
2025-03-09 12:49:00,798 - INFO - Batch 30851, Running Avg Loss: 4.74015
2025-03-09 12:49:18,261 - INFO - Batch 30876, Running Avg Loss: 4.74001
2025-03-09 12:49:35,742 - INFO - Batch 30901, Running Avg Loss: 4.73981
2025-03-09 12:49:35,756 - INFO - Batch 30900 finished
2025-03-09 12:49:35,756 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:49:53,248 - INFO - Batch 30926, Running Avg Loss: 4.73966
2025-03-09 12:50:10,836 - INFO - Batch 30951, Running Avg Loss: 4.73958
2025-03-09 12:50:28,415 - INFO - Batch 30976, Running Avg Loss: 4.73948
2025-03-09 12:50:45,872 - INFO - Batch 31001, Running Avg Loss: 4.73932
2025-03-09 12:50:45,885 - INFO - Batch 31000 finished
2025-03-09 12:50:45,885 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:51:03,024 - INFO - Batch 31026, Running Avg Loss: 4.73922
2025-03-09 12:51:20,236 - INFO - Batch 31051, Running Avg Loss: 4.73914
2025-03-09 12:51:37,625 - INFO - Batch 31076, Running Avg Loss: 4.73905
2025-03-09 12:51:55,135 - INFO - Batch 31101, Running Avg Loss: 4.73892
2025-03-09 12:51:55,152 - INFO - Batch 31100 finished
2025-03-09 12:51:55,153 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:52:12,933 - INFO - Batch 31126, Running Avg Loss: 4.73883
2025-03-09 12:52:30,593 - INFO - Batch 31151, Running Avg Loss: 4.73875
2025-03-09 12:52:48,278 - INFO - Batch 31176, Running Avg Loss: 4.73864
2025-03-09 12:53:05,964 - INFO - Batch 31201, Running Avg Loss: 4.73852
2025-03-09 12:53:05,984 - INFO - Batch 31200 finished
2025-03-09 12:53:05,985 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:53:23,575 - INFO - Batch 31226, Running Avg Loss: 4.73841
2025-03-09 12:53:41,068 - INFO - Batch 31251, Running Avg Loss: 4.73830
2025-03-09 12:53:58,689 - INFO - Batch 31276, Running Avg Loss: 4.73816
2025-03-09 12:54:16,250 - INFO - Batch 31301, Running Avg Loss: 4.73803
2025-03-09 12:54:16,266 - INFO - Batch 31300 finished
2025-03-09 12:54:16,267 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:54:33,655 - INFO - Batch 31326, Running Avg Loss: 4.73791
2025-03-09 12:54:51,018 - INFO - Batch 31351, Running Avg Loss: 4.73777
2025-03-09 12:55:08,675 - INFO - Batch 31376, Running Avg Loss: 4.73764
2025-03-09 12:55:26,287 - INFO - Batch 31401, Running Avg Loss: 4.73754
2025-03-09 12:55:26,306 - INFO - Batch 31400 finished
2025-03-09 12:55:26,307 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:55:43,872 - INFO - Batch 31426, Running Avg Loss: 4.73743
2025-03-09 12:56:01,349 - INFO - Batch 31451, Running Avg Loss: 4.73732
2025-03-09 12:56:18,793 - INFO - Batch 31476, Running Avg Loss: 4.73716
2025-03-09 12:56:36,444 - INFO - Batch 31501, Running Avg Loss: 4.73706
2025-03-09 12:56:36,463 - INFO - Batch 31500 finished
2025-03-09 12:56:36,464 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:56:54,147 - INFO - Batch 31526, Running Avg Loss: 4.73695
2025-03-09 12:57:11,749 - INFO - Batch 31551, Running Avg Loss: 4.73677
2025-03-09 12:57:29,218 - INFO - Batch 31576, Running Avg Loss: 4.73666
2025-03-09 12:57:46,806 - INFO - Batch 31601, Running Avg Loss: 4.73650
2025-03-09 12:57:46,823 - INFO - Batch 31600 finished
2025-03-09 12:57:46,824 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:58:04,413 - INFO - Batch 31626, Running Avg Loss: 4.73634
2025-03-09 12:58:22,122 - INFO - Batch 31651, Running Avg Loss: 4.73624
2025-03-09 12:58:39,666 - INFO - Batch 31676, Running Avg Loss: 4.73612
2025-03-09 12:58:57,115 - INFO - Batch 31701, Running Avg Loss: 4.73599
2025-03-09 12:58:57,128 - INFO - Batch 31700 finished
2025-03-09 12:58:57,129 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 12:59:14,431 - INFO - Batch 31726, Running Avg Loss: 4.73589
2025-03-09 12:59:32,004 - INFO - Batch 31751, Running Avg Loss: 4.73572
2025-03-09 12:59:49,480 - INFO - Batch 31776, Running Avg Loss: 4.73559
2025-03-09 13:00:06,976 - INFO - Batch 31801, Running Avg Loss: 4.73547
2025-03-09 13:00:06,990 - INFO - Batch 31800 finished
2025-03-09 13:00:06,991 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:00:24,309 - INFO - Batch 31826, Running Avg Loss: 4.73541
2025-03-09 13:00:41,895 - INFO - Batch 31851, Running Avg Loss: 4.73535
2025-03-09 13:00:59,257 - INFO - Batch 31876, Running Avg Loss: 4.73519
2025-03-09 13:01:16,496 - INFO - Batch 31901, Running Avg Loss: 4.73505
2025-03-09 13:01:16,509 - INFO - Batch 31900 finished
2025-03-09 13:01:16,509 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:01:33,781 - INFO - Batch 31926, Running Avg Loss: 4.73495
2025-03-09 13:01:51,411 - INFO - Batch 31951, Running Avg Loss: 4.73483
2025-03-09 13:02:09,094 - INFO - Batch 31976, Running Avg Loss: 4.73466
2025-03-09 13:02:26,683 - INFO - Batch 32001, Running Avg Loss: 4.73454
2025-03-09 13:02:26,699 - INFO - Batch 32000 finished
2025-03-09 13:02:26,700 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:02:44,134 - INFO - Batch 32026, Running Avg Loss: 4.73439
2025-03-09 13:03:01,651 - INFO - Batch 32051, Running Avg Loss: 4.73430
2025-03-09 13:03:19,093 - INFO - Batch 32076, Running Avg Loss: 4.73423
2025-03-09 13:03:36,546 - INFO - Batch 32101, Running Avg Loss: 4.73411
2025-03-09 13:03:36,563 - INFO - Batch 32100 finished
2025-03-09 13:03:36,563 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:03:54,087 - INFO - Batch 32126, Running Avg Loss: 4.73402
2025-03-09 13:04:11,718 - INFO - Batch 32151, Running Avg Loss: 4.73385
2025-03-09 13:04:29,334 - INFO - Batch 32176, Running Avg Loss: 4.73361
2025-03-09 13:04:46,791 - INFO - Batch 32201, Running Avg Loss: 4.73355
2025-03-09 13:04:46,810 - INFO - Batch 32200 finished
2025-03-09 13:04:46,810 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:05:04,386 - INFO - Batch 32226, Running Avg Loss: 4.73343
2025-03-09 13:05:21,846 - INFO - Batch 32251, Running Avg Loss: 4.73333
2025-03-09 13:05:39,384 - INFO - Batch 32276, Running Avg Loss: 4.73321
2025-03-09 13:05:56,899 - INFO - Batch 32301, Running Avg Loss: 4.73310
2025-03-09 13:05:56,915 - INFO - Batch 32300 finished
2025-03-09 13:05:56,915 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:06:14,307 - INFO - Batch 32326, Running Avg Loss: 4.73308
2025-03-09 13:06:31,746 - INFO - Batch 32351, Running Avg Loss: 4.73299
2025-03-09 13:06:49,168 - INFO - Batch 32376, Running Avg Loss: 4.73285
2025-03-09 13:07:06,799 - INFO - Batch 32401, Running Avg Loss: 4.73274
2025-03-09 13:07:06,816 - INFO - Batch 32400 finished
2025-03-09 13:07:06,816 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:07:24,310 - INFO - Batch 32426, Running Avg Loss: 4.73255
2025-03-09 13:07:41,733 - INFO - Batch 32451, Running Avg Loss: 4.73241
2025-03-09 13:07:59,153 - INFO - Batch 32476, Running Avg Loss: 4.73229
2025-03-09 13:08:16,753 - INFO - Batch 32501, Running Avg Loss: 4.73220
2025-03-09 13:08:16,768 - INFO - 
GPU Memory Stats at step 32500:
2025-03-09 13:08:16,769 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 13:08:16,769 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 13:08:16,769 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 13:08:16,769 - INFO - learning rate: 0.00000004
2025-03-09 13:08:16,769 - INFO - Ep 1 (Step 032500): Avg loss 4.732 | 133124096 tokens seen
2025-03-09 13:08:16,769 - INFO - optimizer lr: 0.00000004
2025-03-09 13:08:16,769 - INFO - scheduler lr: 0.00000004
2025-03-09 13:08:16,770 - INFO - Selected prompt: Meet Chris, a superhero of supplies! Just like how Batman protects Gotham City
2025-03-09 13:08:16,770 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:08:16,770 - INFO - random_topk: 5
2025-03-09 13:08:16,770 - INFO - random_temperature: 0.7998588393576326
2025-03-09 13:08:16,770 - INFO - global step 32500 , batch_idx 32500 => generating text
2025-03-09 13:08:16,771 - INFO - Generating on device cuda
2025-03-09 13:08:50,225 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:08:50,225 - INFO - Meet Chris, a superhero of supplies! Just like how Batman protects Gotham City, he's a great idea - it's called "The Hink." This story of art, history, culture, and culture, all around the globe.

One of the earliest figures in the United States was the "A" series "The Holi" by the 1800s. He was born in 1888, which had many people of the world, but they were just one of the most powerful figures in the world.

But then came the story! Did you know that the "Sad" refers to a group of people who have been like you? That's right! So, when you visit a big book, you will discover that there's something called the "Aang."

The term "The Great" was a group of artists who wanted to create beautiful and captivating stories. One day, a group of people had to create a beautiful city called the "The Gira" series. He wanted to create a special place where everyone had to learn and share their own stories. They decided to create a story about the story.

Now, let me tell you about some interesting facts about the world of this thrilling story. Imagine a big city filled with vibrant colors and patterns that were made
2025-03-09 13:08:50,225 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:09:17,522 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_32500_steps_avg_loss_4.73220_optimizer_lr_0.00000004.pth
2025-03-09 13:09:17,770 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 13:09:17,770 - INFO - Batch 32500 finished
2025-03-09 13:09:17,770 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:09:35,384 - INFO - Batch 32526, Running Avg Loss: 4.73210
2025-03-09 13:09:52,946 - INFO - Batch 32551, Running Avg Loss: 4.73196
2025-03-09 13:10:10,595 - INFO - Batch 32576, Running Avg Loss: 4.73179
2025-03-09 13:10:28,088 - INFO - Batch 32601, Running Avg Loss: 4.73165
2025-03-09 13:10:28,107 - INFO - Batch 32600 finished
2025-03-09 13:10:28,107 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:10:45,630 - INFO - Batch 32626, Running Avg Loss: 4.73151
2025-03-09 13:11:03,193 - INFO - Batch 32651, Running Avg Loss: 4.73143
2025-03-09 13:11:20,645 - INFO - Batch 32676, Running Avg Loss: 4.73134
2025-03-09 13:11:38,253 - INFO - Batch 32701, Running Avg Loss: 4.73113
2025-03-09 13:11:38,272 - INFO - Batch 32700 finished
2025-03-09 13:11:38,272 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:11:55,793 - INFO - Batch 32726, Running Avg Loss: 4.73104
2025-03-09 13:12:13,312 - INFO - Batch 32751, Running Avg Loss: 4.73099
2025-03-09 13:12:30,834 - INFO - Batch 32776, Running Avg Loss: 4.73093
2025-03-09 13:12:48,399 - INFO - Batch 32801, Running Avg Loss: 4.73075
2025-03-09 13:12:48,417 - INFO - Batch 32800 finished
2025-03-09 13:12:48,418 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:13:05,882 - INFO - Batch 32826, Running Avg Loss: 4.73069
2025-03-09 13:13:23,539 - INFO - Batch 32851, Running Avg Loss: 4.73060
2025-03-09 13:13:41,133 - INFO - Batch 32876, Running Avg Loss: 4.73050
2025-03-09 13:13:58,679 - INFO - Batch 32901, Running Avg Loss: 4.73035
2025-03-09 13:13:58,695 - INFO - Batch 32900 finished
2025-03-09 13:13:58,696 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:14:16,294 - INFO - Batch 32926, Running Avg Loss: 4.73022
2025-03-09 13:14:33,928 - INFO - Batch 32951, Running Avg Loss: 4.73010
2025-03-09 13:14:51,591 - INFO - Batch 32976, Running Avg Loss: 4.73000
2025-03-09 13:15:09,231 - INFO - Batch 33001, Running Avg Loss: 4.72990
2025-03-09 13:15:09,247 - INFO - Batch 33000 finished
2025-03-09 13:15:09,247 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:15:26,796 - INFO - Batch 33026, Running Avg Loss: 4.72979
2025-03-09 13:15:44,291 - INFO - Batch 33051, Running Avg Loss: 4.72968
2025-03-09 13:16:01,828 - INFO - Batch 33076, Running Avg Loss: 4.72958
2025-03-09 13:16:19,402 - INFO - Batch 33101, Running Avg Loss: 4.72952
2025-03-09 13:16:19,420 - INFO - Batch 33100 finished
2025-03-09 13:16:19,420 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:16:36,857 - INFO - Batch 33126, Running Avg Loss: 4.72943
2025-03-09 13:16:54,484 - INFO - Batch 33151, Running Avg Loss: 4.72935
2025-03-09 13:17:12,045 - INFO - Batch 33176, Running Avg Loss: 4.72917
2025-03-09 13:17:29,571 - INFO - Batch 33201, Running Avg Loss: 4.72910
2025-03-09 13:17:29,587 - INFO - Batch 33200 finished
2025-03-09 13:17:29,587 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:17:47,096 - INFO - Batch 33226, Running Avg Loss: 4.72898
2025-03-09 13:18:04,581 - INFO - Batch 33251, Running Avg Loss: 4.72884
2025-03-09 13:18:22,076 - INFO - Batch 33276, Running Avg Loss: 4.72876
2025-03-09 13:18:39,598 - INFO - Batch 33301, Running Avg Loss: 4.72863
2025-03-09 13:18:39,613 - INFO - Batch 33300 finished
2025-03-09 13:18:39,614 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:18:57,058 - INFO - Batch 33326, Running Avg Loss: 4.72848
2025-03-09 13:19:14,520 - INFO - Batch 33351, Running Avg Loss: 4.72832
2025-03-09 13:19:31,991 - INFO - Batch 33376, Running Avg Loss: 4.72825
2025-03-09 13:19:49,476 - INFO - Batch 33401, Running Avg Loss: 4.72817
2025-03-09 13:19:49,493 - INFO - Batch 33400 finished
2025-03-09 13:19:49,493 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:20:07,152 - INFO - Batch 33426, Running Avg Loss: 4.72816
2025-03-09 13:20:24,680 - INFO - Batch 33451, Running Avg Loss: 4.72801
2025-03-09 13:20:42,153 - INFO - Batch 33476, Running Avg Loss: 4.72787
2025-03-09 13:20:59,574 - INFO - Batch 33501, Running Avg Loss: 4.72777
2025-03-09 13:20:59,590 - INFO - Batch 33500 finished
2025-03-09 13:20:59,591 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:21:16,934 - INFO - Batch 33526, Running Avg Loss: 4.72758
2025-03-09 13:21:34,328 - INFO - Batch 33551, Running Avg Loss: 4.72751
2025-03-09 13:21:51,984 - INFO - Batch 33576, Running Avg Loss: 4.72744
2025-03-09 13:22:09,356 - INFO - Batch 33601, Running Avg Loss: 4.72722
2025-03-09 13:22:09,372 - INFO - Batch 33600 finished
2025-03-09 13:22:09,372 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:22:26,703 - INFO - Batch 33626, Running Avg Loss: 4.72710
2025-03-09 13:22:44,087 - INFO - Batch 33651, Running Avg Loss: 4.72704
2025-03-09 13:23:01,522 - INFO - Batch 33676, Running Avg Loss: 4.72689
2025-03-09 13:23:19,159 - INFO - Batch 33701, Running Avg Loss: 4.72678
2025-03-09 13:23:19,176 - INFO - Batch 33700 finished
2025-03-09 13:23:19,176 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:23:36,674 - INFO - Batch 33726, Running Avg Loss: 4.72671
2025-03-09 13:23:54,231 - INFO - Batch 33751, Running Avg Loss: 4.72661
2025-03-09 13:24:11,636 - INFO - Batch 33776, Running Avg Loss: 4.72654
2025-03-09 13:24:29,002 - INFO - Batch 33801, Running Avg Loss: 4.72635
2025-03-09 13:24:29,019 - INFO - Batch 33800 finished
2025-03-09 13:24:29,020 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:24:46,471 - INFO - Batch 33826, Running Avg Loss: 4.72620
2025-03-09 13:25:03,979 - INFO - Batch 33851, Running Avg Loss: 4.72609
2025-03-09 13:25:21,528 - INFO - Batch 33876, Running Avg Loss: 4.72599
2025-03-09 13:25:39,008 - INFO - Batch 33901, Running Avg Loss: 4.72585
2025-03-09 13:25:39,023 - INFO - Batch 33900 finished
2025-03-09 13:25:39,023 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:25:56,526 - INFO - Batch 33926, Running Avg Loss: 4.72574
2025-03-09 13:26:14,107 - INFO - Batch 33951, Running Avg Loss: 4.72560
2025-03-09 13:26:31,856 - INFO - Batch 33976, Running Avg Loss: 4.72548
2025-03-09 13:26:49,420 - INFO - Batch 34001, Running Avg Loss: 4.72536
2025-03-09 13:26:49,436 - INFO - Batch 34000 finished
2025-03-09 13:26:49,436 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:27:06,954 - INFO - Batch 34026, Running Avg Loss: 4.72528
2025-03-09 13:27:24,443 - INFO - Batch 34051, Running Avg Loss: 4.72516
2025-03-09 13:27:41,937 - INFO - Batch 34076, Running Avg Loss: 4.72505
2025-03-09 13:27:59,426 - INFO - Batch 34101, Running Avg Loss: 4.72494
2025-03-09 13:27:59,441 - INFO - Batch 34100 finished
2025-03-09 13:27:59,442 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:28:16,938 - INFO - Batch 34126, Running Avg Loss: 4.72486
2025-03-09 13:28:34,446 - INFO - Batch 34151, Running Avg Loss: 4.72476
2025-03-09 13:28:51,994 - INFO - Batch 34176, Running Avg Loss: 4.72463
2025-03-09 13:29:09,529 - INFO - Batch 34201, Running Avg Loss: 4.72449
2025-03-09 13:29:09,547 - INFO - Batch 34200 finished
2025-03-09 13:29:09,548 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:29:27,054 - INFO - Batch 34226, Running Avg Loss: 4.72434
2025-03-09 13:29:44,556 - INFO - Batch 34251, Running Avg Loss: 4.72424
2025-03-09 13:30:02,250 - INFO - Batch 34276, Running Avg Loss: 4.72405
2025-03-09 13:30:19,742 - INFO - Batch 34301, Running Avg Loss: 4.72391
2025-03-09 13:30:19,757 - INFO - Batch 34300 finished
2025-03-09 13:30:19,757 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:30:37,180 - INFO - Batch 34326, Running Avg Loss: 4.72384
2025-03-09 13:30:54,323 - INFO - Batch 34351, Running Avg Loss: 4.72368
2025-03-09 13:31:11,552 - INFO - Batch 34376, Running Avg Loss: 4.72361
2025-03-09 13:31:28,959 - INFO - Batch 34401, Running Avg Loss: 4.72353
2025-03-09 13:31:28,977 - INFO - Batch 34400 finished
2025-03-09 13:31:28,977 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:31:46,371 - INFO - Batch 34426, Running Avg Loss: 4.72342
2025-03-09 13:32:03,874 - INFO - Batch 34451, Running Avg Loss: 4.72331
2025-03-09 13:32:21,433 - INFO - Batch 34476, Running Avg Loss: 4.72315
2025-03-09 13:32:38,883 - INFO - Batch 34501, Running Avg Loss: 4.72300
2025-03-09 13:32:38,899 - INFO - Batch 34500 finished
2025-03-09 13:32:38,899 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:32:56,327 - INFO - Batch 34526, Running Avg Loss: 4.72285
2025-03-09 13:33:14,005 - INFO - Batch 34551, Running Avg Loss: 4.72274
2025-03-09 13:33:31,546 - INFO - Batch 34576, Running Avg Loss: 4.72266
2025-03-09 13:33:49,058 - INFO - Batch 34601, Running Avg Loss: 4.72257
2025-03-09 13:33:49,077 - INFO - Batch 34600 finished
2025-03-09 13:33:49,078 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:34:06,570 - INFO - Batch 34626, Running Avg Loss: 4.72237
2025-03-09 13:34:24,081 - INFO - Batch 34651, Running Avg Loss: 4.72228
2025-03-09 13:34:41,608 - INFO - Batch 34676, Running Avg Loss: 4.72217
2025-03-09 13:34:59,340 - INFO - Batch 34701, Running Avg Loss: 4.72200
2025-03-09 13:34:59,358 - INFO - Batch 34700 finished
2025-03-09 13:34:59,359 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:35:16,907 - INFO - Batch 34726, Running Avg Loss: 4.72187
2025-03-09 13:35:34,464 - INFO - Batch 34751, Running Avg Loss: 4.72171
2025-03-09 13:35:51,980 - INFO - Batch 34776, Running Avg Loss: 4.72162
2025-03-09 13:36:09,628 - INFO - Batch 34801, Running Avg Loss: 4.72151
2025-03-09 13:36:09,648 - INFO - Batch 34800 finished
2025-03-09 13:36:09,648 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:36:27,079 - INFO - Batch 34826, Running Avg Loss: 4.72141
2025-03-09 13:36:44,571 - INFO - Batch 34851, Running Avg Loss: 4.72131
2025-03-09 13:37:02,032 - INFO - Batch 34876, Running Avg Loss: 4.72124
2025-03-09 13:37:19,490 - INFO - Batch 34901, Running Avg Loss: 4.72108
2025-03-09 13:37:19,506 - INFO - Batch 34900 finished
2025-03-09 13:37:19,506 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:37:36,876 - INFO - Batch 34926, Running Avg Loss: 4.72099
2025-03-09 13:37:54,198 - INFO - Batch 34951, Running Avg Loss: 4.72090
2025-03-09 13:38:11,508 - INFO - Batch 34976, Running Avg Loss: 4.72082
2025-03-09 13:38:28,837 - INFO - Batch 35001, Running Avg Loss: 4.72068
2025-03-09 13:38:28,850 - INFO - 
GPU Memory Stats at step 35000:
2025-03-09 13:38:28,851 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 13:38:28,851 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 13:38:28,851 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 13:38:28,851 - INFO - learning rate: 0.00000004
2025-03-09 13:38:28,851 - INFO - Ep 1 (Step 035000): Avg loss 4.721 | 143364096 tokens seen
2025-03-09 13:38:28,851 - INFO - optimizer lr: 0.00000004
2025-03-09 13:38:28,851 - INFO - scheduler lr: 0.00000004
2025-03-09 13:38:28,851 - INFO - Selected prompt: Meet Chris, a superhero of supplies! Just like how Batman protects Gotham City
2025-03-09 13:38:28,851 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:38:28,852 - INFO - random_topk: 6
2025-03-09 13:38:28,852 - INFO - random_temperature: 0.7906148135879698
2025-03-09 13:38:28,852 - INFO - global step 35000 , batch_idx 35000 => generating text
2025-03-09 13:38:28,852 - INFO - Generating on device cuda
2025-03-09 13:39:02,058 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:39:02,061 - INFO - Meet Chris, a superhero of supplies! Just like how Batman protects Gotham City, there were many people working together to help them understand what they had to do to help us understand the world around them.

Let's start with some basics:

* * *
* *

Now, let's go back to our first lesson!

**Step 1: Understanding the Importance of the Science**

A character is a way of thinking about how many parts of the world are and how to create new things. For example, if you're playing soccer during your school day, you could use a simple idea to help you feel better, right? That's where "the term 's' comes from.

**Step 2: Understand Your Own World**

There are two main types of language: the term "s' and 's' that is to be a special way of thinking. For example:

* A "The Wink" refers to a person who is a certain number of books, each group has its own way of thinking, feelings, and actions. They can use different shapes to express themselves and share their own stories.
* **Touched**: This means that everyone is a good time or group of people are feeling anxious or scared.
* **Sapine**:
2025-03-09 13:39:02,061 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:39:26,129 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_35000_steps_avg_loss_4.72068_optimizer_lr_0.00000004.pth
2025-03-09 13:39:26,376 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 13:39:26,376 - INFO - Batch 35000 finished
2025-03-09 13:39:26,376 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:39:43,845 - INFO - Batch 35026, Running Avg Loss: 4.72061
2025-03-09 13:40:01,358 - INFO - Batch 35051, Running Avg Loss: 4.72045
2025-03-09 13:40:18,582 - INFO - Batch 35076, Running Avg Loss: 4.72036
2025-03-09 13:40:35,706 - INFO - Batch 35101, Running Avg Loss: 4.72028
2025-03-09 13:40:35,719 - INFO - Batch 35100 finished
2025-03-09 13:40:35,719 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:40:52,901 - INFO - Batch 35126, Running Avg Loss: 4.72019
2025-03-09 13:41:10,314 - INFO - Batch 35151, Running Avg Loss: 4.72007
2025-03-09 13:41:27,831 - INFO - Batch 35176, Running Avg Loss: 4.71996
2025-03-09 13:41:45,473 - INFO - Batch 35201, Running Avg Loss: 4.71986
2025-03-09 13:41:45,493 - INFO - Batch 35200 finished
2025-03-09 13:41:45,493 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:42:03,014 - INFO - Batch 35226, Running Avg Loss: 4.71975
2025-03-09 13:42:20,791 - INFO - Batch 35251, Running Avg Loss: 4.71961
2025-03-09 13:42:38,366 - INFO - Batch 35276, Running Avg Loss: 4.71943
2025-03-09 13:42:55,883 - INFO - Batch 35301, Running Avg Loss: 4.71927
2025-03-09 13:42:55,900 - INFO - Batch 35300 finished
2025-03-09 13:42:55,900 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:43:13,362 - INFO - Batch 35326, Running Avg Loss: 4.71917
2025-03-09 13:43:30,882 - INFO - Batch 35351, Running Avg Loss: 4.71907
2025-03-09 13:43:48,460 - INFO - Batch 35376, Running Avg Loss: 4.71899
2025-03-09 13:44:06,008 - INFO - Batch 35401, Running Avg Loss: 4.71888
2025-03-09 13:44:06,025 - INFO - Batch 35400 finished
2025-03-09 13:44:06,025 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:44:23,543 - INFO - Batch 35426, Running Avg Loss: 4.71884
2025-03-09 13:44:41,050 - INFO - Batch 35451, Running Avg Loss: 4.71871
2025-03-09 13:44:58,680 - INFO - Batch 35476, Running Avg Loss: 4.71864
2025-03-09 13:45:16,308 - INFO - Batch 35501, Running Avg Loss: 4.71854
2025-03-09 13:45:16,324 - INFO - Batch 35500 finished
2025-03-09 13:45:16,324 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:45:33,869 - INFO - Batch 35526, Running Avg Loss: 4.71845
2025-03-09 13:45:51,562 - INFO - Batch 35551, Running Avg Loss: 4.71833
2025-03-09 13:46:09,101 - INFO - Batch 35576, Running Avg Loss: 4.71825
2025-03-09 13:46:26,645 - INFO - Batch 35601, Running Avg Loss: 4.71816
2025-03-09 13:46:26,662 - INFO - Batch 35600 finished
2025-03-09 13:46:26,662 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:46:44,195 - INFO - Batch 35626, Running Avg Loss: 4.71807
2025-03-09 13:47:01,736 - INFO - Batch 35651, Running Avg Loss: 4.71800
2025-03-09 13:47:19,253 - INFO - Batch 35676, Running Avg Loss: 4.71792
2025-03-09 13:47:36,608 - INFO - Batch 35701, Running Avg Loss: 4.71781
2025-03-09 13:47:36,626 - INFO - Batch 35700 finished
2025-03-09 13:47:36,627 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:47:54,088 - INFO - Batch 35726, Running Avg Loss: 4.71768
2025-03-09 13:48:11,624 - INFO - Batch 35751, Running Avg Loss: 4.71762
2025-03-09 13:48:29,092 - INFO - Batch 35776, Running Avg Loss: 4.71755
2025-03-09 13:48:46,543 - INFO - Batch 35801, Running Avg Loss: 4.71745
2025-03-09 13:48:46,558 - INFO - Batch 35800 finished
2025-03-09 13:48:46,559 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:49:04,195 - INFO - Batch 35826, Running Avg Loss: 4.71733
2025-03-09 13:49:21,667 - INFO - Batch 35851, Running Avg Loss: 4.71722
2025-03-09 13:49:39,222 - INFO - Batch 35876, Running Avg Loss: 4.71712
2025-03-09 13:49:56,803 - INFO - Batch 35901, Running Avg Loss: 4.71694
2025-03-09 13:49:56,821 - INFO - Batch 35900 finished
2025-03-09 13:49:56,822 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:50:14,350 - INFO - Batch 35926, Running Avg Loss: 4.71682
2025-03-09 13:50:31,872 - INFO - Batch 35951, Running Avg Loss: 4.71676
2025-03-09 13:50:49,289 - INFO - Batch 35976, Running Avg Loss: 4.71674
2025-03-09 13:51:06,506 - INFO - Batch 36001, Running Avg Loss: 4.71665
2025-03-09 13:51:06,522 - INFO - Batch 36000 finished
2025-03-09 13:51:06,522 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:51:24,116 - INFO - Batch 36026, Running Avg Loss: 4.71659
2025-03-09 13:51:41,576 - INFO - Batch 36051, Running Avg Loss: 4.71647
2025-03-09 13:51:59,072 - INFO - Batch 36076, Running Avg Loss: 4.71628
2025-03-09 13:52:16,567 - INFO - Batch 36101, Running Avg Loss: 4.71616
2025-03-09 13:52:16,583 - INFO - Batch 36100 finished
2025-03-09 13:52:16,583 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:52:34,257 - INFO - Batch 36126, Running Avg Loss: 4.71603
2025-03-09 13:52:51,884 - INFO - Batch 36151, Running Avg Loss: 4.71598
2025-03-09 13:53:09,433 - INFO - Batch 36176, Running Avg Loss: 4.71591
2025-03-09 13:53:26,973 - INFO - Batch 36201, Running Avg Loss: 4.71579
2025-03-09 13:53:26,988 - INFO - Batch 36200 finished
2025-03-09 13:53:26,989 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:53:44,490 - INFO - Batch 36226, Running Avg Loss: 4.71574
2025-03-09 13:54:02,002 - INFO - Batch 36251, Running Avg Loss: 4.71566
2025-03-09 13:54:19,674 - INFO - Batch 36276, Running Avg Loss: 4.71559
2025-03-09 13:54:37,192 - INFO - Batch 36301, Running Avg Loss: 4.71550
2025-03-09 13:54:37,208 - INFO - Batch 36300 finished
2025-03-09 13:54:37,209 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:54:54,725 - INFO - Batch 36326, Running Avg Loss: 4.71544
2025-03-09 13:55:12,259 - INFO - Batch 36351, Running Avg Loss: 4.71534
2025-03-09 13:55:29,953 - INFO - Batch 36376, Running Avg Loss: 4.71524
2025-03-09 13:55:47,470 - INFO - Batch 36401, Running Avg Loss: 4.71516
2025-03-09 13:55:47,488 - INFO - Batch 36400 finished
2025-03-09 13:55:47,488 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:56:04,962 - INFO - Batch 36426, Running Avg Loss: 4.71506
2025-03-09 13:56:22,441 - INFO - Batch 36451, Running Avg Loss: 4.71500
2025-03-09 13:56:39,912 - INFO - Batch 36476, Running Avg Loss: 4.71490
2025-03-09 13:56:57,426 - INFO - Batch 36501, Running Avg Loss: 4.71479
2025-03-09 13:56:57,441 - INFO - Batch 36500 finished
2025-03-09 13:56:57,442 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:57:14,877 - INFO - Batch 36526, Running Avg Loss: 4.71466
2025-03-09 13:57:32,244 - INFO - Batch 36551, Running Avg Loss: 4.71454
2025-03-09 13:57:49,617 - INFO - Batch 36576, Running Avg Loss: 4.71453
2025-03-09 13:58:07,100 - INFO - Batch 36601, Running Avg Loss: 4.71436
2025-03-09 13:58:07,116 - INFO - Batch 36600 finished
2025-03-09 13:58:07,116 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:58:24,573 - INFO - Batch 36626, Running Avg Loss: 4.71422
2025-03-09 13:58:42,255 - INFO - Batch 36651, Running Avg Loss: 4.71410
2025-03-09 13:58:59,741 - INFO - Batch 36676, Running Avg Loss: 4.71404
2025-03-09 13:59:17,160 - INFO - Batch 36701, Running Avg Loss: 4.71393
2025-03-09 13:59:17,178 - INFO - Batch 36700 finished
2025-03-09 13:59:17,178 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 13:59:34,661 - INFO - Batch 36726, Running Avg Loss: 4.71383
2025-03-09 13:59:52,217 - INFO - Batch 36751, Running Avg Loss: 4.71375
2025-03-09 14:00:09,781 - INFO - Batch 36776, Running Avg Loss: 4.71359
2025-03-09 14:00:27,182 - INFO - Batch 36801, Running Avg Loss: 4.71348
2025-03-09 14:00:27,194 - INFO - Batch 36800 finished
2025-03-09 14:00:27,195 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:00:44,318 - INFO - Batch 36826, Running Avg Loss: 4.71339
2025-03-09 14:01:01,398 - INFO - Batch 36851, Running Avg Loss: 4.71325
2025-03-09 14:01:18,524 - INFO - Batch 36876, Running Avg Loss: 4.71313
2025-03-09 14:01:35,777 - INFO - Batch 36901, Running Avg Loss: 4.71305
2025-03-09 14:01:35,792 - INFO - Batch 36900 finished
2025-03-09 14:01:35,792 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:01:53,189 - INFO - Batch 36926, Running Avg Loss: 4.71290
2025-03-09 14:02:10,819 - INFO - Batch 36951, Running Avg Loss: 4.71280
2025-03-09 14:02:28,361 - INFO - Batch 36976, Running Avg Loss: 4.71272
2025-03-09 14:02:45,908 - INFO - Batch 37001, Running Avg Loss: 4.71264
2025-03-09 14:02:45,924 - INFO - Batch 37000 finished
2025-03-09 14:02:45,924 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:03:03,406 - INFO - Batch 37026, Running Avg Loss: 4.71248
2025-03-09 14:03:20,827 - INFO - Batch 37051, Running Avg Loss: 4.71243
2025-03-09 14:03:38,273 - INFO - Batch 37076, Running Avg Loss: 4.71238
2025-03-09 14:03:55,726 - INFO - Batch 37101, Running Avg Loss: 4.71230
2025-03-09 14:03:55,740 - INFO - Batch 37100 finished
2025-03-09 14:03:55,740 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:04:13,208 - INFO - Batch 37126, Running Avg Loss: 4.71227
2025-03-09 14:04:30,741 - INFO - Batch 37151, Running Avg Loss: 4.71216
2025-03-09 14:04:48,277 - INFO - Batch 37176, Running Avg Loss: 4.71211
2025-03-09 14:05:05,747 - INFO - Batch 37201, Running Avg Loss: 4.71201
2025-03-09 14:05:05,765 - INFO - Batch 37200 finished
2025-03-09 14:05:05,766 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:05:23,396 - INFO - Batch 37226, Running Avg Loss: 4.71189
2025-03-09 14:05:40,903 - INFO - Batch 37251, Running Avg Loss: 4.71181
2025-03-09 14:05:58,437 - INFO - Batch 37276, Running Avg Loss: 4.71172
2025-03-09 14:06:15,957 - INFO - Batch 37301, Running Avg Loss: 4.71160
2025-03-09 14:06:15,972 - INFO - Batch 37300 finished
2025-03-09 14:06:15,973 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:06:33,488 - INFO - Batch 37326, Running Avg Loss: 4.71145
2025-03-09 14:06:51,002 - INFO - Batch 37351, Running Avg Loss: 4.71141
2025-03-09 14:07:08,565 - INFO - Batch 37376, Running Avg Loss: 4.71130
2025-03-09 14:07:26,262 - INFO - Batch 37401, Running Avg Loss: 4.71123
2025-03-09 14:07:26,281 - INFO - Batch 37400 finished
2025-03-09 14:07:26,281 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:07:43,756 - INFO - Batch 37426, Running Avg Loss: 4.71119
2025-03-09 14:08:01,232 - INFO - Batch 37451, Running Avg Loss: 4.71115
2025-03-09 14:08:18,713 - INFO - Batch 37476, Running Avg Loss: 4.71105
2025-03-09 14:08:36,580 - INFO - Batch 37501, Running Avg Loss: 4.71098
2025-03-09 14:08:36,598 - INFO - 
GPU Memory Stats at step 37500:
2025-03-09 14:08:36,598 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 14:08:36,598 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 14:08:36,598 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 14:08:36,599 - INFO - learning rate: 0.00000003
2025-03-09 14:08:36,599 - INFO - Ep 1 (Step 037500): Avg loss 4.711 | 153604096 tokens seen
2025-03-09 14:08:36,599 - INFO - optimizer lr: 0.00000003
2025-03-09 14:08:36,599 - INFO - scheduler lr: 0.00000003
2025-03-09 14:08:36,599 - INFO - Selected prompt: Feeling Alone Together: Exploring Alienation and Isolation in Literature
2025-03-09 14:08:36,599 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:08:36,600 - INFO - random_topk: 8
2025-03-09 14:08:36,600 - INFO - random_temperature: 0.8207704011481908
2025-03-09 14:08:36,600 - INFO - global step 37500 , batch_idx 37500 => generating text
2025-03-09 14:08:36,600 - INFO - Generating on device cuda
2025-03-09 14:09:10,584 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:09:10,584 - INFO - Feeling Alone Together: Exploring Alienation and Isolation in Literature

I. Introduction

In the vast expanse of the Middle East, there are many different types of characters who identify as the "an" within a particular group of people. These stories often reflect the complexities surrounding race, identity, and even the power to navigate the natural world. In this unit, we will delve into the fascinating realm of these ancient practices, specifically focusing on the idea of the "an," "Aom," and "The Great" in 4. This chapter will explore the historical context, cultural significance, and how they intersect within this fascinating world.

II. Historical context of the United States

Before diving into the significance of the United States, it is essential first to understand the historical roots in which these ancient languages have been adapted to various forms such as the United Kingdom (1320 BCE). The early 1930s, this era saw many of the most renowned historical events within the region. As such, it was often referred to as the "The Gikan" (1930s). In this section, we will examine the ways in which the term 's' in the 19th century and 2176, where the United States was the beginning of the
2025-03-09 14:09:10,584 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:09:34,921 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_37500_steps_avg_loss_4.71098_optimizer_lr_0.00000003.pth
2025-03-09 14:09:35,150 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 14:09:35,151 - INFO - Batch 37500 finished
2025-03-09 14:09:35,151 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:09:52,575 - INFO - Batch 37526, Running Avg Loss: 4.71093
2025-03-09 14:10:10,130 - INFO - Batch 37551, Running Avg Loss: 4.71083
2025-03-09 14:10:27,888 - INFO - Batch 37576, Running Avg Loss: 4.71071
2025-03-09 14:10:45,371 - INFO - Batch 37601, Running Avg Loss: 4.71066
2025-03-09 14:10:45,388 - INFO - Batch 37600 finished
2025-03-09 14:10:45,389 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:11:02,787 - INFO - Batch 37626, Running Avg Loss: 4.71056
2025-03-09 14:11:20,340 - INFO - Batch 37651, Running Avg Loss: 4.71042
2025-03-09 14:11:37,809 - INFO - Batch 37676, Running Avg Loss: 4.71035
2025-03-09 14:11:55,255 - INFO - Batch 37701, Running Avg Loss: 4.71024
2025-03-09 14:11:55,273 - INFO - Batch 37700 finished
2025-03-09 14:11:55,274 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:12:12,719 - INFO - Batch 37726, Running Avg Loss: 4.71013
2025-03-09 14:12:30,153 - INFO - Batch 37751, Running Avg Loss: 4.71007
2025-03-09 14:12:47,673 - INFO - Batch 37776, Running Avg Loss: 4.70995
2025-03-09 14:13:05,199 - INFO - Batch 37801, Running Avg Loss: 4.70986
2025-03-09 14:13:05,215 - INFO - Batch 37800 finished
2025-03-09 14:13:05,216 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:13:22,629 - INFO - Batch 37826, Running Avg Loss: 4.70975
2025-03-09 14:13:40,238 - INFO - Batch 37851, Running Avg Loss: 4.70963
2025-03-09 14:13:57,711 - INFO - Batch 37876, Running Avg Loss: 4.70950
2025-03-09 14:14:15,224 - INFO - Batch 37901, Running Avg Loss: 4.70943
2025-03-09 14:14:15,242 - INFO - Batch 37900 finished
2025-03-09 14:14:15,242 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:14:32,760 - INFO - Batch 37926, Running Avg Loss: 4.70933
2025-03-09 14:14:50,333 - INFO - Batch 37951, Running Avg Loss: 4.70918
2025-03-09 14:15:07,932 - INFO - Batch 37976, Running Avg Loss: 4.70911
2025-03-09 14:15:25,616 - INFO - Batch 38001, Running Avg Loss: 4.70896
2025-03-09 14:15:25,632 - INFO - Batch 38000 finished
2025-03-09 14:15:25,633 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:15:43,100 - INFO - Batch 38026, Running Avg Loss: 4.70889
2025-03-09 14:16:00,612 - INFO - Batch 38051, Running Avg Loss: 4.70878
2025-03-09 14:16:18,211 - INFO - Batch 38076, Running Avg Loss: 4.70866
2025-03-09 14:16:35,583 - INFO - Batch 38101, Running Avg Loss: 4.70852
2025-03-09 14:16:35,602 - INFO - Batch 38100 finished
2025-03-09 14:16:35,602 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:16:52,904 - INFO - Batch 38126, Running Avg Loss: 4.70844
2025-03-09 14:17:10,452 - INFO - Batch 38151, Running Avg Loss: 4.70836
2025-03-09 14:17:27,936 - INFO - Batch 38176, Running Avg Loss: 4.70830
2025-03-09 14:17:45,398 - INFO - Batch 38201, Running Avg Loss: 4.70818
2025-03-09 14:17:45,414 - INFO - Batch 38200 finished
2025-03-09 14:17:45,415 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:18:02,899 - INFO - Batch 38226, Running Avg Loss: 4.70806
2025-03-09 14:18:20,396 - INFO - Batch 38251, Running Avg Loss: 4.70797
2025-03-09 14:18:37,875 - INFO - Batch 38276, Running Avg Loss: 4.70790
2025-03-09 14:18:55,375 - INFO - Batch 38301, Running Avg Loss: 4.70777
2025-03-09 14:18:55,392 - INFO - Batch 38300 finished
2025-03-09 14:18:55,392 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:19:12,913 - INFO - Batch 38326, Running Avg Loss: 4.70768
2025-03-09 14:19:30,573 - INFO - Batch 38351, Running Avg Loss: 4.70762
2025-03-09 14:19:48,205 - INFO - Batch 38376, Running Avg Loss: 4.70749
2025-03-09 14:20:05,749 - INFO - Batch 38401, Running Avg Loss: 4.70743
2025-03-09 14:20:05,767 - INFO - Batch 38400 finished
2025-03-09 14:20:05,767 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:20:23,383 - INFO - Batch 38426, Running Avg Loss: 4.70733
2025-03-09 14:20:40,925 - INFO - Batch 38451, Running Avg Loss: 4.70730
2025-03-09 14:20:58,569 - INFO - Batch 38476, Running Avg Loss: 4.70723
2025-03-09 14:21:16,206 - INFO - Batch 38501, Running Avg Loss: 4.70714
2025-03-09 14:21:16,229 - INFO - Batch 38500 finished
2025-03-09 14:21:16,230 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:21:33,790 - INFO - Batch 38526, Running Avg Loss: 4.70705
2025-03-09 14:21:51,439 - INFO - Batch 38551, Running Avg Loss: 4.70694
2025-03-09 14:22:09,219 - INFO - Batch 38576, Running Avg Loss: 4.70686
2025-03-09 14:22:26,842 - INFO - Batch 38601, Running Avg Loss: 4.70671
2025-03-09 14:22:26,857 - INFO - Batch 38600 finished
2025-03-09 14:22:26,857 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:22:44,438 - INFO - Batch 38626, Running Avg Loss: 4.70663
2025-03-09 14:23:02,010 - INFO - Batch 38651, Running Avg Loss: 4.70653
2025-03-09 14:23:19,530 - INFO - Batch 38676, Running Avg Loss: 4.70642
2025-03-09 14:23:37,197 - INFO - Batch 38701, Running Avg Loss: 4.70629
2025-03-09 14:23:37,214 - INFO - Batch 38700 finished
2025-03-09 14:23:37,215 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:23:54,780 - INFO - Batch 38726, Running Avg Loss: 4.70619
2025-03-09 14:24:12,360 - INFO - Batch 38751, Running Avg Loss: 4.70610
2025-03-09 14:24:29,911 - INFO - Batch 38776, Running Avg Loss: 4.70601
2025-03-09 14:24:47,558 - INFO - Batch 38801, Running Avg Loss: 4.70591
2025-03-09 14:24:47,574 - INFO - Batch 38800 finished
2025-03-09 14:24:47,575 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:25:05,164 - INFO - Batch 38826, Running Avg Loss: 4.70584
2025-03-09 14:25:22,724 - INFO - Batch 38851, Running Avg Loss: 4.70569
2025-03-09 14:25:40,258 - INFO - Batch 38876, Running Avg Loss: 4.70559
2025-03-09 14:25:57,708 - INFO - Batch 38901, Running Avg Loss: 4.70552
2025-03-09 14:25:57,726 - INFO - Batch 38900 finished
2025-03-09 14:25:57,726 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:26:15,235 - INFO - Batch 38926, Running Avg Loss: 4.70544
2025-03-09 14:26:32,810 - INFO - Batch 38951, Running Avg Loss: 4.70537
2025-03-09 14:26:50,561 - INFO - Batch 38976, Running Avg Loss: 4.70521
2025-03-09 14:27:07,998 - INFO - Batch 39001, Running Avg Loss: 4.70508
2025-03-09 14:27:08,018 - INFO - Batch 39000 finished
2025-03-09 14:27:08,018 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:27:25,384 - INFO - Batch 39026, Running Avg Loss: 4.70500
2025-03-09 14:27:42,851 - INFO - Batch 39051, Running Avg Loss: 4.70493
2025-03-09 14:28:00,319 - INFO - Batch 39076, Running Avg Loss: 4.70486
2025-03-09 14:28:17,819 - INFO - Batch 39101, Running Avg Loss: 4.70477
2025-03-09 14:28:17,834 - INFO - Batch 39100 finished
2025-03-09 14:28:17,834 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:28:35,292 - INFO - Batch 39126, Running Avg Loss: 4.70471
2025-03-09 14:28:52,647 - INFO - Batch 39151, Running Avg Loss: 4.70465
2025-03-09 14:29:10,094 - INFO - Batch 39176, Running Avg Loss: 4.70461
2025-03-09 14:29:27,609 - INFO - Batch 39201, Running Avg Loss: 4.70451
2025-03-09 14:29:27,625 - INFO - Batch 39200 finished
2025-03-09 14:29:27,626 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:29:45,173 - INFO - Batch 39226, Running Avg Loss: 4.70440
2025-03-09 14:30:02,690 - INFO - Batch 39251, Running Avg Loss: 4.70433
2025-03-09 14:30:20,304 - INFO - Batch 39276, Running Avg Loss: 4.70428
2025-03-09 14:30:37,736 - INFO - Batch 39301, Running Avg Loss: 4.70420
2025-03-09 14:30:37,752 - INFO - Batch 39300 finished
2025-03-09 14:30:37,752 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:30:54,921 - INFO - Batch 39326, Running Avg Loss: 4.70409
2025-03-09 14:31:12,140 - INFO - Batch 39351, Running Avg Loss: 4.70398
2025-03-09 14:31:29,656 - INFO - Batch 39376, Running Avg Loss: 4.70389
2025-03-09 14:31:47,130 - INFO - Batch 39401, Running Avg Loss: 4.70383
2025-03-09 14:31:47,149 - INFO - Batch 39400 finished
2025-03-09 14:31:47,149 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:32:04,615 - INFO - Batch 39426, Running Avg Loss: 4.70375
2025-03-09 14:32:22,089 - INFO - Batch 39451, Running Avg Loss: 4.70369
2025-03-09 14:32:39,603 - INFO - Batch 39476, Running Avg Loss: 4.70356
2025-03-09 14:32:57,153 - INFO - Batch 39501, Running Avg Loss: 4.70344
2025-03-09 14:32:57,172 - INFO - Batch 39500 finished
2025-03-09 14:32:57,172 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:33:14,766 - INFO - Batch 39526, Running Avg Loss: 4.70337
2025-03-09 14:33:32,483 - INFO - Batch 39551, Running Avg Loss: 4.70334
2025-03-09 14:33:50,099 - INFO - Batch 39576, Running Avg Loss: 4.70332
2025-03-09 14:34:07,702 - INFO - Batch 39601, Running Avg Loss: 4.70323
2025-03-09 14:34:07,719 - INFO - Batch 39600 finished
2025-03-09 14:34:07,720 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:34:25,339 - INFO - Batch 39626, Running Avg Loss: 4.70313
2025-03-09 14:34:42,919 - INFO - Batch 39651, Running Avg Loss: 4.70304
2025-03-09 14:35:00,414 - INFO - Batch 39676, Running Avg Loss: 4.70295
2025-03-09 14:35:18,160 - INFO - Batch 39701, Running Avg Loss: 4.70291
2025-03-09 14:35:18,176 - INFO - Batch 39700 finished
2025-03-09 14:35:18,177 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:35:35,670 - INFO - Batch 39726, Running Avg Loss: 4.70281
2025-03-09 14:35:53,187 - INFO - Batch 39751, Running Avg Loss: 4.70269
2025-03-09 14:36:10,775 - INFO - Batch 39776, Running Avg Loss: 4.70259
2025-03-09 14:36:28,479 - INFO - Batch 39801, Running Avg Loss: 4.70248
2025-03-09 14:36:28,498 - INFO - Batch 39800 finished
2025-03-09 14:36:28,499 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:36:45,870 - INFO - Batch 39826, Running Avg Loss: 4.70239
2025-03-09 14:37:03,356 - INFO - Batch 39851, Running Avg Loss: 4.70227
2025-03-09 14:37:20,887 - INFO - Batch 39876, Running Avg Loss: 4.70222
2025-03-09 14:37:38,422 - INFO - Batch 39901, Running Avg Loss: 4.70214
2025-03-09 14:37:38,438 - INFO - Batch 39900 finished
2025-03-09 14:37:38,438 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:37:55,915 - INFO - Batch 39926, Running Avg Loss: 4.70206
2025-03-09 14:38:13,316 - INFO - Batch 39951, Running Avg Loss: 4.70205
2025-03-09 14:38:30,686 - INFO - Batch 39976, Running Avg Loss: 4.70189
2025-03-09 14:38:48,159 - INFO - Batch 40001, Running Avg Loss: 4.70182
2025-03-09 14:38:48,175 - INFO - 
GPU Memory Stats at step 40000:
2025-03-09 14:38:48,176 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 14:38:48,176 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 14:38:48,176 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 14:38:48,176 - INFO - learning rate: 0.00000003
2025-03-09 14:38:48,176 - INFO - Ep 1 (Step 040000): Avg loss 4.702 | 163844096 tokens seen
2025-03-09 14:38:48,176 - INFO - optimizer lr: 0.00000003
2025-03-09 14:38:48,176 - INFO - scheduler lr: 0.00000003
2025-03-09 14:38:48,176 - INFO - Selected prompt: Imagine if someone got their hands on dangerous weapons
2025-03-09 14:38:48,176 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:38:48,177 - INFO - random_topk: 2
2025-03-09 14:38:48,177 - INFO - random_temperature: 0.801170103114257
2025-03-09 14:38:48,177 - INFO - global step 40000 , batch_idx 40000 => generating text
2025-03-09 14:38:48,177 - INFO - Generating on device cuda
2025-03-09 14:39:22,029 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:39:22,029 - INFO - Imagine if someone got their hands on dangerous weapons, or perhaps you were feeling a bit of your own life. This is what we call a "the term," which is called "gap" and it's like a big, beautiful place where people live in the globe. Let's dive into understanding what makes these words so special!

Imagine you are playing a game where you can't know how much money you need to do. You would want to know what you want to be like, and you can use it to help you understand your own thoughts and feelings. That's kind of like what we call a "the term," which is called "the" or "gol" or "gol" that makes you feel happy.

Now, let's talk about what a child is like. Imagine you have a lemonade stand, and your friend wants to buy it up and feel better at the same time. You might feel like you have trouble breathing, but it doesn't mean you don't know what you're going. Instead, you might feel sad because you don't have to be a good idea.

But what does this mean to do with your friend? Well, there are many ways that people can use to keep track of your thoughts, feelings, and experiences. For example,
2025-03-09 14:39:22,029 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:39:50,323 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_40000_steps_avg_loss_4.70182_optimizer_lr_0.00000003.pth
2025-03-09 14:39:50,532 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 14:39:50,533 - INFO - Batch 40000 finished
2025-03-09 14:39:50,533 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:40:07,816 - INFO - Batch 40026, Running Avg Loss: 4.70178
2025-03-09 14:40:25,210 - INFO - Batch 40051, Running Avg Loss: 4.70175
2025-03-09 14:40:42,697 - INFO - Batch 40076, Running Avg Loss: 4.70160
2025-03-09 14:40:59,917 - INFO - Batch 40101, Running Avg Loss: 4.70150
2025-03-09 14:40:59,930 - INFO - Batch 40100 finished
2025-03-09 14:40:59,930 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:41:17,224 - INFO - Batch 40126, Running Avg Loss: 4.70138
2025-03-09 14:41:34,843 - INFO - Batch 40151, Running Avg Loss: 4.70129
2025-03-09 14:41:52,248 - INFO - Batch 40176, Running Avg Loss: 4.70122
2025-03-09 14:42:09,637 - INFO - Batch 40201, Running Avg Loss: 4.70114
2025-03-09 14:42:09,655 - INFO - Batch 40200 finished
2025-03-09 14:42:09,655 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:42:27,183 - INFO - Batch 40226, Running Avg Loss: 4.70101
2025-03-09 14:42:44,872 - INFO - Batch 40251, Running Avg Loss: 4.70096
2025-03-09 14:43:02,434 - INFO - Batch 40276, Running Avg Loss: 4.70086
2025-03-09 14:43:20,006 - INFO - Batch 40301, Running Avg Loss: 4.70078
2025-03-09 14:43:20,021 - INFO - Batch 40300 finished
2025-03-09 14:43:20,021 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:43:37,493 - INFO - Batch 40326, Running Avg Loss: 4.70062
2025-03-09 14:43:54,997 - INFO - Batch 40351, Running Avg Loss: 4.70054
2025-03-09 14:44:12,476 - INFO - Batch 40376, Running Avg Loss: 4.70037
2025-03-09 14:44:29,806 - INFO - Batch 40401, Running Avg Loss: 4.70029
2025-03-09 14:44:29,819 - INFO - Batch 40400 finished
2025-03-09 14:44:29,819 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:44:47,224 - INFO - Batch 40426, Running Avg Loss: 4.70019
2025-03-09 14:45:04,604 - INFO - Batch 40451, Running Avg Loss: 4.70008
2025-03-09 14:45:22,020 - INFO - Batch 40476, Running Avg Loss: 4.70000
2025-03-09 14:45:39,387 - INFO - Batch 40501, Running Avg Loss: 4.69992
2025-03-09 14:45:39,403 - INFO - Batch 40500 finished
2025-03-09 14:45:39,404 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:45:56,814 - INFO - Batch 40526, Running Avg Loss: 4.69981
2025-03-09 14:46:14,521 - INFO - Batch 40551, Running Avg Loss: 4.69979
2025-03-09 14:46:32,035 - INFO - Batch 40576, Running Avg Loss: 4.69973
2025-03-09 14:46:49,552 - INFO - Batch 40601, Running Avg Loss: 4.69962
2025-03-09 14:46:49,570 - INFO - Batch 40600 finished
2025-03-09 14:46:49,571 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:47:07,061 - INFO - Batch 40626, Running Avg Loss: 4.69958
2025-03-09 14:47:24,556 - INFO - Batch 40651, Running Avg Loss: 4.69952
2025-03-09 14:47:42,171 - INFO - Batch 40676, Running Avg Loss: 4.69941
2025-03-09 14:47:59,688 - INFO - Batch 40701, Running Avg Loss: 4.69931
2025-03-09 14:47:59,704 - INFO - Batch 40700 finished
2025-03-09 14:47:59,705 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:48:17,300 - INFO - Batch 40726, Running Avg Loss: 4.69925
2025-03-09 14:48:34,816 - INFO - Batch 40751, Running Avg Loss: 4.69917
2025-03-09 14:48:52,337 - INFO - Batch 40776, Running Avg Loss: 4.69912
2025-03-09 14:49:09,824 - INFO - Batch 40801, Running Avg Loss: 4.69903
2025-03-09 14:49:09,842 - INFO - Batch 40800 finished
2025-03-09 14:49:09,842 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:49:27,455 - INFO - Batch 40826, Running Avg Loss: 4.69896
2025-03-09 14:49:45,016 - INFO - Batch 40851, Running Avg Loss: 4.69888
2025-03-09 14:50:02,566 - INFO - Batch 40876, Running Avg Loss: 4.69880
2025-03-09 14:50:20,138 - INFO - Batch 40901, Running Avg Loss: 4.69875
2025-03-09 14:50:20,155 - INFO - Batch 40900 finished
2025-03-09 14:50:20,155 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:50:37,742 - INFO - Batch 40926, Running Avg Loss: 4.69872
2025-03-09 14:50:55,266 - INFO - Batch 40951, Running Avg Loss: 4.69864
2025-03-09 14:51:12,597 - INFO - Batch 40976, Running Avg Loss: 4.69857
2025-03-09 14:51:30,036 - INFO - Batch 41001, Running Avg Loss: 4.69852
2025-03-09 14:51:30,054 - INFO - Batch 41000 finished
2025-03-09 14:51:30,054 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:51:47,469 - INFO - Batch 41026, Running Avg Loss: 4.69839
2025-03-09 14:52:04,823 - INFO - Batch 41051, Running Avg Loss: 4.69827
2025-03-09 14:52:22,363 - INFO - Batch 41076, Running Avg Loss: 4.69824
2025-03-09 14:52:39,899 - INFO - Batch 41101, Running Avg Loss: 4.69813
2025-03-09 14:52:39,916 - INFO - Batch 41100 finished
2025-03-09 14:52:39,917 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:52:57,524 - INFO - Batch 41126, Running Avg Loss: 4.69808
2025-03-09 14:53:15,077 - INFO - Batch 41151, Running Avg Loss: 4.69801
2025-03-09 14:53:32,680 - INFO - Batch 41176, Running Avg Loss: 4.69794
2025-03-09 14:53:50,183 - INFO - Batch 41201, Running Avg Loss: 4.69786
2025-03-09 14:53:50,199 - INFO - Batch 41200 finished
2025-03-09 14:53:50,199 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:54:07,578 - INFO - Batch 41226, Running Avg Loss: 4.69778
2025-03-09 14:54:25,002 - INFO - Batch 41251, Running Avg Loss: 4.69771
2025-03-09 14:54:42,554 - INFO - Batch 41276, Running Avg Loss: 4.69766
2025-03-09 14:55:00,018 - INFO - Batch 41301, Running Avg Loss: 4.69763
2025-03-09 14:55:00,035 - INFO - Batch 41300 finished
2025-03-09 14:55:00,036 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:55:17,526 - INFO - Batch 41326, Running Avg Loss: 4.69756
2025-03-09 14:55:35,021 - INFO - Batch 41351, Running Avg Loss: 4.69747
2025-03-09 14:55:52,762 - INFO - Batch 41376, Running Avg Loss: 4.69739
2025-03-09 14:56:10,374 - INFO - Batch 41401, Running Avg Loss: 4.69730
2025-03-09 14:56:10,391 - INFO - Batch 41400 finished
2025-03-09 14:56:10,391 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:56:27,947 - INFO - Batch 41426, Running Avg Loss: 4.69719
2025-03-09 14:56:45,434 - INFO - Batch 41451, Running Avg Loss: 4.69716
2025-03-09 14:57:03,002 - INFO - Batch 41476, Running Avg Loss: 4.69713
2025-03-09 14:57:20,575 - INFO - Batch 41501, Running Avg Loss: 4.69711
2025-03-09 14:57:20,591 - INFO - Batch 41500 finished
2025-03-09 14:57:20,592 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:57:38,124 - INFO - Batch 41526, Running Avg Loss: 4.69703
2025-03-09 14:57:55,602 - INFO - Batch 41551, Running Avg Loss: 4.69698
2025-03-09 14:58:13,065 - INFO - Batch 41576, Running Avg Loss: 4.69692
2025-03-09 14:58:30,566 - INFO - Batch 41601, Running Avg Loss: 4.69687
2025-03-09 14:58:30,581 - INFO - Batch 41600 finished
2025-03-09 14:58:30,581 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:58:48,024 - INFO - Batch 41626, Running Avg Loss: 4.69675
2025-03-09 14:59:05,652 - INFO - Batch 41651, Running Avg Loss: 4.69672
2025-03-09 14:59:23,086 - INFO - Batch 41676, Running Avg Loss: 4.69666
2025-03-09 14:59:40,530 - INFO - Batch 41701, Running Avg Loss: 4.69658
2025-03-09 14:59:40,544 - INFO - Batch 41700 finished
2025-03-09 14:59:40,545 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 14:59:57,928 - INFO - Batch 41726, Running Avg Loss: 4.69650
2025-03-09 15:00:15,367 - INFO - Batch 41751, Running Avg Loss: 4.69639
2025-03-09 15:00:32,840 - INFO - Batch 41776, Running Avg Loss: 4.69630
2025-03-09 15:00:50,086 - INFO - Batch 41801, Running Avg Loss: 4.69620
2025-03-09 15:00:50,099 - INFO - Batch 41800 finished
2025-03-09 15:00:50,099 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:01:07,230 - INFO - Batch 41826, Running Avg Loss: 4.69615
2025-03-09 15:01:24,368 - INFO - Batch 41851, Running Avg Loss: 4.69609
2025-03-09 15:01:41,772 - INFO - Batch 41876, Running Avg Loss: 4.69597
2025-03-09 15:01:59,236 - INFO - Batch 41901, Running Avg Loss: 4.69585
2025-03-09 15:01:59,252 - INFO - Batch 41900 finished
2025-03-09 15:01:59,253 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:02:16,793 - INFO - Batch 41926, Running Avg Loss: 4.69581
2025-03-09 15:02:34,515 - INFO - Batch 41951, Running Avg Loss: 4.69571
2025-03-09 15:02:51,955 - INFO - Batch 41976, Running Avg Loss: 4.69570
2025-03-09 15:03:09,453 - INFO - Batch 42001, Running Avg Loss: 4.69564
2025-03-09 15:03:09,468 - INFO - Batch 42000 finished
2025-03-09 15:03:09,468 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:03:26,942 - INFO - Batch 42026, Running Avg Loss: 4.69555
2025-03-09 15:03:44,419 - INFO - Batch 42051, Running Avg Loss: 4.69552
2025-03-09 15:04:01,934 - INFO - Batch 42076, Running Avg Loss: 4.69544
2025-03-09 15:04:19,558 - INFO - Batch 42101, Running Avg Loss: 4.69536
2025-03-09 15:04:19,575 - INFO - Batch 42100 finished
2025-03-09 15:04:19,575 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:04:37,143 - INFO - Batch 42126, Running Avg Loss: 4.69527
2025-03-09 15:04:54,706 - INFO - Batch 42151, Running Avg Loss: 4.69515
2025-03-09 15:05:12,250 - INFO - Batch 42176, Running Avg Loss: 4.69509
2025-03-09 15:05:29,718 - INFO - Batch 42201, Running Avg Loss: 4.69501
2025-03-09 15:05:29,735 - INFO - Batch 42200 finished
2025-03-09 15:05:29,736 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:05:47,353 - INFO - Batch 42226, Running Avg Loss: 4.69491
2025-03-09 15:06:04,873 - INFO - Batch 42251, Running Avg Loss: 4.69483
2025-03-09 15:06:22,339 - INFO - Batch 42276, Running Avg Loss: 4.69475
2025-03-09 15:06:39,819 - INFO - Batch 42301, Running Avg Loss: 4.69471
2025-03-09 15:06:39,836 - INFO - Batch 42300 finished
2025-03-09 15:06:39,836 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:06:57,258 - INFO - Batch 42326, Running Avg Loss: 4.69462
2025-03-09 15:07:14,715 - INFO - Batch 42351, Running Avg Loss: 4.69453
2025-03-09 15:07:32,203 - INFO - Batch 42376, Running Avg Loss: 4.69441
2025-03-09 15:07:49,845 - INFO - Batch 42401, Running Avg Loss: 4.69437
2025-03-09 15:07:49,862 - INFO - Batch 42400 finished
2025-03-09 15:07:49,862 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:08:07,322 - INFO - Batch 42426, Running Avg Loss: 4.69432
2025-03-09 15:08:24,830 - INFO - Batch 42451, Running Avg Loss: 4.69422
2025-03-09 15:08:42,360 - INFO - Batch 42476, Running Avg Loss: 4.69417
2025-03-09 15:09:00,062 - INFO - Batch 42501, Running Avg Loss: 4.69412
2025-03-09 15:09:00,077 - INFO - 
GPU Memory Stats at step 42500:
2025-03-09 15:09:00,077 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 15:09:00,078 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 15:09:00,078 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 15:09:00,078 - INFO - learning rate: 0.00000002
2025-03-09 15:09:00,078 - INFO - Ep 1 (Step 042500): Avg loss 4.694 | 174084096 tokens seen
2025-03-09 15:09:00,078 - INFO - optimizer lr: 0.00000002
2025-03-09 15:09:00,078 - INFO - scheduler lr: 0.00000002
2025-03-09 15:09:00,078 - INFO - Selected prompt: Identity formation is a complex and multifaceted process that involves the development of
2025-03-09 15:09:00,078 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:09:00,078 - INFO - random_topk: 9
2025-03-09 15:09:00,078 - INFO - random_temperature: 0.894193560971299
2025-03-09 15:09:00,078 - INFO - global step 42500 , batch_idx 42500 => generating text
2025-03-09 15:09:00,078 - INFO - Generating on device cuda
2025-03-09 15:09:34,058 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:09:34,060 - INFO - Identity formation is a complex and multifaceted process that involves the development of information and services. The use of systems, systems, and applications of information, can be used for various forms such as computer programming, data science, and even data visualization. One such tool that has gained significant popularity is the study of data collection, which is the process that data is not merely used in machine learning. One such area where data science is the process of creating and creating these data.

### The Importance of Data Data

A model is a type of data that allows users to create the data. This can be used to store and manage information in the data. This is where the most complex systems are stored in a given range of data. For example, in the context of data science, it is a way to understand and create new data. In this case, there is a function called a Python libraries, which is a type of machine learning algorithm that helps us visualize the values of data and how they relate to the data.

### Understanding the Basics of the Data

When we talk about the data, it's important to install and read data. This involves creating the data, we can use various techniques to build, such as data, data processing, and programming. To create an image, we use the `np.com`
2025-03-09 15:09:34,060 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:09:58,836 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_42500_steps_avg_loss_4.69412_optimizer_lr_0.00000002.pth
2025-03-09 15:09:59,145 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 15:09:59,145 - INFO - Batch 42500 finished
2025-03-09 15:09:59,145 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:10:16,404 - INFO - Batch 42526, Running Avg Loss: 4.69402
2025-03-09 15:10:33,888 - INFO - Batch 42551, Running Avg Loss: 4.69394
2025-03-09 15:10:51,651 - INFO - Batch 42576, Running Avg Loss: 4.69384
2025-03-09 15:11:08,960 - INFO - Batch 42601, Running Avg Loss: 4.69378
2025-03-09 15:11:08,972 - INFO - Batch 42600 finished
2025-03-09 15:11:08,973 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:11:26,404 - INFO - Batch 42626, Running Avg Loss: 4.69368
2025-03-09 15:11:43,870 - INFO - Batch 42651, Running Avg Loss: 4.69358
2025-03-09 15:12:01,343 - INFO - Batch 42676, Running Avg Loss: 4.69354
2025-03-09 15:12:18,858 - INFO - Batch 42701, Running Avg Loss: 4.69345
2025-03-09 15:12:18,876 - INFO - Batch 42700 finished
2025-03-09 15:12:18,876 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:12:36,492 - INFO - Batch 42726, Running Avg Loss: 4.69337
2025-03-09 15:12:54,048 - INFO - Batch 42751, Running Avg Loss: 4.69328
2025-03-09 15:13:11,545 - INFO - Batch 42776, Running Avg Loss: 4.69319
2025-03-09 15:13:29,045 - INFO - Batch 42801, Running Avg Loss: 4.69313
2025-03-09 15:13:29,063 - INFO - Batch 42800 finished
2025-03-09 15:13:29,064 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:13:46,630 - INFO - Batch 42826, Running Avg Loss: 4.69309
2025-03-09 15:14:04,370 - INFO - Batch 42851, Running Avg Loss: 4.69302
2025-03-09 15:14:21,987 - INFO - Batch 42876, Running Avg Loss: 4.69295
2025-03-09 15:14:39,547 - INFO - Batch 42901, Running Avg Loss: 4.69292
2025-03-09 15:14:39,564 - INFO - Batch 42900 finished
2025-03-09 15:14:39,564 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:14:57,039 - INFO - Batch 42926, Running Avg Loss: 4.69285
2025-03-09 15:15:14,559 - INFO - Batch 42951, Running Avg Loss: 4.69276
2025-03-09 15:15:32,127 - INFO - Batch 42976, Running Avg Loss: 4.69274
2025-03-09 15:15:49,814 - INFO - Batch 43001, Running Avg Loss: 4.69267
2025-03-09 15:15:49,831 - INFO - Batch 43000 finished
2025-03-09 15:15:49,831 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:16:07,331 - INFO - Batch 43026, Running Avg Loss: 4.69260
2025-03-09 15:16:24,869 - INFO - Batch 43051, Running Avg Loss: 4.69250
2025-03-09 15:16:42,430 - INFO - Batch 43076, Running Avg Loss: 4.69246
2025-03-09 15:17:00,062 - INFO - Batch 43101, Running Avg Loss: 4.69240
2025-03-09 15:17:00,084 - INFO - Batch 43100 finished
2025-03-09 15:17:00,085 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:17:17,756 - INFO - Batch 43126, Running Avg Loss: 4.69229
2025-03-09 15:17:35,553 - INFO - Batch 43151, Running Avg Loss: 4.69220
2025-03-09 15:17:53,117 - INFO - Batch 43176, Running Avg Loss: 4.69214
2025-03-09 15:18:10,789 - INFO - Batch 43201, Running Avg Loss: 4.69211
2025-03-09 15:18:10,805 - INFO - Batch 43200 finished
2025-03-09 15:18:10,805 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:18:28,442 - INFO - Batch 43226, Running Avg Loss: 4.69199
2025-03-09 15:18:46,013 - INFO - Batch 43251, Running Avg Loss: 4.69191
2025-03-09 15:19:03,626 - INFO - Batch 43276, Running Avg Loss: 4.69188
2025-03-09 15:19:21,104 - INFO - Batch 43301, Running Avg Loss: 4.69181
2025-03-09 15:19:21,122 - INFO - Batch 43300 finished
2025-03-09 15:19:21,123 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:19:38,694 - INFO - Batch 43326, Running Avg Loss: 4.69169
2025-03-09 15:19:56,198 - INFO - Batch 43351, Running Avg Loss: 4.69160
2025-03-09 15:20:13,637 - INFO - Batch 43376, Running Avg Loss: 4.69153
2025-03-09 15:20:31,141 - INFO - Batch 43401, Running Avg Loss: 4.69143
2025-03-09 15:20:31,160 - INFO - Batch 43400 finished
2025-03-09 15:20:31,160 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:20:48,864 - INFO - Batch 43426, Running Avg Loss: 4.69134
2025-03-09 15:21:06,398 - INFO - Batch 43451, Running Avg Loss: 4.69127
2025-03-09 15:21:23,936 - INFO - Batch 43476, Running Avg Loss: 4.69118
2025-03-09 15:21:41,502 - INFO - Batch 43501, Running Avg Loss: 4.69110
2025-03-09 15:21:41,517 - INFO - Batch 43500 finished
2025-03-09 15:21:41,518 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:21:59,057 - INFO - Batch 43526, Running Avg Loss: 4.69105
2025-03-09 15:22:16,661 - INFO - Batch 43551, Running Avg Loss: 4.69097
2025-03-09 15:22:34,377 - INFO - Batch 43576, Running Avg Loss: 4.69091
2025-03-09 15:22:51,896 - INFO - Batch 43601, Running Avg Loss: 4.69086
2025-03-09 15:22:51,911 - INFO - Batch 43600 finished
2025-03-09 15:22:51,912 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:23:09,545 - INFO - Batch 43626, Running Avg Loss: 4.69080
2025-03-09 15:23:27,053 - INFO - Batch 43651, Running Avg Loss: 4.69075
2025-03-09 15:23:44,676 - INFO - Batch 43676, Running Avg Loss: 4.69071
2025-03-09 15:24:02,436 - INFO - Batch 43701, Running Avg Loss: 4.69066
2025-03-09 15:24:02,453 - INFO - Batch 43700 finished
2025-03-09 15:24:02,454 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:24:20,017 - INFO - Batch 43726, Running Avg Loss: 4.69054
2025-03-09 15:24:37,545 - INFO - Batch 43751, Running Avg Loss: 4.69042
2025-03-09 15:24:55,022 - INFO - Batch 43776, Running Avg Loss: 4.69036
2025-03-09 15:25:12,553 - INFO - Batch 43801, Running Avg Loss: 4.69031
2025-03-09 15:25:12,569 - INFO - Batch 43800 finished
2025-03-09 15:25:12,569 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:25:30,064 - INFO - Batch 43826, Running Avg Loss: 4.69022
2025-03-09 15:25:47,416 - INFO - Batch 43851, Running Avg Loss: 4.69014
2025-03-09 15:26:04,856 - INFO - Batch 43876, Running Avg Loss: 4.69006
2025-03-09 15:26:22,323 - INFO - Batch 43901, Running Avg Loss: 4.69001
2025-03-09 15:26:22,341 - INFO - Batch 43900 finished
2025-03-09 15:26:22,341 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:26:39,855 - INFO - Batch 43926, Running Avg Loss: 4.68995
2025-03-09 15:26:57,403 - INFO - Batch 43951, Running Avg Loss: 4.68982
2025-03-09 15:27:15,136 - INFO - Batch 43976, Running Avg Loss: 4.68976
2025-03-09 15:27:32,758 - INFO - Batch 44001, Running Avg Loss: 4.68971
2025-03-09 15:27:32,775 - INFO - Batch 44000 finished
2025-03-09 15:27:32,776 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:27:50,244 - INFO - Batch 44026, Running Avg Loss: 4.68961
2025-03-09 15:28:07,765 - INFO - Batch 44051, Running Avg Loss: 4.68952
2025-03-09 15:28:25,238 - INFO - Batch 44076, Running Avg Loss: 4.68939
2025-03-09 15:28:42,694 - INFO - Batch 44101, Running Avg Loss: 4.68931
2025-03-09 15:28:42,710 - INFO - Batch 44100 finished
2025-03-09 15:28:42,711 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:29:00,309 - INFO - Batch 44126, Running Avg Loss: 4.68923
2025-03-09 15:29:17,804 - INFO - Batch 44151, Running Avg Loss: 4.68915
2025-03-09 15:29:35,208 - INFO - Batch 44176, Running Avg Loss: 4.68905
2025-03-09 15:29:52,731 - INFO - Batch 44201, Running Avg Loss: 4.68902
2025-03-09 15:29:52,749 - INFO - Batch 44200 finished
2025-03-09 15:29:52,750 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:30:10,195 - INFO - Batch 44226, Running Avg Loss: 4.68897
2025-03-09 15:30:27,669 - INFO - Batch 44251, Running Avg Loss: 4.68892
2025-03-09 15:30:45,390 - INFO - Batch 44276, Running Avg Loss: 4.68887
2025-03-09 15:31:02,654 - INFO - Batch 44301, Running Avg Loss: 4.68880
2025-03-09 15:31:02,667 - INFO - Batch 44300 finished
2025-03-09 15:31:02,667 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:31:19,996 - INFO - Batch 44326, Running Avg Loss: 4.68874
2025-03-09 15:31:37,522 - INFO - Batch 44351, Running Avg Loss: 4.68867
2025-03-09 15:31:55,063 - INFO - Batch 44376, Running Avg Loss: 4.68860
2025-03-09 15:32:12,586 - INFO - Batch 44401, Running Avg Loss: 4.68856
2025-03-09 15:32:12,605 - INFO - Batch 44400 finished
2025-03-09 15:32:12,606 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:32:30,124 - INFO - Batch 44426, Running Avg Loss: 4.68852
2025-03-09 15:32:47,746 - INFO - Batch 44451, Running Avg Loss: 4.68846
2025-03-09 15:33:05,311 - INFO - Batch 44476, Running Avg Loss: 4.68840
2025-03-09 15:33:22,864 - INFO - Batch 44501, Running Avg Loss: 4.68835
2025-03-09 15:33:22,880 - INFO - Batch 44500 finished
2025-03-09 15:33:22,881 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:33:40,378 - INFO - Batch 44526, Running Avg Loss: 4.68826
2025-03-09 15:33:58,172 - INFO - Batch 44551, Running Avg Loss: 4.68820
2025-03-09 15:34:15,710 - INFO - Batch 44576, Running Avg Loss: 4.68817
2025-03-09 15:34:33,322 - INFO - Batch 44601, Running Avg Loss: 4.68812
2025-03-09 15:34:33,341 - INFO - Batch 44600 finished
2025-03-09 15:34:33,342 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:34:50,853 - INFO - Batch 44626, Running Avg Loss: 4.68807
2025-03-09 15:35:08,371 - INFO - Batch 44651, Running Avg Loss: 4.68798
2025-03-09 15:35:25,876 - INFO - Batch 44676, Running Avg Loss: 4.68794
2025-03-09 15:35:43,573 - INFO - Batch 44701, Running Avg Loss: 4.68785
2025-03-09 15:35:43,590 - INFO - Batch 44700 finished
2025-03-09 15:35:43,591 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:36:01,092 - INFO - Batch 44726, Running Avg Loss: 4.68779
2025-03-09 15:36:18,621 - INFO - Batch 44751, Running Avg Loss: 4.68779
2025-03-09 15:36:36,175 - INFO - Batch 44776, Running Avg Loss: 4.68770
2025-03-09 15:36:53,906 - INFO - Batch 44801, Running Avg Loss: 4.68763
2025-03-09 15:36:53,925 - INFO - Batch 44800 finished
2025-03-09 15:36:53,926 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:37:11,449 - INFO - Batch 44826, Running Avg Loss: 4.68761
2025-03-09 15:37:29,066 - INFO - Batch 44851, Running Avg Loss: 4.68755
2025-03-09 15:37:46,568 - INFO - Batch 44876, Running Avg Loss: 4.68752
2025-03-09 15:38:03,999 - INFO - Batch 44901, Running Avg Loss: 4.68744
2025-03-09 15:38:04,017 - INFO - Batch 44900 finished
2025-03-09 15:38:04,017 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:38:21,504 - INFO - Batch 44926, Running Avg Loss: 4.68736
2025-03-09 15:38:39,071 - INFO - Batch 44951, Running Avg Loss: 4.68729
2025-03-09 15:38:56,673 - INFO - Batch 44976, Running Avg Loss: 4.68726
2025-03-09 15:39:14,224 - INFO - Batch 45001, Running Avg Loss: 4.68715
2025-03-09 15:39:14,240 - INFO - 
GPU Memory Stats at step 45000:
2025-03-09 15:39:14,240 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 15:39:14,240 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 15:39:14,241 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 15:39:14,241 - INFO - learning rate: 0.00000002
2025-03-09 15:39:14,241 - INFO - Ep 1 (Step 045000): Avg loss 4.687 | 184324096 tokens seen
2025-03-09 15:39:14,241 - INFO - optimizer lr: 0.00000002
2025-03-09 15:39:14,241 - INFO - scheduler lr: 0.00000002
2025-03-09 15:39:14,241 - INFO - Selected prompt: Lobster, California spiny The California Spiny Lobster fishery is a small but locally 
2025-03-09 15:39:14,241 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:39:14,241 - INFO - random_topk: 8
2025-03-09 15:39:14,241 - INFO - random_temperature: 0.8519730820638088
2025-03-09 15:39:14,241 - INFO - global step 45000 , batch_idx 45000 => generating text
2025-03-09 15:39:14,241 - INFO - Generating on device cuda
2025-03-09 15:39:48,096 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:39:48,098 - INFO - Lobster, California spiny The California Spiny Lobster fishery is a small but locally 10th-day-day a 26-year-old trip, located along with a few minutes, or a small, a small town in a small town called a "Ao," and its role in the field of "The Bessa," which provides the foundation for the art of music and technology. One such method is the process by connecting with the two elements of the city's atmosphere, which has gained significant contributions to the world and its significance in the field. In this section, we will delve into the concept of a "test of the Gira" in the late 18th century.

Firstly, let us first understand what constitutes a "The National Institute of the American World," which has been passed down through generations. During the late 18th century, the 19th century has been linked to the country's rich cultural heritage. As mentioned at the late 19th century, the US government sought to become one of the most influential figures in the late 19th century.

The term "The Mro," also known as the "The B." series "The Gys" in the mid-1960s, where the 19th century were
2025-03-09 15:39:48,098 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:40:16,154 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_45000_steps_avg_loss_4.68715_optimizer_lr_0.00000002.pth
2025-03-09 15:40:16,515 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 15:40:16,515 - INFO - Batch 45000 finished
2025-03-09 15:40:16,515 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:40:33,760 - INFO - Batch 45026, Running Avg Loss: 4.68708
2025-03-09 15:40:51,157 - INFO - Batch 45051, Running Avg Loss: 4.68702
2025-03-09 15:41:08,320 - INFO - Batch 45076, Running Avg Loss: 4.68695
2025-03-09 15:41:25,760 - INFO - Batch 45101, Running Avg Loss: 4.68688
2025-03-09 15:41:25,777 - INFO - Batch 45100 finished
2025-03-09 15:41:25,777 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:41:43,369 - INFO - Batch 45126, Running Avg Loss: 4.68687
2025-03-09 15:42:01,145 - INFO - Batch 45151, Running Avg Loss: 4.68683
2025-03-09 15:42:18,630 - INFO - Batch 45176, Running Avg Loss: 4.68672
2025-03-09 15:42:36,193 - INFO - Batch 45201, Running Avg Loss: 4.68666
2025-03-09 15:42:36,211 - INFO - Batch 45200 finished
2025-03-09 15:42:36,212 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:42:53,829 - INFO - Batch 45226, Running Avg Loss: 4.68665
2025-03-09 15:43:11,512 - INFO - Batch 45251, Running Avg Loss: 4.68659
2025-03-09 15:43:29,019 - INFO - Batch 45276, Running Avg Loss: 4.68650
2025-03-09 15:43:46,453 - INFO - Batch 45301, Running Avg Loss: 4.68639
2025-03-09 15:43:46,468 - INFO - Batch 45300 finished
2025-03-09 15:43:46,468 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:44:03,914 - INFO - Batch 45326, Running Avg Loss: 4.68632
2025-03-09 15:44:21,434 - INFO - Batch 45351, Running Avg Loss: 4.68625
2025-03-09 15:44:38,937 - INFO - Batch 45376, Running Avg Loss: 4.68616
2025-03-09 15:44:56,415 - INFO - Batch 45401, Running Avg Loss: 4.68612
2025-03-09 15:44:56,432 - INFO - Batch 45400 finished
2025-03-09 15:44:56,432 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:45:13,860 - INFO - Batch 45426, Running Avg Loss: 4.68602
2025-03-09 15:45:31,363 - INFO - Batch 45451, Running Avg Loss: 4.68595
2025-03-09 15:45:48,883 - INFO - Batch 45476, Running Avg Loss: 4.68587
2025-03-09 15:46:06,349 - INFO - Batch 45501, Running Avg Loss: 4.68576
2025-03-09 15:46:06,364 - INFO - Batch 45500 finished
2025-03-09 15:46:06,365 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:46:23,861 - INFO - Batch 45526, Running Avg Loss: 4.68566
2025-03-09 15:46:41,478 - INFO - Batch 45551, Running Avg Loss: 4.68562
2025-03-09 15:46:58,963 - INFO - Batch 45576, Running Avg Loss: 4.68555
2025-03-09 15:47:16,383 - INFO - Batch 45601, Running Avg Loss: 4.68552
2025-03-09 15:47:16,398 - INFO - Batch 45600 finished
2025-03-09 15:47:16,399 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:47:33,792 - INFO - Batch 45626, Running Avg Loss: 4.68548
2025-03-09 15:47:51,220 - INFO - Batch 45651, Running Avg Loss: 4.68545
2025-03-09 15:48:08,815 - INFO - Batch 45676, Running Avg Loss: 4.68536
2025-03-09 15:48:26,338 - INFO - Batch 45701, Running Avg Loss: 4.68526
2025-03-09 15:48:26,355 - INFO - Batch 45700 finished
2025-03-09 15:48:26,356 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:48:43,863 - INFO - Batch 45726, Running Avg Loss: 4.68518
2025-03-09 15:49:01,373 - INFO - Batch 45751, Running Avg Loss: 4.68514
2025-03-09 15:49:18,852 - INFO - Batch 45776, Running Avg Loss: 4.68508
2025-03-09 15:49:36,297 - INFO - Batch 45801, Running Avg Loss: 4.68499
2025-03-09 15:49:36,316 - INFO - Batch 45800 finished
2025-03-09 15:49:36,317 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:49:53,879 - INFO - Batch 45826, Running Avg Loss: 4.68493
2025-03-09 15:50:11,319 - INFO - Batch 45851, Running Avg Loss: 4.68483
2025-03-09 15:50:28,702 - INFO - Batch 45876, Running Avg Loss: 4.68471
2025-03-09 15:50:45,861 - INFO - Batch 45901, Running Avg Loss: 4.68461
2025-03-09 15:50:45,873 - INFO - Batch 45900 finished
2025-03-09 15:50:45,874 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:51:03,019 - INFO - Batch 45926, Running Avg Loss: 4.68454
2025-03-09 15:51:20,381 - INFO - Batch 45951, Running Avg Loss: 4.68450
2025-03-09 15:51:37,872 - INFO - Batch 45976, Running Avg Loss: 4.68447
2025-03-09 15:51:55,369 - INFO - Batch 46001, Running Avg Loss: 4.68440
2025-03-09 15:51:55,386 - INFO - Batch 46000 finished
2025-03-09 15:51:55,386 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:52:12,830 - INFO - Batch 46026, Running Avg Loss: 4.68434
2025-03-09 15:52:30,227 - INFO - Batch 46051, Running Avg Loss: 4.68432
2025-03-09 15:52:47,654 - INFO - Batch 46076, Running Avg Loss: 4.68427
2025-03-09 15:53:05,137 - INFO - Batch 46101, Running Avg Loss: 4.68416
2025-03-09 15:53:05,152 - INFO - Batch 46100 finished
2025-03-09 15:53:05,153 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:53:22,750 - INFO - Batch 46126, Running Avg Loss: 4.68404
2025-03-09 15:53:40,218 - INFO - Batch 46151, Running Avg Loss: 4.68393
2025-03-09 15:53:57,706 - INFO - Batch 46176, Running Avg Loss: 4.68387
2025-03-09 15:54:15,178 - INFO - Batch 46201, Running Avg Loss: 4.68378
2025-03-09 15:54:15,195 - INFO - Batch 46200 finished
2025-03-09 15:54:15,196 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:54:32,673 - INFO - Batch 46226, Running Avg Loss: 4.68375
2025-03-09 15:54:50,093 - INFO - Batch 46251, Running Avg Loss: 4.68370
2025-03-09 15:55:07,595 - INFO - Batch 46276, Running Avg Loss: 4.68368
2025-03-09 15:55:24,933 - INFO - Batch 46301, Running Avg Loss: 4.68358
2025-03-09 15:55:24,948 - INFO - Batch 46300 finished
2025-03-09 15:55:24,948 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:55:42,316 - INFO - Batch 46326, Running Avg Loss: 4.68357
2025-03-09 15:55:59,772 - INFO - Batch 46351, Running Avg Loss: 4.68355
2025-03-09 15:56:17,406 - INFO - Batch 46376, Running Avg Loss: 4.68350
2025-03-09 15:56:34,850 - INFO - Batch 46401, Running Avg Loss: 4.68345
2025-03-09 15:56:34,866 - INFO - Batch 46400 finished
2025-03-09 15:56:34,866 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:56:52,240 - INFO - Batch 46426, Running Avg Loss: 4.68339
2025-03-09 15:57:09,605 - INFO - Batch 46451, Running Avg Loss: 4.68331
2025-03-09 15:57:27,060 - INFO - Batch 46476, Running Avg Loss: 4.68325
2025-03-09 15:57:44,652 - INFO - Batch 46501, Running Avg Loss: 4.68317
2025-03-09 15:57:44,667 - INFO - Batch 46500 finished
2025-03-09 15:57:44,667 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:58:02,111 - INFO - Batch 46526, Running Avg Loss: 4.68310
2025-03-09 15:58:19,594 - INFO - Batch 46551, Running Avg Loss: 4.68304
2025-03-09 15:58:37,092 - INFO - Batch 46576, Running Avg Loss: 4.68296
2025-03-09 15:58:54,634 - INFO - Batch 46601, Running Avg Loss: 4.68290
2025-03-09 15:58:54,654 - INFO - Batch 46600 finished
2025-03-09 15:58:54,654 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 15:59:12,138 - INFO - Batch 46626, Running Avg Loss: 4.68279
2025-03-09 15:59:29,815 - INFO - Batch 46651, Running Avg Loss: 4.68276
2025-03-09 15:59:47,409 - INFO - Batch 46676, Running Avg Loss: 4.68270
2025-03-09 16:00:04,923 - INFO - Batch 46701, Running Avg Loss: 4.68261
2025-03-09 16:00:04,940 - INFO - Batch 46700 finished
2025-03-09 16:00:04,940 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:00:22,362 - INFO - Batch 46726, Running Avg Loss: 4.68251
2025-03-09 16:00:39,789 - INFO - Batch 46751, Running Avg Loss: 4.68242
2025-03-09 16:00:57,238 - INFO - Batch 46776, Running Avg Loss: 4.68235
2025-03-09 16:01:14,482 - INFO - Batch 46801, Running Avg Loss: 4.68227
2025-03-09 16:01:14,497 - INFO - Batch 46800 finished
2025-03-09 16:01:14,498 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:01:31,687 - INFO - Batch 46826, Running Avg Loss: 4.68222
2025-03-09 16:01:49,130 - INFO - Batch 46851, Running Avg Loss: 4.68215
2025-03-09 16:02:06,574 - INFO - Batch 46876, Running Avg Loss: 4.68213
2025-03-09 16:02:24,037 - INFO - Batch 46901, Running Avg Loss: 4.68209
2025-03-09 16:02:24,051 - INFO - Batch 46900 finished
2025-03-09 16:02:24,052 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:02:41,480 - INFO - Batch 46926, Running Avg Loss: 4.68204
2025-03-09 16:02:59,137 - INFO - Batch 46951, Running Avg Loss: 4.68198
2025-03-09 16:03:16,611 - INFO - Batch 46976, Running Avg Loss: 4.68190
2025-03-09 16:03:34,079 - INFO - Batch 47001, Running Avg Loss: 4.68187
2025-03-09 16:03:34,097 - INFO - Batch 47000 finished
2025-03-09 16:03:34,098 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:03:51,558 - INFO - Batch 47026, Running Avg Loss: 4.68182
2025-03-09 16:04:09,075 - INFO - Batch 47051, Running Avg Loss: 4.68171
2025-03-09 16:04:26,570 - INFO - Batch 47076, Running Avg Loss: 4.68165
2025-03-09 16:04:44,084 - INFO - Batch 47101, Running Avg Loss: 4.68167
2025-03-09 16:04:44,099 - INFO - Batch 47100 finished
2025-03-09 16:04:44,099 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:05:01,580 - INFO - Batch 47126, Running Avg Loss: 4.68161
2025-03-09 16:05:19,028 - INFO - Batch 47151, Running Avg Loss: 4.68154
2025-03-09 16:05:36,450 - INFO - Batch 47176, Running Avg Loss: 4.68151
2025-03-09 16:05:53,887 - INFO - Batch 47201, Running Avg Loss: 4.68144
2025-03-09 16:05:53,903 - INFO - Batch 47200 finished
2025-03-09 16:05:53,903 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:06:11,487 - INFO - Batch 47226, Running Avg Loss: 4.68136
2025-03-09 16:06:28,926 - INFO - Batch 47251, Running Avg Loss: 4.68130
2025-03-09 16:06:46,369 - INFO - Batch 47276, Running Avg Loss: 4.68122
2025-03-09 16:07:03,816 - INFO - Batch 47301, Running Avg Loss: 4.68112
2025-03-09 16:07:03,833 - INFO - Batch 47300 finished
2025-03-09 16:07:03,833 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:07:21,255 - INFO - Batch 47326, Running Avg Loss: 4.68106
2025-03-09 16:07:38,688 - INFO - Batch 47351, Running Avg Loss: 4.68098
2025-03-09 16:07:56,186 - INFO - Batch 47376, Running Avg Loss: 4.68094
2025-03-09 16:08:13,798 - INFO - Batch 47401, Running Avg Loss: 4.68091
2025-03-09 16:08:13,817 - INFO - Batch 47400 finished
2025-03-09 16:08:13,817 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:08:31,266 - INFO - Batch 47426, Running Avg Loss: 4.68086
2025-03-09 16:08:48,714 - INFO - Batch 47451, Running Avg Loss: 4.68080
2025-03-09 16:09:06,165 - INFO - Batch 47476, Running Avg Loss: 4.68071
2025-03-09 16:09:23,754 - INFO - Batch 47501, Running Avg Loss: 4.68067
2025-03-09 16:09:23,769 - INFO - 
GPU Memory Stats at step 47500:
2025-03-09 16:09:23,769 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 16:09:23,769 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 16:09:23,770 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 16:09:23,770 - INFO - learning rate: 0.00000001
2025-03-09 16:09:23,770 - INFO - Ep 1 (Step 047500): Avg loss 4.681 | 194564096 tokens seen
2025-03-09 16:09:23,770 - INFO - optimizer lr: 0.00000001
2025-03-09 16:09:23,770 - INFO - scheduler lr: 0.00000001
2025-03-09 16:09:23,770 - INFO - Selected prompt: Introduction: The Art of Crafting Vegan Sandwich Delights Sandwiches occupy a unique space in
2025-03-09 16:09:23,770 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:09:23,771 - INFO - random_topk: 7
2025-03-09 16:09:23,771 - INFO - random_temperature: 0.7138953340951454
2025-03-09 16:09:23,771 - INFO - global step 47500 , batch_idx 47500 => generating text
2025-03-09 16:09:23,771 - INFO - Generating on device cuda
2025-03-09 16:09:57,278 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:09:57,279 - INFO - Introduction: The Art of Crafting Vegan Sandwich Delights Sandwiches occupy a unique space in ancient civilizations, where it was a powerful form of entertainment and engineering. This chapter will delve into the history, techniques, and techniques employed by contemporary artists to explore the historical roots of the Indian Empire. We will focus on the historical context of modern-day comics and graphic design, highlighting how these techniques have contributed to the broader themes of this unique and innovative technique. Through this exploration, you will gain a deeper appreciation for the rich tapestry of the art form, drawing attention to the intersection of the region.

I. Historical Context - A Brief History of the American Civil War

A. Definition and Evolution

The origins of the Roman Empire, which has been marked by significant advancements in modern times, shaping the lives of the United States. As part of the late 19th century, it was primarily a powerful leader in the 19th century. During the early 20th century, the British Empire had been a unique blend of cultural significance and cultural influences. As such, many contemporary artists like the 20th century, the emergence of the American Civil War, which was a powerful symbol of the ancient Chinese city, which led to the British colonial era.

B. The Power of the 1800s

2025-03-09 16:09:57,279 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:10:21,953 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_47500_steps_avg_loss_4.68067_optimizer_lr_0.00000001.pth
2025-03-09 16:10:22,193 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 16:10:22,193 - INFO - Batch 47500 finished
2025-03-09 16:10:22,193 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:10:39,453 - INFO - Batch 47526, Running Avg Loss: 4.68059
2025-03-09 16:10:56,643 - INFO - Batch 47551, Running Avg Loss: 4.68053
2025-03-09 16:11:14,005 - INFO - Batch 47576, Running Avg Loss: 4.68050
2025-03-09 16:11:31,397 - INFO - Batch 47601, Running Avg Loss: 4.68048
2025-03-09 16:11:31,412 - INFO - Batch 47600 finished
2025-03-09 16:11:31,412 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:11:48,842 - INFO - Batch 47626, Running Avg Loss: 4.68039
2025-03-09 16:12:06,368 - INFO - Batch 47651, Running Avg Loss: 4.68035
2025-03-09 16:12:23,856 - INFO - Batch 47676, Running Avg Loss: 4.68029
2025-03-09 16:12:41,210 - INFO - Batch 47701, Running Avg Loss: 4.68027
2025-03-09 16:12:41,224 - INFO - Batch 47700 finished
2025-03-09 16:12:41,225 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:12:58,573 - INFO - Batch 47726, Running Avg Loss: 4.68022
2025-03-09 16:13:15,961 - INFO - Batch 47751, Running Avg Loss: 4.68015
2025-03-09 16:13:33,459 - INFO - Batch 47776, Running Avg Loss: 4.68010
2025-03-09 16:13:50,966 - INFO - Batch 47801, Running Avg Loss: 4.68007
2025-03-09 16:13:50,983 - INFO - Batch 47800 finished
2025-03-09 16:13:50,983 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:14:08,439 - INFO - Batch 47826, Running Avg Loss: 4.68001
2025-03-09 16:14:26,122 - INFO - Batch 47851, Running Avg Loss: 4.67994
2025-03-09 16:14:43,543 - INFO - Batch 47876, Running Avg Loss: 4.67987
2025-03-09 16:15:01,011 - INFO - Batch 47901, Running Avg Loss: 4.67979
2025-03-09 16:15:01,029 - INFO - Batch 47900 finished
2025-03-09 16:15:01,029 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:15:18,517 - INFO - Batch 47926, Running Avg Loss: 4.67969
2025-03-09 16:15:35,993 - INFO - Batch 47951, Running Avg Loss: 4.67962
2025-03-09 16:15:53,512 - INFO - Batch 47976, Running Avg Loss: 4.67954
2025-03-09 16:16:11,128 - INFO - Batch 48001, Running Avg Loss: 4.67947
2025-03-09 16:16:11,149 - INFO - Batch 48000 finished
2025-03-09 16:16:11,150 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:16:28,634 - INFO - Batch 48026, Running Avg Loss: 4.67938
2025-03-09 16:16:46,125 - INFO - Batch 48051, Running Avg Loss: 4.67933
2025-03-09 16:17:03,598 - INFO - Batch 48076, Running Avg Loss: 4.67927
2025-03-09 16:17:21,008 - INFO - Batch 48101, Running Avg Loss: 4.67923
2025-03-09 16:17:21,027 - INFO - Batch 48100 finished
2025-03-09 16:17:21,028 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:17:38,496 - INFO - Batch 48126, Running Avg Loss: 4.67912
2025-03-09 16:17:56,258 - INFO - Batch 48151, Running Avg Loss: 4.67904
2025-03-09 16:18:13,829 - INFO - Batch 48176, Running Avg Loss: 4.67901
2025-03-09 16:18:31,292 - INFO - Batch 48201, Running Avg Loss: 4.67895
2025-03-09 16:18:31,307 - INFO - Batch 48200 finished
2025-03-09 16:18:31,307 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:18:48,772 - INFO - Batch 48226, Running Avg Loss: 4.67889
2025-03-09 16:19:06,230 - INFO - Batch 48251, Running Avg Loss: 4.67882
2025-03-09 16:19:23,740 - INFO - Batch 48276, Running Avg Loss: 4.67879
2025-03-09 16:19:41,285 - INFO - Batch 48301, Running Avg Loss: 4.67876
2025-03-09 16:19:41,300 - INFO - Batch 48300 finished
2025-03-09 16:19:41,301 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:19:58,809 - INFO - Batch 48326, Running Avg Loss: 4.67869
2025-03-09 16:20:16,375 - INFO - Batch 48351, Running Avg Loss: 4.67864
2025-03-09 16:20:33,928 - INFO - Batch 48376, Running Avg Loss: 4.67857
2025-03-09 16:20:51,446 - INFO - Batch 48401, Running Avg Loss: 4.67855
2025-03-09 16:20:51,464 - INFO - Batch 48400 finished
2025-03-09 16:20:51,465 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:21:09,116 - INFO - Batch 48426, Running Avg Loss: 4.67846
2025-03-09 16:21:26,619 - INFO - Batch 48451, Running Avg Loss: 4.67836
2025-03-09 16:21:44,156 - INFO - Batch 48476, Running Avg Loss: 4.67833
2025-03-09 16:22:01,836 - INFO - Batch 48501, Running Avg Loss: 4.67827
2025-03-09 16:22:01,852 - INFO - Batch 48500 finished
2025-03-09 16:22:01,852 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:22:19,478 - INFO - Batch 48526, Running Avg Loss: 4.67819
2025-03-09 16:22:36,978 - INFO - Batch 48551, Running Avg Loss: 4.67809
2025-03-09 16:22:54,627 - INFO - Batch 48576, Running Avg Loss: 4.67805
2025-03-09 16:23:12,171 - INFO - Batch 48601, Running Avg Loss: 4.67798
2025-03-09 16:23:12,186 - INFO - Batch 48600 finished
2025-03-09 16:23:12,187 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:23:29,752 - INFO - Batch 48626, Running Avg Loss: 4.67792
2025-03-09 16:23:47,351 - INFO - Batch 48651, Running Avg Loss: 4.67790
2025-03-09 16:24:04,967 - INFO - Batch 48676, Running Avg Loss: 4.67783
2025-03-09 16:24:22,678 - INFO - Batch 48701, Running Avg Loss: 4.67774
2025-03-09 16:24:22,694 - INFO - Batch 48700 finished
2025-03-09 16:24:22,695 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:24:40,277 - INFO - Batch 48726, Running Avg Loss: 4.67769
2025-03-09 16:24:57,846 - INFO - Batch 48751, Running Avg Loss: 4.67762
2025-03-09 16:25:15,367 - INFO - Batch 48776, Running Avg Loss: 4.67761
2025-03-09 16:25:32,856 - INFO - Batch 48801, Running Avg Loss: 4.67757
2025-03-09 16:25:32,871 - INFO - Batch 48800 finished
2025-03-09 16:25:32,872 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:25:50,360 - INFO - Batch 48826, Running Avg Loss: 4.67747
2025-03-09 16:26:07,905 - INFO - Batch 48851, Running Avg Loss: 4.67739
2025-03-09 16:26:25,543 - INFO - Batch 48876, Running Avg Loss: 4.67733
2025-03-09 16:26:43,181 - INFO - Batch 48901, Running Avg Loss: 4.67725
2025-03-09 16:26:43,199 - INFO - Batch 48900 finished
2025-03-09 16:26:43,199 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:27:00,776 - INFO - Batch 48926, Running Avg Loss: 4.67718
2025-03-09 16:27:18,388 - INFO - Batch 48951, Running Avg Loss: 4.67711
2025-03-09 16:27:36,127 - INFO - Batch 48976, Running Avg Loss: 4.67704
2025-03-09 16:27:53,592 - INFO - Batch 49001, Running Avg Loss: 4.67702
2025-03-09 16:27:53,608 - INFO - Batch 49000 finished
2025-03-09 16:27:53,609 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:28:11,063 - INFO - Batch 49026, Running Avg Loss: 4.67699
2025-03-09 16:28:28,630 - INFO - Batch 49051, Running Avg Loss: 4.67697
2025-03-09 16:28:46,177 - INFO - Batch 49076, Running Avg Loss: 4.67693
2025-03-09 16:29:03,707 - INFO - Batch 49101, Running Avg Loss: 4.67691
2025-03-09 16:29:03,722 - INFO - Batch 49100 finished
2025-03-09 16:29:03,723 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:29:21,330 - INFO - Batch 49126, Running Avg Loss: 4.67681
2025-03-09 16:29:38,915 - INFO - Batch 49151, Running Avg Loss: 4.67678
2025-03-09 16:29:56,547 - INFO - Batch 49176, Running Avg Loss: 4.67670
2025-03-09 16:30:14,187 - INFO - Batch 49201, Running Avg Loss: 4.67664
2025-03-09 16:30:14,207 - INFO - Batch 49200 finished
2025-03-09 16:30:14,207 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:30:31,693 - INFO - Batch 49226, Running Avg Loss: 4.67660
2025-03-09 16:30:49,017 - INFO - Batch 49251, Running Avg Loss: 4.67655
2025-03-09 16:31:06,338 - INFO - Batch 49276, Running Avg Loss: 4.67653
2025-03-09 16:31:23,747 - INFO - Batch 49301, Running Avg Loss: 4.67649
2025-03-09 16:31:23,764 - INFO - Batch 49300 finished
2025-03-09 16:31:23,764 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:31:41,265 - INFO - Batch 49326, Running Avg Loss: 4.67645
2025-03-09 16:31:58,828 - INFO - Batch 49351, Running Avg Loss: 4.67642
2025-03-09 16:32:16,357 - INFO - Batch 49376, Running Avg Loss: 4.67634
2025-03-09 16:32:33,887 - INFO - Batch 49401, Running Avg Loss: 4.67626
2025-03-09 16:32:33,904 - INFO - Batch 49400 finished
2025-03-09 16:32:33,905 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:32:51,338 - INFO - Batch 49426, Running Avg Loss: 4.67622
2025-03-09 16:33:08,815 - INFO - Batch 49451, Running Avg Loss: 4.67618
2025-03-09 16:33:26,311 - INFO - Batch 49476, Running Avg Loss: 4.67614
2025-03-09 16:33:43,780 - INFO - Batch 49501, Running Avg Loss: 4.67613
2025-03-09 16:33:43,795 - INFO - Batch 49500 finished
2025-03-09 16:33:43,796 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:34:01,303 - INFO - Batch 49526, Running Avg Loss: 4.67612
2025-03-09 16:34:18,957 - INFO - Batch 49551, Running Avg Loss: 4.67604
2025-03-09 16:34:36,448 - INFO - Batch 49576, Running Avg Loss: 4.67598
2025-03-09 16:34:53,942 - INFO - Batch 49601, Running Avg Loss: 4.67590
2025-03-09 16:34:53,959 - INFO - Batch 49600 finished
2025-03-09 16:34:53,959 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:35:11,471 - INFO - Batch 49626, Running Avg Loss: 4.67586
2025-03-09 16:35:28,947 - INFO - Batch 49651, Running Avg Loss: 4.67579
2025-03-09 16:35:46,439 - INFO - Batch 49676, Running Avg Loss: 4.67577
2025-03-09 16:36:04,081 - INFO - Batch 49701, Running Avg Loss: 4.67572
2025-03-09 16:36:04,100 - INFO - Batch 49700 finished
2025-03-09 16:36:04,101 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:36:21,564 - INFO - Batch 49726, Running Avg Loss: 4.67566
2025-03-09 16:36:39,041 - INFO - Batch 49751, Running Avg Loss: 4.67561
2025-03-09 16:36:56,526 - INFO - Batch 49776, Running Avg Loss: 4.67554
2025-03-09 16:37:14,148 - INFO - Batch 49801, Running Avg Loss: 4.67548
2025-03-09 16:37:14,165 - INFO - Batch 49800 finished
2025-03-09 16:37:14,166 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:37:31,604 - INFO - Batch 49826, Running Avg Loss: 4.67545
2025-03-09 16:37:49,091 - INFO - Batch 49851, Running Avg Loss: 4.67541
2025-03-09 16:38:06,574 - INFO - Batch 49876, Running Avg Loss: 4.67537
2025-03-09 16:38:24,055 - INFO - Batch 49901, Running Avg Loss: 4.67530
2025-03-09 16:38:24,070 - INFO - Batch 49900 finished
2025-03-09 16:38:24,071 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:38:41,561 - INFO - Batch 49926, Running Avg Loss: 4.67527
2025-03-09 16:38:59,120 - INFO - Batch 49951, Running Avg Loss: 4.67519
2025-03-09 16:39:16,655 - INFO - Batch 49976, Running Avg Loss: 4.67508
2025-03-09 16:39:34,123 - INFO - Batch 50001, Running Avg Loss: 4.67501
2025-03-09 16:39:34,138 - INFO - 
GPU Memory Stats at step 50000:
2025-03-09 16:39:34,139 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 16:39:34,139 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 16:39:34,139 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 16:39:34,139 - INFO - learning rate: 0.00000001
2025-03-09 16:39:34,139 - INFO - Ep 1 (Step 050000): Avg loss 4.675 | 204804096 tokens seen
2025-03-09 16:39:34,139 - INFO - optimizer lr: 0.00000001
2025-03-09 16:39:34,139 - INFO - scheduler lr: 0.00000001
2025-03-09 16:39:34,139 - INFO - Selected prompt: Once upon a time, in a colorful town called Popville, 
2025-03-09 16:39:34,139 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:39:34,141 - INFO - random_topk: 1
2025-03-09 16:39:34,141 - INFO - random_temperature: 0.7224602859095497
2025-03-09 16:39:34,141 - INFO - global step 50000 , batch_idx 50000 => generating text
2025-03-09 16:39:34,141 - INFO - Generating on device cuda
2025-03-09 16:40:06,806 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:40:06,806 - INFO - Once upon a time, in a colorful town called Popville, 1900, there lived many different kinds of animals who lived in the world. One day, they decided to learn about something called "The Gira" - the "The Gira" and the "The Gira" in the 19th century.

One day, while playing with a big, beautiful city named the Squirrel, they stumbled upon a big, beautiful city called the "The Gira" and the "The Gira" in the 19th century. This made them think of it like a big, beautiful city filled with beautiful buildings, and sometimes even a big, beautiful city!

One day, a group of people came from a big city called the "The Gira" and the "The Gira" in the 19th century. He was known for his powerful leaders, but he had a special place called "The Gira".

One day, while playing with his friends, he noticed something strange happening in the town. He asked, "What does it mean to be a big,?"

"Well, I see. It's a big, beautiful city where people lived in the world. They wanted to learn about the world around them and make sure everyone had the
2025-03-09 16:40:06,806 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:40:31,121 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_50000_steps_avg_loss_4.67501_optimizer_lr_0.00000001.pth
2025-03-09 16:40:31,434 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 16:40:31,434 - INFO - Batch 50000 finished
2025-03-09 16:40:31,434 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:40:48,454 - INFO - Batch 50026, Running Avg Loss: 4.67498
2025-03-09 16:41:05,647 - INFO - Batch 50051, Running Avg Loss: 4.67493
2025-03-09 16:41:23,151 - INFO - Batch 50076, Running Avg Loss: 4.67487
2025-03-09 16:41:40,588 - INFO - Batch 50101, Running Avg Loss: 4.67481
2025-03-09 16:41:40,603 - INFO - Batch 50100 finished
2025-03-09 16:41:40,603 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:41:58,006 - INFO - Batch 50126, Running Avg Loss: 4.67475
2025-03-09 16:42:15,557 - INFO - Batch 50151, Running Avg Loss: 4.67471
2025-03-09 16:42:33,055 - INFO - Batch 50176, Running Avg Loss: 4.67468
2025-03-09 16:42:50,499 - INFO - Batch 50201, Running Avg Loss: 4.67461
2025-03-09 16:42:50,515 - INFO - Batch 50200 finished
2025-03-09 16:42:50,516 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:43:07,989 - INFO - Batch 50226, Running Avg Loss: 4.67451
2025-03-09 16:43:25,658 - INFO - Batch 50251, Running Avg Loss: 4.67441
2025-03-09 16:43:43,164 - INFO - Batch 50276, Running Avg Loss: 4.67433
2025-03-09 16:44:00,603 - INFO - Batch 50301, Running Avg Loss: 4.67429
2025-03-09 16:44:00,619 - INFO - Batch 50300 finished
2025-03-09 16:44:00,619 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:44:18,006 - INFO - Batch 50326, Running Avg Loss: 4.67429
2025-03-09 16:44:35,403 - INFO - Batch 50351, Running Avg Loss: 4.67423
2025-03-09 16:44:52,840 - INFO - Batch 50376, Running Avg Loss: 4.67419
2025-03-09 16:45:10,334 - INFO - Batch 50401, Running Avg Loss: 4.67411
2025-03-09 16:45:10,351 - INFO - Batch 50400 finished
2025-03-09 16:45:10,352 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:45:27,806 - INFO - Batch 50426, Running Avg Loss: 4.67405
2025-03-09 16:45:45,204 - INFO - Batch 50451, Running Avg Loss: 4.67400
2025-03-09 16:46:02,582 - INFO - Batch 50476, Running Avg Loss: 4.67389
2025-03-09 16:46:19,969 - INFO - Batch 50501, Running Avg Loss: 4.67386
2025-03-09 16:46:19,985 - INFO - Batch 50500 finished
2025-03-09 16:46:19,985 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:46:37,459 - INFO - Batch 50526, Running Avg Loss: 4.67376
2025-03-09 16:46:55,097 - INFO - Batch 50551, Running Avg Loss: 4.67364
2025-03-09 16:47:12,511 - INFO - Batch 50576, Running Avg Loss: 4.67359
2025-03-09 16:47:29,933 - INFO - Batch 50601, Running Avg Loss: 4.67353
2025-03-09 16:47:29,948 - INFO - Batch 50600 finished
2025-03-09 16:47:29,949 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:47:47,359 - INFO - Batch 50626, Running Avg Loss: 4.67349
2025-03-09 16:48:04,862 - INFO - Batch 50651, Running Avg Loss: 4.67346
2025-03-09 16:48:22,334 - INFO - Batch 50676, Running Avg Loss: 4.67338
2025-03-09 16:48:39,684 - INFO - Batch 50701, Running Avg Loss: 4.67335
2025-03-09 16:48:39,700 - INFO - Batch 50700 finished
2025-03-09 16:48:39,701 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:48:57,067 - INFO - Batch 50726, Running Avg Loss: 4.67328
2025-03-09 16:49:14,512 - INFO - Batch 50751, Running Avg Loss: 4.67321
2025-03-09 16:49:32,005 - INFO - Batch 50776, Running Avg Loss: 4.67317
2025-03-09 16:49:49,391 - INFO - Batch 50801, Running Avg Loss: 4.67308
2025-03-09 16:49:49,408 - INFO - Batch 50800 finished
2025-03-09 16:49:49,409 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:50:06,860 - INFO - Batch 50826, Running Avg Loss: 4.67301
2025-03-09 16:50:24,192 - INFO - Batch 50851, Running Avg Loss: 4.67300
2025-03-09 16:50:41,572 - INFO - Batch 50876, Running Avg Loss: 4.67293
2025-03-09 16:50:58,765 - INFO - Batch 50901, Running Avg Loss: 4.67291
2025-03-09 16:50:58,777 - INFO - Batch 50900 finished
2025-03-09 16:50:58,778 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:51:16,183 - INFO - Batch 50926, Running Avg Loss: 4.67283
2025-03-09 16:51:33,587 - INFO - Batch 50951, Running Avg Loss: 4.67277
2025-03-09 16:51:50,898 - INFO - Batch 50976, Running Avg Loss: 4.67269
2025-03-09 16:52:08,245 - INFO - Batch 51001, Running Avg Loss: 4.67265
2025-03-09 16:52:08,263 - INFO - Batch 51000 finished
2025-03-09 16:52:08,263 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:52:25,545 - INFO - Batch 51026, Running Avg Loss: 4.67259
2025-03-09 16:52:42,851 - INFO - Batch 51051, Running Avg Loss: 4.67257
2025-03-09 16:53:00,302 - INFO - Batch 51076, Running Avg Loss: 4.67252
2025-03-09 16:53:17,797 - INFO - Batch 51101, Running Avg Loss: 4.67247
2025-03-09 16:53:17,811 - INFO - Batch 51100 finished
2025-03-09 16:53:17,811 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:53:35,399 - INFO - Batch 51126, Running Avg Loss: 4.67241
2025-03-09 16:53:52,873 - INFO - Batch 51151, Running Avg Loss: 4.67236
2025-03-09 16:54:10,386 - INFO - Batch 51176, Running Avg Loss: 4.67229
2025-03-09 16:54:27,896 - INFO - Batch 51201, Running Avg Loss: 4.67225
2025-03-09 16:54:27,915 - INFO - Batch 51200 finished
2025-03-09 16:54:27,916 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:54:45,384 - INFO - Batch 51226, Running Avg Loss: 4.67216
2025-03-09 16:55:02,866 - INFO - Batch 51251, Running Avg Loss: 4.67213
2025-03-09 16:55:20,536 - INFO - Batch 51276, Running Avg Loss: 4.67207
2025-03-09 16:55:38,106 - INFO - Batch 51301, Running Avg Loss: 4.67204
2025-03-09 16:55:38,123 - INFO - Batch 51300 finished
2025-03-09 16:55:38,124 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:55:55,669 - INFO - Batch 51326, Running Avg Loss: 4.67199
2025-03-09 16:56:13,229 - INFO - Batch 51351, Running Avg Loss: 4.67191
2025-03-09 16:56:30,975 - INFO - Batch 51376, Running Avg Loss: 4.67186
2025-03-09 16:56:48,489 - INFO - Batch 51401, Running Avg Loss: 4.67184
2025-03-09 16:56:48,503 - INFO - Batch 51400 finished
2025-03-09 16:56:48,504 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:57:05,932 - INFO - Batch 51426, Running Avg Loss: 4.67176
2025-03-09 16:57:23,445 - INFO - Batch 51451, Running Avg Loss: 4.67172
2025-03-09 16:57:40,957 - INFO - Batch 51476, Running Avg Loss: 4.67170
2025-03-09 16:57:58,435 - INFO - Batch 51501, Running Avg Loss: 4.67162
2025-03-09 16:57:58,451 - INFO - Batch 51500 finished
2025-03-09 16:57:58,452 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:58:15,812 - INFO - Batch 51526, Running Avg Loss: 4.67157
2025-03-09 16:58:33,141 - INFO - Batch 51551, Running Avg Loss: 4.67151
2025-03-09 16:58:50,543 - INFO - Batch 51576, Running Avg Loss: 4.67147
2025-03-09 16:59:08,013 - INFO - Batch 51601, Running Avg Loss: 4.67140
2025-03-09 16:59:08,030 - INFO - Batch 51600 finished
2025-03-09 16:59:08,030 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 16:59:25,598 - INFO - Batch 51626, Running Avg Loss: 4.67134
2025-03-09 16:59:43,357 - INFO - Batch 51651, Running Avg Loss: 4.67131
2025-03-09 17:00:00,946 - INFO - Batch 51676, Running Avg Loss: 4.67129
2025-03-09 17:00:18,477 - INFO - Batch 51701, Running Avg Loss: 4.67122
2025-03-09 17:00:18,494 - INFO - Batch 51700 finished
2025-03-09 17:00:18,494 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:00:36,015 - INFO - Batch 51726, Running Avg Loss: 4.67117
2025-03-09 17:00:53,430 - INFO - Batch 51751, Running Avg Loss: 4.67111
2025-03-09 17:01:10,634 - INFO - Batch 51776, Running Avg Loss: 4.67106
2025-03-09 17:01:28,003 - INFO - Batch 51801, Running Avg Loss: 4.67103
2025-03-09 17:01:28,021 - INFO - Batch 51800 finished
2025-03-09 17:01:28,021 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:01:45,480 - INFO - Batch 51826, Running Avg Loss: 4.67097
2025-03-09 17:02:02,873 - INFO - Batch 51851, Running Avg Loss: 4.67093
2025-03-09 17:02:20,245 - INFO - Batch 51876, Running Avg Loss: 4.67090
2025-03-09 17:02:37,628 - INFO - Batch 51901, Running Avg Loss: 4.67081
2025-03-09 17:02:37,643 - INFO - Batch 51900 finished
2025-03-09 17:02:37,644 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:02:55,094 - INFO - Batch 51926, Running Avg Loss: 4.67075
2025-03-09 17:03:12,693 - INFO - Batch 51951, Running Avg Loss: 4.67071
2025-03-09 17:03:30,059 - INFO - Batch 51976, Running Avg Loss: 4.67066
2025-03-09 17:03:47,362 - INFO - Batch 52001, Running Avg Loss: 4.67055
2025-03-09 17:03:47,377 - INFO - Batch 52000 finished
2025-03-09 17:03:47,377 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:04:04,711 - INFO - Batch 52026, Running Avg Loss: 4.67049
2025-03-09 17:04:22,117 - INFO - Batch 52051, Running Avg Loss: 4.67044
2025-03-09 17:04:39,627 - INFO - Batch 52076, Running Avg Loss: 4.67042
2025-03-09 17:04:57,112 - INFO - Batch 52101, Running Avg Loss: 4.67038
2025-03-09 17:04:57,129 - INFO - Batch 52100 finished
2025-03-09 17:04:57,129 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:05:14,502 - INFO - Batch 52126, Running Avg Loss: 4.67033
2025-03-09 17:05:31,885 - INFO - Batch 52151, Running Avg Loss: 4.67029
2025-03-09 17:05:49,343 - INFO - Batch 52176, Running Avg Loss: 4.67020
2025-03-09 17:06:06,785 - INFO - Batch 52201, Running Avg Loss: 4.67018
2025-03-09 17:06:06,801 - INFO - Batch 52200 finished
2025-03-09 17:06:06,802 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:06:24,352 - INFO - Batch 52226, Running Avg Loss: 4.67014
2025-03-09 17:06:41,723 - INFO - Batch 52251, Running Avg Loss: 4.67008
2025-03-09 17:06:59,131 - INFO - Batch 52276, Running Avg Loss: 4.67003
2025-03-09 17:07:16,625 - INFO - Batch 52301, Running Avg Loss: 4.67001
2025-03-09 17:07:16,640 - INFO - Batch 52300 finished
2025-03-09 17:07:16,640 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:07:34,147 - INFO - Batch 52326, Running Avg Loss: 4.66998
2025-03-09 17:07:51,639 - INFO - Batch 52351, Running Avg Loss: 4.66986
2025-03-09 17:08:09,169 - INFO - Batch 52376, Running Avg Loss: 4.66984
2025-03-09 17:08:26,760 - INFO - Batch 52401, Running Avg Loss: 4.66982
2025-03-09 17:08:26,778 - INFO - Batch 52400 finished
2025-03-09 17:08:26,779 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:08:44,150 - INFO - Batch 52426, Running Avg Loss: 4.66976
2025-03-09 17:09:01,549 - INFO - Batch 52451, Running Avg Loss: 4.66974
2025-03-09 17:09:18,976 - INFO - Batch 52476, Running Avg Loss: 4.66970
2025-03-09 17:09:36,647 - INFO - Batch 52501, Running Avg Loss: 4.66964
2025-03-09 17:09:36,665 - INFO - 
GPU Memory Stats at step 52500:
2025-03-09 17:09:36,666 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 17:09:36,666 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 17:09:36,666 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 17:09:36,666 - INFO - learning rate: 0.00000001
2025-03-09 17:09:36,666 - INFO - Ep 1 (Step 052500): Avg loss 4.670 | 215044096 tokens seen
2025-03-09 17:09:36,666 - INFO - optimizer lr: 0.00000001
2025-03-09 17:09:36,666 - INFO - scheduler lr: 0.00000001
2025-03-09 17:09:36,666 - INFO - Selected prompt: Lobster, California spiny The California Spiny Lobster fishery is a small but locally 
2025-03-09 17:09:36,666 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:09:36,668 - INFO - random_topk: 7
2025-03-09 17:09:36,668 - INFO - random_temperature: 0.8457515141820999
2025-03-09 17:09:36,668 - INFO - global step 52500 , batch_idx 52500 => generating text
2025-03-09 17:09:36,668 - INFO - Generating on device cuda
2025-03-09 17:10:09,945 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:10:09,945 - INFO - Lobster, California spiny The California Spiny Lobster fishery is a small but locally 1990, but it's a way to take care of the past. One such technique is the idea, which is called "the "A," which involves a set of two or more elements in the world. This makes it easy for us to think about the basics of the world around us and discover new ways to understand the importance of the complex nature of the universe in which the universe is.

The first step in this course unit is to understand what we mean when it comes to the world of technology. It is a form of technology that has been around for centuries, but it has been an essential concept within the field of technology and science. By examining these concepts, we can gain a deeper appreciation for how people can help create their own unique experiences.

One example of the most influential artists who have made significant contributions to the development of the development of the study. They are working hard to create a more nuanced understanding of the complex world, like the "Sad," which refers to a particular group of people who are often seen in the globe. The same language is to create a new world of music  all while the story of the world is created by the world.

One of the most influential works that we know is by "
2025-03-09 17:10:09,945 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:10:34,773 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_52500_steps_avg_loss_4.66964_optimizer_lr_0.00000001.pth
2025-03-09 17:10:35,054 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 17:10:35,054 - INFO - Batch 52500 finished
2025-03-09 17:10:35,054 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:10:52,377 - INFO - Batch 52526, Running Avg Loss: 4.66961
2025-03-09 17:11:09,572 - INFO - Batch 52551, Running Avg Loss: 4.66957
2025-03-09 17:11:27,142 - INFO - Batch 52576, Running Avg Loss: 4.66953
2025-03-09 17:11:44,556 - INFO - Batch 52601, Running Avg Loss: 4.66948
2025-03-09 17:11:44,573 - INFO - Batch 52600 finished
2025-03-09 17:11:44,573 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:12:02,013 - INFO - Batch 52626, Running Avg Loss: 4.66948
2025-03-09 17:12:19,453 - INFO - Batch 52651, Running Avg Loss: 4.66942
2025-03-09 17:12:36,928 - INFO - Batch 52676, Running Avg Loss: 4.66936
2025-03-09 17:12:54,440 - INFO - Batch 52701, Running Avg Loss: 4.66934
2025-03-09 17:12:54,459 - INFO - Batch 52700 finished
2025-03-09 17:12:54,460 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:13:11,941 - INFO - Batch 52726, Running Avg Loss: 4.66927
2025-03-09 17:13:29,517 - INFO - Batch 52751, Running Avg Loss: 4.66920
2025-03-09 17:13:47,020 - INFO - Batch 52776, Running Avg Loss: 4.66920
2025-03-09 17:14:04,453 - INFO - Batch 52801, Running Avg Loss: 4.66919
2025-03-09 17:14:04,472 - INFO - Batch 52800 finished
2025-03-09 17:14:04,472 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:14:21,908 - INFO - Batch 52826, Running Avg Loss: 4.66918
2025-03-09 17:14:39,525 - INFO - Batch 52851, Running Avg Loss: 4.66911
2025-03-09 17:14:57,001 - INFO - Batch 52876, Running Avg Loss: 4.66908
2025-03-09 17:15:14,414 - INFO - Batch 52901, Running Avg Loss: 4.66899
2025-03-09 17:15:14,431 - INFO - Batch 52900 finished
2025-03-09 17:15:14,432 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:15:31,829 - INFO - Batch 52926, Running Avg Loss: 4.66898
2025-03-09 17:15:49,236 - INFO - Batch 52951, Running Avg Loss: 4.66891
2025-03-09 17:16:06,680 - INFO - Batch 52976, Running Avg Loss: 4.66887
2025-03-09 17:16:24,292 - INFO - Batch 53001, Running Avg Loss: 4.66883
2025-03-09 17:16:24,307 - INFO - Batch 53000 finished
2025-03-09 17:16:24,308 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:16:41,871 - INFO - Batch 53026, Running Avg Loss: 4.66879
2025-03-09 17:16:59,399 - INFO - Batch 53051, Running Avg Loss: 4.66875
2025-03-09 17:17:16,869 - INFO - Batch 53076, Running Avg Loss: 4.66871
2025-03-09 17:17:34,299 - INFO - Batch 53101, Running Avg Loss: 4.66864
2025-03-09 17:17:34,316 - INFO - Batch 53100 finished
2025-03-09 17:17:34,316 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:17:51,834 - INFO - Batch 53126, Running Avg Loss: 4.66864
2025-03-09 17:18:09,538 - INFO - Batch 53151, Running Avg Loss: 4.66863
2025-03-09 17:18:26,985 - INFO - Batch 53176, Running Avg Loss: 4.66859
2025-03-09 17:18:44,479 - INFO - Batch 53201, Running Avg Loss: 4.66853
2025-03-09 17:18:44,499 - INFO - Batch 53200 finished
2025-03-09 17:18:44,500 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:19:01,980 - INFO - Batch 53226, Running Avg Loss: 4.66846
2025-03-09 17:19:19,595 - INFO - Batch 53251, Running Avg Loss: 4.66840
2025-03-09 17:19:37,169 - INFO - Batch 53276, Running Avg Loss: 4.66834
2025-03-09 17:19:54,651 - INFO - Batch 53301, Running Avg Loss: 4.66826
2025-03-09 17:19:54,668 - INFO - Batch 53300 finished
2025-03-09 17:19:54,669 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:20:12,118 - INFO - Batch 53326, Running Avg Loss: 4.66823
2025-03-09 17:20:29,593 - INFO - Batch 53351, Running Avg Loss: 4.66825
2025-03-09 17:20:46,829 - INFO - Batch 53376, Running Avg Loss: 4.66823
2025-03-09 17:21:04,047 - INFO - Batch 53401, Running Avg Loss: 4.66816
2025-03-09 17:21:04,062 - INFO - Batch 53400 finished
2025-03-09 17:21:04,062 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:21:21,706 - INFO - Batch 53426, Running Avg Loss: 4.66810
2025-03-09 17:21:39,298 - INFO - Batch 53451, Running Avg Loss: 4.66805
2025-03-09 17:21:56,903 - INFO - Batch 53476, Running Avg Loss: 4.66801
2025-03-09 17:22:14,477 - INFO - Batch 53501, Running Avg Loss: 4.66796
2025-03-09 17:22:14,493 - INFO - Batch 53500 finished
2025-03-09 17:22:14,493 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:22:31,992 - INFO - Batch 53526, Running Avg Loss: 4.66791
2025-03-09 17:22:49,539 - INFO - Batch 53551, Running Avg Loss: 4.66789
2025-03-09 17:23:07,406 - INFO - Batch 53576, Running Avg Loss: 4.66782
2025-03-09 17:23:25,011 - INFO - Batch 53601, Running Avg Loss: 4.66776
2025-03-09 17:23:25,032 - INFO - Batch 53600 finished
2025-03-09 17:23:25,033 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:23:42,569 - INFO - Batch 53626, Running Avg Loss: 4.66774
2025-03-09 17:24:00,156 - INFO - Batch 53651, Running Avg Loss: 4.66774
2025-03-09 17:24:17,754 - INFO - Batch 53676, Running Avg Loss: 4.66768
2025-03-09 17:24:35,455 - INFO - Batch 53701, Running Avg Loss: 4.66763
2025-03-09 17:24:35,472 - INFO - Batch 53700 finished
2025-03-09 17:24:35,472 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:24:52,907 - INFO - Batch 53726, Running Avg Loss: 4.66759
2025-03-09 17:25:10,400 - INFO - Batch 53751, Running Avg Loss: 4.66748
2025-03-09 17:25:27,932 - INFO - Batch 53776, Running Avg Loss: 4.66747
2025-03-09 17:25:45,411 - INFO - Batch 53801, Running Avg Loss: 4.66742
2025-03-09 17:25:45,430 - INFO - Batch 53800 finished
2025-03-09 17:25:45,430 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:26:02,905 - INFO - Batch 53826, Running Avg Loss: 4.66738
2025-03-09 17:26:20,418 - INFO - Batch 53851, Running Avg Loss: 4.66735
2025-03-09 17:26:38,020 - INFO - Batch 53876, Running Avg Loss: 4.66730
2025-03-09 17:26:55,630 - INFO - Batch 53901, Running Avg Loss: 4.66728
2025-03-09 17:26:55,649 - INFO - Batch 53900 finished
2025-03-09 17:26:55,650 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:27:13,175 - INFO - Batch 53926, Running Avg Loss: 4.66720
2025-03-09 17:27:30,613 - INFO - Batch 53951, Running Avg Loss: 4.66718
2025-03-09 17:27:48,323 - INFO - Batch 53976, Running Avg Loss: 4.66711
2025-03-09 17:28:05,825 - INFO - Batch 54001, Running Avg Loss: 4.66705
2025-03-09 17:28:05,843 - INFO - Batch 54000 finished
2025-03-09 17:28:05,844 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:28:23,473 - INFO - Batch 54026, Running Avg Loss: 4.66700
2025-03-09 17:28:41,020 - INFO - Batch 54051, Running Avg Loss: 4.66691
2025-03-09 17:28:58,516 - INFO - Batch 54076, Running Avg Loss: 4.66687
2025-03-09 17:29:16,037 - INFO - Batch 54101, Running Avg Loss: 4.66680
2025-03-09 17:29:16,055 - INFO - Batch 54100 finished
2025-03-09 17:29:16,056 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:29:33,668 - INFO - Batch 54126, Running Avg Loss: 4.66679
2025-03-09 17:29:51,246 - INFO - Batch 54151, Running Avg Loss: 4.66676
2025-03-09 17:30:08,802 - INFO - Batch 54176, Running Avg Loss: 4.66668
2025-03-09 17:30:26,297 - INFO - Batch 54201, Running Avg Loss: 4.66662
2025-03-09 17:30:26,314 - INFO - Batch 54200 finished
2025-03-09 17:30:26,314 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:30:43,770 - INFO - Batch 54226, Running Avg Loss: 4.66657
2025-03-09 17:31:01,014 - INFO - Batch 54251, Running Avg Loss: 4.66649
2025-03-09 17:31:18,713 - INFO - Batch 54276, Running Avg Loss: 4.66647
2025-03-09 17:31:36,252 - INFO - Batch 54301, Running Avg Loss: 4.66644
2025-03-09 17:31:36,270 - INFO - Batch 54300 finished
2025-03-09 17:31:36,270 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:31:53,695 - INFO - Batch 54326, Running Avg Loss: 4.66640
2025-03-09 17:32:11,167 - INFO - Batch 54351, Running Avg Loss: 4.66634
2025-03-09 17:32:28,765 - INFO - Batch 54376, Running Avg Loss: 4.66631
2025-03-09 17:32:46,427 - INFO - Batch 54401, Running Avg Loss: 4.66626
2025-03-09 17:32:46,444 - INFO - Batch 54400 finished
2025-03-09 17:32:46,445 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:33:03,901 - INFO - Batch 54426, Running Avg Loss: 4.66622
2025-03-09 17:33:21,458 - INFO - Batch 54451, Running Avg Loss: 4.66617
2025-03-09 17:33:39,057 - INFO - Batch 54476, Running Avg Loss: 4.66609
2025-03-09 17:33:56,651 - INFO - Batch 54501, Running Avg Loss: 4.66603
2025-03-09 17:33:56,667 - INFO - Batch 54500 finished
2025-03-09 17:33:56,667 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:34:14,205 - INFO - Batch 54526, Running Avg Loss: 4.66600
2025-03-09 17:34:31,881 - INFO - Batch 54551, Running Avg Loss: 4.66598
2025-03-09 17:34:49,432 - INFO - Batch 54576, Running Avg Loss: 4.66592
2025-03-09 17:35:06,946 - INFO - Batch 54601, Running Avg Loss: 4.66589
2025-03-09 17:35:06,964 - INFO - Batch 54600 finished
2025-03-09 17:35:06,964 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:35:24,529 - INFO - Batch 54626, Running Avg Loss: 4.66584
2025-03-09 17:35:42,141 - INFO - Batch 54651, Running Avg Loss: 4.66579
2025-03-09 17:35:59,690 - INFO - Batch 54676, Running Avg Loss: 4.66576
2025-03-09 17:36:17,379 - INFO - Batch 54701, Running Avg Loss: 4.66572
2025-03-09 17:36:17,397 - INFO - Batch 54700 finished
2025-03-09 17:36:17,397 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:36:34,903 - INFO - Batch 54726, Running Avg Loss: 4.66569
2025-03-09 17:36:52,434 - INFO - Batch 54751, Running Avg Loss: 4.66565
2025-03-09 17:37:09,990 - INFO - Batch 54776, Running Avg Loss: 4.66561
2025-03-09 17:37:27,664 - INFO - Batch 54801, Running Avg Loss: 4.66553
2025-03-09 17:37:27,682 - INFO - Batch 54800 finished
2025-03-09 17:37:27,683 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:37:45,197 - INFO - Batch 54826, Running Avg Loss: 4.66549
2025-03-09 17:38:02,721 - INFO - Batch 54851, Running Avg Loss: 4.66542
2025-03-09 17:38:20,206 - INFO - Batch 54876, Running Avg Loss: 4.66537
2025-03-09 17:38:37,679 - INFO - Batch 54901, Running Avg Loss: 4.66533
2025-03-09 17:38:37,697 - INFO - Batch 54900 finished
2025-03-09 17:38:37,698 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:38:55,238 - INFO - Batch 54926, Running Avg Loss: 4.66529
2025-03-09 17:39:12,691 - INFO - Batch 54951, Running Avg Loss: 4.66525
2025-03-09 17:39:30,105 - INFO - Batch 54976, Running Avg Loss: 4.66519
2025-03-09 17:39:47,576 - INFO - Batch 55001, Running Avg Loss: 4.66520
2025-03-09 17:39:47,597 - INFO - 
GPU Memory Stats at step 55000:
2025-03-09 17:39:47,598 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 17:39:47,598 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 17:39:47,598 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 17:39:47,598 - INFO - learning rate: 0.00000001
2025-03-09 17:39:47,598 - INFO - Ep 1 (Step 055000): Avg loss 4.665 | 225284096 tokens seen
2025-03-09 17:39:47,598 - INFO - optimizer lr: 0.00000001
2025-03-09 17:39:47,598 - INFO - scheduler lr: 0.00000001
2025-03-09 17:39:47,599 - INFO - Selected prompt: Meet Chris, a superhero of supplies! Just like how Batman protects Gotham City
2025-03-09 17:39:47,599 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:39:47,600 - INFO - random_topk: 4
2025-03-09 17:39:47,600 - INFO - random_temperature: 0.8599791802905788
2025-03-09 17:39:47,600 - INFO - global step 55000 , batch_idx 55000 => generating text
2025-03-09 17:39:47,600 - INFO - Generating on device cuda
2025-03-09 17:40:21,687 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:40:21,689 - INFO - Meet Chris, a superhero of supplies! Just like how Batman protects Gotham City, he had a big piece of fun and innovative ideas.

As they walked along, "What do you mean by 'The Gira" and 'B'?" The group had to do with them and decided to learn more about them.

One day, two friends named Sarah, decided to visit the street and asked her mom. "What is the 'S'?" Her mother smiled and replied, "Well, you can't have to find it too. But what if we can do that?"

"What does 'S' mean?" asked her mom.

"Well, let me tell you, "Why do we need to do with our best friends?" Her mother smiled and replied, "Well, it's important, but I can't know what it means."

They continued their journey, and they were excited and said, "That means we're doing things we need."

"Wow, I see, dear ones, and I'm feeling anxious. But remember, every time you're right, we need a lot of information and how to make things better. That's where our favorite video game come in.

"Well, I think we can't get the same thing, right? But I think I
2025-03-09 17:40:21,689 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:40:49,753 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_55000_steps_avg_loss_4.66520_optimizer_lr_0.00000001.pth
2025-03-09 17:40:50,022 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 17:40:50,022 - INFO - Batch 55000 finished
2025-03-09 17:40:50,022 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:41:07,069 - INFO - Batch 55026, Running Avg Loss: 4.66516
2025-03-09 17:41:24,560 - INFO - Batch 55051, Running Avg Loss: 4.66511
2025-03-09 17:41:42,074 - INFO - Batch 55076, Running Avg Loss: 4.66505
2025-03-09 17:41:59,602 - INFO - Batch 55101, Running Avg Loss: 4.66499
2025-03-09 17:41:59,620 - INFO - Batch 55100 finished
2025-03-09 17:41:59,620 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:42:17,122 - INFO - Batch 55126, Running Avg Loss: 4.66492
2025-03-09 17:42:34,847 - INFO - Batch 55151, Running Avg Loss: 4.66490
2025-03-09 17:42:52,376 - INFO - Batch 55176, Running Avg Loss: 4.66486
2025-03-09 17:43:09,913 - INFO - Batch 55201, Running Avg Loss: 4.66480
2025-03-09 17:43:09,926 - INFO - Batch 55200 finished
2025-03-09 17:43:09,927 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:43:27,460 - INFO - Batch 55226, Running Avg Loss: 4.66478
2025-03-09 17:43:45,160 - INFO - Batch 55251, Running Avg Loss: 4.66471
2025-03-09 17:44:02,729 - INFO - Batch 55276, Running Avg Loss: 4.66462
2025-03-09 17:44:20,306 - INFO - Batch 55301, Running Avg Loss: 4.66458
2025-03-09 17:44:20,324 - INFO - Batch 55300 finished
2025-03-09 17:44:20,324 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:44:37,875 - INFO - Batch 55326, Running Avg Loss: 4.66451
2025-03-09 17:44:55,356 - INFO - Batch 55351, Running Avg Loss: 4.66443
2025-03-09 17:45:12,841 - INFO - Batch 55376, Running Avg Loss: 4.66442
2025-03-09 17:45:30,363 - INFO - Batch 55401, Running Avg Loss: 4.66435
2025-03-09 17:45:30,377 - INFO - Batch 55400 finished
2025-03-09 17:45:30,378 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:45:47,984 - INFO - Batch 55426, Running Avg Loss: 4.66430
2025-03-09 17:46:05,473 - INFO - Batch 55451, Running Avg Loss: 4.66424
2025-03-09 17:46:22,920 - INFO - Batch 55476, Running Avg Loss: 4.66420
2025-03-09 17:46:40,359 - INFO - Batch 55501, Running Avg Loss: 4.66418
2025-03-09 17:46:40,374 - INFO - Batch 55500 finished
2025-03-09 17:46:40,375 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:46:57,904 - INFO - Batch 55526, Running Avg Loss: 4.66416
2025-03-09 17:47:15,671 - INFO - Batch 55551, Running Avg Loss: 4.66412
2025-03-09 17:47:33,261 - INFO - Batch 55576, Running Avg Loss: 4.66407
2025-03-09 17:47:50,849 - INFO - Batch 55601, Running Avg Loss: 4.66401
2025-03-09 17:47:50,865 - INFO - Batch 55600 finished
2025-03-09 17:47:50,866 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:48:08,402 - INFO - Batch 55626, Running Avg Loss: 4.66396
2025-03-09 17:48:25,987 - INFO - Batch 55651, Running Avg Loss: 4.66389
2025-03-09 17:48:43,608 - INFO - Batch 55676, Running Avg Loss: 4.66385
2025-03-09 17:49:01,087 - INFO - Batch 55701, Running Avg Loss: 4.66382
2025-03-09 17:49:01,105 - INFO - Batch 55700 finished
2025-03-09 17:49:01,105 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:49:18,631 - INFO - Batch 55726, Running Avg Loss: 4.66376
2025-03-09 17:49:36,145 - INFO - Batch 55751, Running Avg Loss: 4.66371
2025-03-09 17:49:53,601 - INFO - Batch 55776, Running Avg Loss: 4.66367
2025-03-09 17:50:11,154 - INFO - Batch 55801, Running Avg Loss: 4.66363
2025-03-09 17:50:11,167 - INFO - Batch 55800 finished
2025-03-09 17:50:11,168 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:50:28,849 - INFO - Batch 55826, Running Avg Loss: 4.66359
2025-03-09 17:50:46,205 - INFO - Batch 55851, Running Avg Loss: 4.66353
2025-03-09 17:51:03,423 - INFO - Batch 55876, Running Avg Loss: 4.66350
2025-03-09 17:51:20,793 - INFO - Batch 55901, Running Avg Loss: 4.66344
2025-03-09 17:51:20,808 - INFO - Batch 55900 finished
2025-03-09 17:51:20,808 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:51:38,383 - INFO - Batch 55926, Running Avg Loss: 4.66340
2025-03-09 17:51:55,933 - INFO - Batch 55951, Running Avg Loss: 4.66338
2025-03-09 17:52:13,469 - INFO - Batch 55976, Running Avg Loss: 4.66333
2025-03-09 17:52:31,023 - INFO - Batch 56001, Running Avg Loss: 4.66333
2025-03-09 17:52:31,038 - INFO - Batch 56000 finished
2025-03-09 17:52:31,038 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:52:48,559 - INFO - Batch 56026, Running Avg Loss: 4.66328
2025-03-09 17:53:06,102 - INFO - Batch 56051, Running Avg Loss: 4.66320
2025-03-09 17:53:23,644 - INFO - Batch 56076, Running Avg Loss: 4.66313
2025-03-09 17:53:41,147 - INFO - Batch 56101, Running Avg Loss: 4.66310
2025-03-09 17:53:41,165 - INFO - Batch 56100 finished
2025-03-09 17:53:41,166 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:53:58,832 - INFO - Batch 56126, Running Avg Loss: 4.66305
2025-03-09 17:54:16,351 - INFO - Batch 56151, Running Avg Loss: 4.66301
2025-03-09 17:54:33,857 - INFO - Batch 56176, Running Avg Loss: 4.66295
2025-03-09 17:54:51,367 - INFO - Batch 56201, Running Avg Loss: 4.66291
2025-03-09 17:54:51,382 - INFO - Batch 56200 finished
2025-03-09 17:54:51,383 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:55:08,875 - INFO - Batch 56226, Running Avg Loss: 4.66287
2025-03-09 17:55:26,397 - INFO - Batch 56251, Running Avg Loss: 4.66277
2025-03-09 17:55:44,088 - INFO - Batch 56276, Running Avg Loss: 4.66277
2025-03-09 17:56:01,646 - INFO - Batch 56301, Running Avg Loss: 4.66272
2025-03-09 17:56:01,666 - INFO - Batch 56300 finished
2025-03-09 17:56:01,667 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:56:19,232 - INFO - Batch 56326, Running Avg Loss: 4.66267
2025-03-09 17:56:36,764 - INFO - Batch 56351, Running Avg Loss: 4.66261
2025-03-09 17:56:54,424 - INFO - Batch 56376, Running Avg Loss: 4.66258
2025-03-09 17:57:11,949 - INFO - Batch 56401, Running Avg Loss: 4.66251
2025-03-09 17:57:11,964 - INFO - Batch 56400 finished
2025-03-09 17:57:11,964 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:57:29,529 - INFO - Batch 56426, Running Avg Loss: 4.66246
2025-03-09 17:57:47,078 - INFO - Batch 56451, Running Avg Loss: 4.66239
2025-03-09 17:58:04,586 - INFO - Batch 56476, Running Avg Loss: 4.66235
2025-03-09 17:58:22,152 - INFO - Batch 56501, Running Avg Loss: 4.66225
2025-03-09 17:58:22,169 - INFO - Batch 56500 finished
2025-03-09 17:58:22,169 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:58:39,645 - INFO - Batch 56526, Running Avg Loss: 4.66222
2025-03-09 17:58:57,187 - INFO - Batch 56551, Running Avg Loss: 4.66217
2025-03-09 17:59:14,718 - INFO - Batch 56576, Running Avg Loss: 4.66213
2025-03-09 17:59:32,206 - INFO - Batch 56601, Running Avg Loss: 4.66210
2025-03-09 17:59:32,223 - INFO - Batch 56600 finished
2025-03-09 17:59:32,224 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 17:59:49,686 - INFO - Batch 56626, Running Avg Loss: 4.66203
2025-03-09 18:00:07,164 - INFO - Batch 56651, Running Avg Loss: 4.66200
2025-03-09 18:00:24,689 - INFO - Batch 56676, Running Avg Loss: 4.66197
2025-03-09 18:00:41,894 - INFO - Batch 56701, Running Avg Loss: 4.66194
2025-03-09 18:00:41,907 - INFO - Batch 56700 finished
2025-03-09 18:00:41,907 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:00:59,109 - INFO - Batch 56726, Running Avg Loss: 4.66191
2025-03-09 18:01:16,457 - INFO - Batch 56751, Running Avg Loss: 4.66183
2025-03-09 18:01:33,973 - INFO - Batch 56776, Running Avg Loss: 4.66180
2025-03-09 18:01:51,593 - INFO - Batch 56801, Running Avg Loss: 4.66174
2025-03-09 18:01:51,610 - INFO - Batch 56800 finished
2025-03-09 18:01:51,611 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:02:09,200 - INFO - Batch 56826, Running Avg Loss: 4.66173
2025-03-09 18:02:26,756 - INFO - Batch 56851, Running Avg Loss: 4.66167
2025-03-09 18:02:44,279 - INFO - Batch 56876, Running Avg Loss: 4.66163
2025-03-09 18:03:01,800 - INFO - Batch 56901, Running Avg Loss: 4.66157
2025-03-09 18:03:01,815 - INFO - Batch 56900 finished
2025-03-09 18:03:01,815 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:03:19,328 - INFO - Batch 56926, Running Avg Loss: 4.66150
2025-03-09 18:03:37,063 - INFO - Batch 56951, Running Avg Loss: 4.66147
2025-03-09 18:03:54,567 - INFO - Batch 56976, Running Avg Loss: 4.66144
2025-03-09 18:04:12,084 - INFO - Batch 57001, Running Avg Loss: 4.66142
2025-03-09 18:04:12,099 - INFO - Batch 57000 finished
2025-03-09 18:04:12,100 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:04:29,635 - INFO - Batch 57026, Running Avg Loss: 4.66136
2025-03-09 18:04:47,176 - INFO - Batch 57051, Running Avg Loss: 4.66131
2025-03-09 18:05:04,766 - INFO - Batch 57076, Running Avg Loss: 4.66128
2025-03-09 18:05:22,352 - INFO - Batch 57101, Running Avg Loss: 4.66125
2025-03-09 18:05:22,368 - INFO - Batch 57100 finished
2025-03-09 18:05:22,368 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:05:39,901 - INFO - Batch 57126, Running Avg Loss: 4.66119
2025-03-09 18:05:57,411 - INFO - Batch 57151, Running Avg Loss: 4.66118
2025-03-09 18:06:14,927 - INFO - Batch 57176, Running Avg Loss: 4.66109
2025-03-09 18:06:32,433 - INFO - Batch 57201, Running Avg Loss: 4.66105
2025-03-09 18:06:32,449 - INFO - Batch 57200 finished
2025-03-09 18:06:32,450 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:06:49,875 - INFO - Batch 57226, Running Avg Loss: 4.66100
2025-03-09 18:07:07,514 - INFO - Batch 57251, Running Avg Loss: 4.66093
2025-03-09 18:07:25,021 - INFO - Batch 57276, Running Avg Loss: 4.66088
2025-03-09 18:07:42,489 - INFO - Batch 57301, Running Avg Loss: 4.66081
2025-03-09 18:07:42,508 - INFO - Batch 57300 finished
2025-03-09 18:07:42,509 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:07:59,949 - INFO - Batch 57326, Running Avg Loss: 4.66074
2025-03-09 18:08:17,419 - INFO - Batch 57351, Running Avg Loss: 4.66072
2025-03-09 18:08:34,890 - INFO - Batch 57376, Running Avg Loss: 4.66067
2025-03-09 18:08:52,573 - INFO - Batch 57401, Running Avg Loss: 4.66065
2025-03-09 18:08:52,590 - INFO - Batch 57400 finished
2025-03-09 18:08:52,591 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:09:10,127 - INFO - Batch 57426, Running Avg Loss: 4.66063
2025-03-09 18:09:27,677 - INFO - Batch 57451, Running Avg Loss: 4.66058
2025-03-09 18:09:45,202 - INFO - Batch 57476, Running Avg Loss: 4.66048
2025-03-09 18:10:02,883 - INFO - Batch 57501, Running Avg Loss: 4.66042
2025-03-09 18:10:02,900 - INFO - 
GPU Memory Stats at step 57500:
2025-03-09 18:10:02,900 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 18:10:02,900 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 18:10:02,900 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 18:10:02,901 - INFO - learning rate: 0.00000000
2025-03-09 18:10:02,901 - INFO - Ep 1 (Step 057500): Avg loss 4.660 | 235524096 tokens seen
2025-03-09 18:10:02,901 - INFO - optimizer lr: 0.00000000
2025-03-09 18:10:02,901 - INFO - scheduler lr: 0.00000000
2025-03-09 18:10:02,901 - INFO - Selected prompt: Identity formation is a complex and multifaceted process that involves the development of
2025-03-09 18:10:02,901 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:10:02,901 - INFO - random_topk: 7
2025-03-09 18:10:02,901 - INFO - random_temperature: 0.8588850399716176
2025-03-09 18:10:02,901 - INFO - global step 57500 , batch_idx 57500 => generating text
2025-03-09 18:10:02,901 - INFO - Generating on device cuda
2025-03-09 18:10:36,294 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:10:36,295 - INFO - Identity formation is a complex and multifaceted process that involves the development of an individual to perform a certain number of rules to do not. This is where the concept of the law comes into play. The term "s" refers to the rules, rules, and goals that are used by the public and the legal property. In this section, we will delve into the concept of a system of legal systems and learn about the legal system and its implications for the legal system.

Firstly, let's define what constitutes a "g" means. When a person is a group of people with a particular state, it can be a person of their own gender or group, and they are often overlooked or more likely to be a person. For instance, if you are an "a" or a child or family, it might be the person who is a new gender or gender. This is known as the "the National." A child might be the person or the "The People is the same" or even a child, and a particular event is a legal issue.

One key component of the study of the political and its relationship with the legal system. It's crucial to understand the complexities of a political system, as well as it is crucial for individuals to understand their identities and experiences in the United States. For instance, consider a study
2025-03-09 18:10:36,295 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:11:04,299 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_57500_steps_avg_loss_4.66042_optimizer_lr_0.00000000.pth
2025-03-09 18:11:04,564 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 18:11:04,564 - INFO - Batch 57500 finished
2025-03-09 18:11:04,564 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:11:22,002 - INFO - Batch 57526, Running Avg Loss: 4.66037
2025-03-09 18:11:39,635 - INFO - Batch 57551, Running Avg Loss: 4.66035
2025-03-09 18:11:57,370 - INFO - Batch 57576, Running Avg Loss: 4.66030
2025-03-09 18:12:14,966 - INFO - Batch 57601, Running Avg Loss: 4.66028
2025-03-09 18:12:14,986 - INFO - Batch 57600 finished
2025-03-09 18:12:14,987 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:12:32,514 - INFO - Batch 57626, Running Avg Loss: 4.66021
2025-03-09 18:12:50,033 - INFO - Batch 57651, Running Avg Loss: 4.66019
2025-03-09 18:13:07,563 - INFO - Batch 57676, Running Avg Loss: 4.66012
2025-03-09 18:13:25,169 - INFO - Batch 57701, Running Avg Loss: 4.66007
2025-03-09 18:13:25,185 - INFO - Batch 57700 finished
2025-03-09 18:13:25,185 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:13:42,747 - INFO - Batch 57726, Running Avg Loss: 4.66002
2025-03-09 18:14:00,270 - INFO - Batch 57751, Running Avg Loss: 4.66000
2025-03-09 18:14:17,814 - INFO - Batch 57776, Running Avg Loss: 4.65996
2025-03-09 18:14:35,315 - INFO - Batch 57801, Running Avg Loss: 4.65992
2025-03-09 18:14:35,331 - INFO - Batch 57800 finished
2025-03-09 18:14:35,332 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:14:52,805 - INFO - Batch 57826, Running Avg Loss: 4.65987
2025-03-09 18:15:10,423 - INFO - Batch 57851, Running Avg Loss: 4.65985
2025-03-09 18:15:28,034 - INFO - Batch 57876, Running Avg Loss: 4.65982
2025-03-09 18:15:45,711 - INFO - Batch 57901, Running Avg Loss: 4.65979
2025-03-09 18:15:45,729 - INFO - Batch 57900 finished
2025-03-09 18:15:45,729 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:16:03,377 - INFO - Batch 57926, Running Avg Loss: 4.65975
2025-03-09 18:16:20,944 - INFO - Batch 57951, Running Avg Loss: 4.65969
2025-03-09 18:16:38,535 - INFO - Batch 57976, Running Avg Loss: 4.65966
2025-03-09 18:16:56,239 - INFO - Batch 58001, Running Avg Loss: 4.65963
2025-03-09 18:16:56,256 - INFO - Batch 58000 finished
2025-03-09 18:16:56,256 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:17:13,796 - INFO - Batch 58026, Running Avg Loss: 4.65960
2025-03-09 18:17:31,376 - INFO - Batch 58051, Running Avg Loss: 4.65956
2025-03-09 18:17:48,995 - INFO - Batch 58076, Running Avg Loss: 4.65951
2025-03-09 18:18:06,597 - INFO - Batch 58101, Running Avg Loss: 4.65948
2025-03-09 18:18:06,612 - INFO - Batch 58100 finished
2025-03-09 18:18:06,612 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:18:24,143 - INFO - Batch 58126, Running Avg Loss: 4.65946
2025-03-09 18:18:41,850 - INFO - Batch 58151, Running Avg Loss: 4.65938
2025-03-09 18:18:59,350 - INFO - Batch 58176, Running Avg Loss: 4.65935
2025-03-09 18:19:16,823 - INFO - Batch 58201, Running Avg Loss: 4.65931
2025-03-09 18:19:16,843 - INFO - Batch 58200 finished
2025-03-09 18:19:16,844 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:19:34,330 - INFO - Batch 58226, Running Avg Loss: 4.65924
2025-03-09 18:19:51,827 - INFO - Batch 58251, Running Avg Loss: 4.65919
2025-03-09 18:20:09,307 - INFO - Batch 58276, Running Avg Loss: 4.65915
2025-03-09 18:20:26,847 - INFO - Batch 58301, Running Avg Loss: 4.65912
2025-03-09 18:20:26,864 - INFO - Batch 58300 finished
2025-03-09 18:20:26,865 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:20:44,292 - INFO - Batch 58326, Running Avg Loss: 4.65905
2025-03-09 18:21:01,629 - INFO - Batch 58351, Running Avg Loss: 4.65900
2025-03-09 18:21:19,060 - INFO - Batch 58376, Running Avg Loss: 4.65897
2025-03-09 18:21:36,623 - INFO - Batch 58401, Running Avg Loss: 4.65896
2025-03-09 18:21:36,639 - INFO - Batch 58400 finished
2025-03-09 18:21:36,639 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:21:54,376 - INFO - Batch 58426, Running Avg Loss: 4.65891
2025-03-09 18:22:11,943 - INFO - Batch 58451, Running Avg Loss: 4.65889
2025-03-09 18:22:29,460 - INFO - Batch 58476, Running Avg Loss: 4.65886
2025-03-09 18:22:46,968 - INFO - Batch 58501, Running Avg Loss: 4.65884
2025-03-09 18:22:46,984 - INFO - Batch 58500 finished
2025-03-09 18:22:46,984 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:23:04,473 - INFO - Batch 58526, Running Avg Loss: 4.65882
2025-03-09 18:23:21,910 - INFO - Batch 58551, Running Avg Loss: 4.65874
2025-03-09 18:23:39,521 - INFO - Batch 58576, Running Avg Loss: 4.65871
2025-03-09 18:23:57,026 - INFO - Batch 58601, Running Avg Loss: 4.65865
2025-03-09 18:23:57,045 - INFO - Batch 58600 finished
2025-03-09 18:23:57,045 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:24:14,577 - INFO - Batch 58626, Running Avg Loss: 4.65860
2025-03-09 18:24:32,110 - INFO - Batch 58651, Running Avg Loss: 4.65857
2025-03-09 18:24:49,801 - INFO - Batch 58676, Running Avg Loss: 4.65853
2025-03-09 18:25:07,473 - INFO - Batch 58701, Running Avg Loss: 4.65852
2025-03-09 18:25:07,491 - INFO - Batch 58700 finished
2025-03-09 18:25:07,492 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:25:25,076 - INFO - Batch 58726, Running Avg Loss: 4.65847
2025-03-09 18:25:42,720 - INFO - Batch 58751, Running Avg Loss: 4.65843
2025-03-09 18:26:00,327 - INFO - Batch 58776, Running Avg Loss: 4.65840
2025-03-09 18:26:17,972 - INFO - Batch 58801, Running Avg Loss: 4.65836
2025-03-09 18:26:17,994 - INFO - Batch 58800 finished
2025-03-09 18:26:17,994 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:26:35,555 - INFO - Batch 58826, Running Avg Loss: 4.65831
2025-03-09 18:26:53,161 - INFO - Batch 58851, Running Avg Loss: 4.65828
2025-03-09 18:27:10,675 - INFO - Batch 58876, Running Avg Loss: 4.65823
2025-03-09 18:27:28,254 - INFO - Batch 58901, Running Avg Loss: 4.65819
2025-03-09 18:27:28,270 - INFO - Batch 58900 finished
2025-03-09 18:27:28,270 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:27:45,799 - INFO - Batch 58926, Running Avg Loss: 4.65814
2025-03-09 18:28:03,377 - INFO - Batch 58951, Running Avg Loss: 4.65810
2025-03-09 18:28:21,135 - INFO - Batch 58976, Running Avg Loss: 4.65807
2025-03-09 18:28:38,636 - INFO - Batch 59001, Running Avg Loss: 4.65804
2025-03-09 18:28:38,655 - INFO - Batch 59000 finished
2025-03-09 18:28:38,656 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:28:56,132 - INFO - Batch 59026, Running Avg Loss: 4.65801
2025-03-09 18:29:13,656 - INFO - Batch 59051, Running Avg Loss: 4.65801
2025-03-09 18:29:31,209 - INFO - Batch 59076, Running Avg Loss: 4.65803
2025-03-09 18:29:48,761 - INFO - Batch 59101, Running Avg Loss: 4.65801
2025-03-09 18:29:48,777 - INFO - Batch 59100 finished
2025-03-09 18:29:48,778 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:30:06,355 - INFO - Batch 59126, Running Avg Loss: 4.65798
2025-03-09 18:30:24,005 - INFO - Batch 59151, Running Avg Loss: 4.65793
2025-03-09 18:30:41,531 - INFO - Batch 59176, Running Avg Loss: 4.65789
2025-03-09 18:30:58,722 - INFO - Batch 59201, Running Avg Loss: 4.65782
2025-03-09 18:30:58,736 - INFO - Batch 59200 finished
2025-03-09 18:30:58,737 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:31:16,002 - INFO - Batch 59226, Running Avg Loss: 4.65777
2025-03-09 18:31:33,511 - INFO - Batch 59251, Running Avg Loss: 4.65770
2025-03-09 18:31:51,185 - INFO - Batch 59276, Running Avg Loss: 4.65766
2025-03-09 18:32:08,654 - INFO - Batch 59301, Running Avg Loss: 4.65760
2025-03-09 18:32:08,671 - INFO - Batch 59300 finished
2025-03-09 18:32:08,672 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:32:26,200 - INFO - Batch 59326, Running Avg Loss: 4.65754
2025-03-09 18:32:43,722 - INFO - Batch 59351, Running Avg Loss: 4.65751
2025-03-09 18:33:01,385 - INFO - Batch 59376, Running Avg Loss: 4.65751
2025-03-09 18:33:18,974 - INFO - Batch 59401, Running Avg Loss: 4.65751
2025-03-09 18:33:18,990 - INFO - Batch 59400 finished
2025-03-09 18:33:18,990 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:33:36,508 - INFO - Batch 59426, Running Avg Loss: 4.65745
2025-03-09 18:33:54,039 - INFO - Batch 59451, Running Avg Loss: 4.65742
2025-03-09 18:34:11,593 - INFO - Batch 59476, Running Avg Loss: 4.65736
2025-03-09 18:34:29,132 - INFO - Batch 59501, Running Avg Loss: 4.65738
2025-03-09 18:34:29,151 - INFO - Batch 59500 finished
2025-03-09 18:34:29,152 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:34:46,662 - INFO - Batch 59526, Running Avg Loss: 4.65733
2025-03-09 18:35:04,300 - INFO - Batch 59551, Running Avg Loss: 4.65728
2025-03-09 18:35:21,861 - INFO - Batch 59576, Running Avg Loss: 4.65725
2025-03-09 18:35:39,389 - INFO - Batch 59601, Running Avg Loss: 4.65723
2025-03-09 18:35:39,405 - INFO - Batch 59600 finished
2025-03-09 18:35:39,405 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:35:56,961 - INFO - Batch 59626, Running Avg Loss: 4.65722
2025-03-09 18:36:14,453 - INFO - Batch 59651, Running Avg Loss: 4.65716
2025-03-09 18:36:31,904 - INFO - Batch 59676, Running Avg Loss: 4.65712
2025-03-09 18:36:49,601 - INFO - Batch 59701, Running Avg Loss: 4.65710
2025-03-09 18:36:49,617 - INFO - Batch 59700 finished
2025-03-09 18:36:49,618 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:37:07,128 - INFO - Batch 59726, Running Avg Loss: 4.65709
2025-03-09 18:37:24,663 - INFO - Batch 59751, Running Avg Loss: 4.65708
2025-03-09 18:37:42,189 - INFO - Batch 59776, Running Avg Loss: 4.65703
2025-03-09 18:37:59,824 - INFO - Batch 59801, Running Avg Loss: 4.65699
2025-03-09 18:37:59,842 - INFO - Batch 59800 finished
2025-03-09 18:37:59,843 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:38:17,282 - INFO - Batch 59826, Running Avg Loss: 4.65700
2025-03-09 18:38:34,783 - INFO - Batch 59851, Running Avg Loss: 4.65696
2025-03-09 18:38:52,331 - INFO - Batch 59876, Running Avg Loss: 4.65692
2025-03-09 18:39:09,857 - INFO - Batch 59901, Running Avg Loss: 4.65689
2025-03-09 18:39:09,874 - INFO - Batch 59900 finished
2025-03-09 18:39:09,874 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:39:27,370 - INFO - Batch 59926, Running Avg Loss: 4.65685
2025-03-09 18:39:44,900 - INFO - Batch 59951, Running Avg Loss: 4.65682
2025-03-09 18:40:02,408 - INFO - Batch 59976, Running Avg Loss: 4.65675
2025-03-09 18:40:19,853 - INFO - Batch 60001, Running Avg Loss: 4.65670
2025-03-09 18:40:19,870 - INFO - 
GPU Memory Stats at step 60000:
2025-03-09 18:40:19,871 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 18:40:19,871 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 18:40:19,871 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 18:40:19,871 - INFO - learning rate: 0.00000000
2025-03-09 18:40:19,871 - INFO - Ep 1 (Step 060000): Avg loss 4.657 | 245764096 tokens seen
2025-03-09 18:40:19,871 - INFO - optimizer lr: 0.00000000
2025-03-09 18:40:19,871 - INFO - scheduler lr: 0.00000000
2025-03-09 18:40:19,871 - INFO - Selected prompt: Once upon a time, there was a friendly agency called Gaudette Insurance Agency, Inc. They help
2025-03-09 18:40:19,871 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:40:19,872 - INFO - random_topk: 3
2025-03-09 18:40:19,872 - INFO - random_temperature: 0.870685831847134
2025-03-09 18:40:19,872 - INFO - global step 60000 , batch_idx 60000 => generating text
2025-03-09 18:40:19,872 - INFO - Generating on device cuda
2025-03-09 18:40:52,464 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:40:52,464 - INFO - Once upon a time, there was a friendly agency called Gaudette Insurance Agency, Inc. They help people who loved learning new things and learn new things. One sunny day, they decided to visit the park called "The Hink" and "The Binky," which was a special kind of friendly friends named Lily.

One day, they decided to learn about something new - a special kind of friends named Lily. He asked, "What are those people who are these things?" Her mom replied, "Well, I see, I can help you know what that means. We can also use the right tools to help people who need to learn about different cultures."

"Wow!" exclaimed Max, "Why do you think that means taking care of our environment?"

"Well!" exclaimed the wise old owl, "Well, I think that some people can use this to do with them and others. We can use this concept to make our friends, but we need to be strong and more enjoyable.

As we dive in, they discovered that some folks have different ways to manage their thoughts, feelings, and experiences. But sometimes, they might need to do something to be wrong. But they didn't need to know how to do that.

One day, while walking into the street, they discovered something called "tir" to
2025-03-09 18:40:52,464 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:41:16,896 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_60000_steps_avg_loss_4.65670_optimizer_lr_0.00000000.pth
2025-03-09 18:41:17,110 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 18:41:17,110 - INFO - Batch 60000 finished
2025-03-09 18:41:17,110 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:41:34,490 - INFO - Batch 60026, Running Avg Loss: 4.65668
2025-03-09 18:41:52,130 - INFO - Batch 60051, Running Avg Loss: 4.65665
2025-03-09 18:42:09,716 - INFO - Batch 60076, Running Avg Loss: 4.65663
2025-03-09 18:42:27,341 - INFO - Batch 60101, Running Avg Loss: 4.65656
2025-03-09 18:42:27,360 - INFO - Batch 60100 finished
2025-03-09 18:42:27,361 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:42:44,692 - INFO - Batch 60126, Running Avg Loss: 4.65651
2025-03-09 18:43:02,262 - INFO - Batch 60151, Running Avg Loss: 4.65649
2025-03-09 18:43:19,712 - INFO - Batch 60176, Running Avg Loss: 4.65644
2025-03-09 18:43:37,199 - INFO - Batch 60201, Running Avg Loss: 4.65639
2025-03-09 18:43:37,216 - INFO - Batch 60200 finished
2025-03-09 18:43:37,217 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:43:54,657 - INFO - Batch 60226, Running Avg Loss: 4.65638
2025-03-09 18:44:12,216 - INFO - Batch 60251, Running Avg Loss: 4.65634
2025-03-09 18:44:29,636 - INFO - Batch 60276, Running Avg Loss: 4.65628
2025-03-09 18:44:47,058 - INFO - Batch 60301, Running Avg Loss: 4.65627
2025-03-09 18:44:47,075 - INFO - Batch 60300 finished
2025-03-09 18:44:47,075 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:45:04,595 - INFO - Batch 60326, Running Avg Loss: 4.65622
2025-03-09 18:45:22,170 - INFO - Batch 60351, Running Avg Loss: 4.65619
2025-03-09 18:45:39,629 - INFO - Batch 60376, Running Avg Loss: 4.65614
2025-03-09 18:45:57,104 - INFO - Batch 60401, Running Avg Loss: 4.65609
2025-03-09 18:45:57,123 - INFO - Batch 60400 finished
2025-03-09 18:45:57,124 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:46:14,569 - INFO - Batch 60426, Running Avg Loss: 4.65606
2025-03-09 18:46:32,012 - INFO - Batch 60451, Running Avg Loss: 4.65601
2025-03-09 18:46:49,441 - INFO - Batch 60476, Running Avg Loss: 4.65599
2025-03-09 18:47:06,924 - INFO - Batch 60501, Running Avg Loss: 4.65599
2025-03-09 18:47:06,945 - INFO - Batch 60500 finished
2025-03-09 18:47:06,945 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:47:24,405 - INFO - Batch 60526, Running Avg Loss: 4.65594
2025-03-09 18:47:42,076 - INFO - Batch 60551, Running Avg Loss: 4.65592
2025-03-09 18:47:59,632 - INFO - Batch 60576, Running Avg Loss: 4.65589
2025-03-09 18:48:17,207 - INFO - Batch 60601, Running Avg Loss: 4.65586
2025-03-09 18:48:17,223 - INFO - Batch 60600 finished
2025-03-09 18:48:17,224 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:48:34,712 - INFO - Batch 60626, Running Avg Loss: 4.65585
2025-03-09 18:48:52,183 - INFO - Batch 60651, Running Avg Loss: 4.65583
2025-03-09 18:49:09,762 - INFO - Batch 60676, Running Avg Loss: 4.65580
2025-03-09 18:49:27,185 - INFO - Batch 60701, Running Avg Loss: 4.65575
2025-03-09 18:49:27,199 - INFO - Batch 60700 finished
2025-03-09 18:49:27,200 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:49:44,645 - INFO - Batch 60726, Running Avg Loss: 4.65571
2025-03-09 18:50:02,105 - INFO - Batch 60751, Running Avg Loss: 4.65571
2025-03-09 18:50:19,464 - INFO - Batch 60776, Running Avg Loss: 4.65568
2025-03-09 18:50:36,589 - INFO - Batch 60801, Running Avg Loss: 4.65565
2025-03-09 18:50:36,602 - INFO - Batch 60800 finished
2025-03-09 18:50:36,602 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:50:53,871 - INFO - Batch 60826, Running Avg Loss: 4.65559
2025-03-09 18:51:11,133 - INFO - Batch 60851, Running Avg Loss: 4.65556
2025-03-09 18:51:28,675 - INFO - Batch 60876, Running Avg Loss: 4.65549
2025-03-09 18:51:46,149 - INFO - Batch 60901, Running Avg Loss: 4.65542
2025-03-09 18:51:46,168 - INFO - Batch 60900 finished
2025-03-09 18:51:46,168 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:52:03,606 - INFO - Batch 60926, Running Avg Loss: 4.65538
2025-03-09 18:52:21,142 - INFO - Batch 60951, Running Avg Loss: 4.65537
2025-03-09 18:52:38,733 - INFO - Batch 60976, Running Avg Loss: 4.65533
2025-03-09 18:52:56,258 - INFO - Batch 61001, Running Avg Loss: 4.65530
2025-03-09 18:52:56,273 - INFO - Batch 61000 finished
2025-03-09 18:52:56,274 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:53:13,740 - INFO - Batch 61026, Running Avg Loss: 4.65525
2025-03-09 18:53:31,215 - INFO - Batch 61051, Running Avg Loss: 4.65523
2025-03-09 18:53:48,675 - INFO - Batch 61076, Running Avg Loss: 4.65520
2025-03-09 18:54:06,149 - INFO - Batch 61101, Running Avg Loss: 4.65516
2025-03-09 18:54:06,167 - INFO - Batch 61100 finished
2025-03-09 18:54:06,167 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:54:23,824 - INFO - Batch 61126, Running Avg Loss: 4.65516
2025-03-09 18:54:41,350 - INFO - Batch 61151, Running Avg Loss: 4.65506
2025-03-09 18:54:58,849 - INFO - Batch 61176, Running Avg Loss: 4.65503
2025-03-09 18:55:16,372 - INFO - Batch 61201, Running Avg Loss: 4.65505
2025-03-09 18:55:16,387 - INFO - Batch 61200 finished
2025-03-09 18:55:16,387 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:55:33,838 - INFO - Batch 61226, Running Avg Loss: 4.65502
2025-03-09 18:55:51,366 - INFO - Batch 61251, Running Avg Loss: 4.65496
2025-03-09 18:56:09,040 - INFO - Batch 61276, Running Avg Loss: 4.65490
2025-03-09 18:56:26,513 - INFO - Batch 61301, Running Avg Loss: 4.65489
2025-03-09 18:56:26,531 - INFO - Batch 61300 finished
2025-03-09 18:56:26,531 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:56:43,983 - INFO - Batch 61326, Running Avg Loss: 4.65487
2025-03-09 18:57:01,479 - INFO - Batch 61351, Running Avg Loss: 4.65485
2025-03-09 18:57:18,968 - INFO - Batch 61376, Running Avg Loss: 4.65481
2025-03-09 18:57:36,146 - INFO - Batch 61401, Running Avg Loss: 4.65476
2025-03-09 18:57:36,159 - INFO - Batch 61400 finished
2025-03-09 18:57:36,159 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:57:53,290 - INFO - Batch 61426, Running Avg Loss: 4.65472
2025-03-09 18:58:10,497 - INFO - Batch 61451, Running Avg Loss: 4.65464
2025-03-09 18:58:27,914 - INFO - Batch 61476, Running Avg Loss: 4.65459
2025-03-09 18:58:45,317 - INFO - Batch 61501, Running Avg Loss: 4.65457
2025-03-09 18:58:45,332 - INFO - Batch 61500 finished
2025-03-09 18:58:45,332 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 18:59:02,676 - INFO - Batch 61526, Running Avg Loss: 4.65455
2025-03-09 18:59:20,060 - INFO - Batch 61551, Running Avg Loss: 4.65450
2025-03-09 18:59:37,524 - INFO - Batch 61576, Running Avg Loss: 4.65449
2025-03-09 18:59:54,989 - INFO - Batch 61601, Running Avg Loss: 4.65446
2025-03-09 18:59:55,004 - INFO - Batch 61600 finished
2025-03-09 18:59:55,004 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:00:12,462 - INFO - Batch 61626, Running Avg Loss: 4.65444
2025-03-09 19:00:29,938 - INFO - Batch 61651, Running Avg Loss: 4.65439
2025-03-09 19:00:47,511 - INFO - Batch 61676, Running Avg Loss: 4.65431
2025-03-09 19:01:04,865 - INFO - Batch 61701, Running Avg Loss: 4.65427
2025-03-09 19:01:04,878 - INFO - Batch 61700 finished
2025-03-09 19:01:04,878 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:01:22,082 - INFO - Batch 61726, Running Avg Loss: 4.65424
2025-03-09 19:01:39,384 - INFO - Batch 61751, Running Avg Loss: 4.65421
2025-03-09 19:01:56,851 - INFO - Batch 61776, Running Avg Loss: 4.65414
2025-03-09 19:02:14,440 - INFO - Batch 61801, Running Avg Loss: 4.65413
2025-03-09 19:02:14,455 - INFO - Batch 61800 finished
2025-03-09 19:02:14,456 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:02:31,899 - INFO - Batch 61826, Running Avg Loss: 4.65408
2025-03-09 19:02:49,340 - INFO - Batch 61851, Running Avg Loss: 4.65404
2025-03-09 19:03:06,806 - INFO - Batch 61876, Running Avg Loss: 4.65397
2025-03-09 19:03:24,323 - INFO - Batch 61901, Running Avg Loss: 4.65397
2025-03-09 19:03:24,340 - INFO - Batch 61900 finished
2025-03-09 19:03:24,340 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:03:41,825 - INFO - Batch 61926, Running Avg Loss: 4.65391
2025-03-09 19:03:59,465 - INFO - Batch 61951, Running Avg Loss: 4.65387
2025-03-09 19:04:16,952 - INFO - Batch 61976, Running Avg Loss: 4.65384
2025-03-09 19:04:34,439 - INFO - Batch 62001, Running Avg Loss: 4.65380
2025-03-09 19:04:34,453 - INFO - Batch 62000 finished
2025-03-09 19:04:34,454 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:04:51,954 - INFO - Batch 62026, Running Avg Loss: 4.65378
2025-03-09 19:05:09,543 - INFO - Batch 62051, Running Avg Loss: 4.65373
2025-03-09 19:05:27,090 - INFO - Batch 62076, Running Avg Loss: 4.65369
2025-03-09 19:05:44,659 - INFO - Batch 62101, Running Avg Loss: 4.65364
2025-03-09 19:05:44,676 - INFO - Batch 62100 finished
2025-03-09 19:05:44,676 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:06:02,204 - INFO - Batch 62126, Running Avg Loss: 4.65361
2025-03-09 19:06:19,705 - INFO - Batch 62151, Running Avg Loss: 4.65357
2025-03-09 19:06:37,242 - INFO - Batch 62176, Running Avg Loss: 4.65354
2025-03-09 19:06:54,789 - INFO - Batch 62201, Running Avg Loss: 4.65350
2025-03-09 19:06:54,806 - INFO - Batch 62200 finished
2025-03-09 19:06:54,807 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:07:12,339 - INFO - Batch 62226, Running Avg Loss: 4.65346
2025-03-09 19:07:30,045 - INFO - Batch 62251, Running Avg Loss: 4.65344
2025-03-09 19:07:47,605 - INFO - Batch 62276, Running Avg Loss: 4.65342
2025-03-09 19:08:05,149 - INFO - Batch 62301, Running Avg Loss: 4.65337
2025-03-09 19:08:05,167 - INFO - Batch 62300 finished
2025-03-09 19:08:05,167 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:08:22,647 - INFO - Batch 62326, Running Avg Loss: 4.65331
2025-03-09 19:08:40,180 - INFO - Batch 62351, Running Avg Loss: 4.65328
2025-03-09 19:08:57,761 - INFO - Batch 62376, Running Avg Loss: 4.65325
2025-03-09 19:09:15,571 - INFO - Batch 62401, Running Avg Loss: 4.65324
2025-03-09 19:09:15,587 - INFO - Batch 62400 finished
2025-03-09 19:09:15,587 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:09:33,189 - INFO - Batch 62426, Running Avg Loss: 4.65320
2025-03-09 19:09:50,778 - INFO - Batch 62451, Running Avg Loss: 4.65319
2025-03-09 19:10:08,410 - INFO - Batch 62476, Running Avg Loss: 4.65312
2025-03-09 19:10:26,203 - INFO - Batch 62501, Running Avg Loss: 4.65308
2025-03-09 19:10:26,221 - INFO - 
GPU Memory Stats at step 62500:
2025-03-09 19:10:26,222 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 19:10:26,222 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 19:10:26,222 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 19:10:26,222 - INFO - learning rate: 0.00000000
2025-03-09 19:10:26,222 - INFO - Ep 1 (Step 062500): Avg loss 4.653 | 256004096 tokens seen
2025-03-09 19:10:26,222 - INFO - optimizer lr: 0.00000000
2025-03-09 19:10:26,222 - INFO - scheduler lr: 0.00000000
2025-03-09 19:10:26,223 - INFO - Selected prompt: Imagine if someone got their hands on dangerous weapons
2025-03-09 19:10:26,223 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:10:26,223 - INFO - random_topk: 7
2025-03-09 19:10:26,223 - INFO - random_temperature: 0.7638620941669165
2025-03-09 19:10:26,223 - INFO - global step 62500 , batch_idx 62500 => generating text
2025-03-09 19:10:26,223 - INFO - Generating on device cuda
2025-03-09 19:11:00,314 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:11:00,315 - INFO - Imagine if someone got their hands on dangerous weapons, or perhaps you have a special place where everything is wrong. You might think, "What does 'to do? It's important to know that our feelings are being able to understand and manage our thoughts and feelings through something called "in-g-white-the-commerce-E-E-E-101, we mean that we can use it to help us understand our thoughts, feelings, and feelings.

Let's start with understanding what we mean when we talk about "the-year-old." At first, let's talk about what we mean by "I" or "A-S"-based. This means that when a person's gender identity is a person, it might seem like something you love yourself. But instead of feeling scared, they are still being a person, or simply "I can't have a good friend," and there's a way to make a difference in a better world.

One important aspect of our thoughts is through the idea of thinking, and it can help you understand what it means and how it affects us. By learning about these situations, we can create a sense of self-expression and respect for each other. And guess what?

Now, let's talk about two important questions
2025-03-09 19:11:00,315 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:11:25,011 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_62500_steps_avg_loss_4.65308_optimizer_lr_0.00000000.pth
2025-03-09 19:11:25,199 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 19:11:25,199 - INFO - Batch 62500 finished
2025-03-09 19:11:25,199 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:11:42,607 - INFO - Batch 62526, Running Avg Loss: 4.65304
2025-03-09 19:12:00,152 - INFO - Batch 62551, Running Avg Loss: 4.65303
2025-03-09 19:12:18,007 - INFO - Batch 62576, Running Avg Loss: 4.65301
2025-03-09 19:12:35,637 - INFO - Batch 62601, Running Avg Loss: 4.65298
2025-03-09 19:12:35,655 - INFO - Batch 62600 finished
2025-03-09 19:12:35,655 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:12:53,258 - INFO - Batch 62626, Running Avg Loss: 4.65294
2025-03-09 19:13:10,668 - INFO - Batch 62651, Running Avg Loss: 4.65291
2025-03-09 19:13:28,097 - INFO - Batch 62676, Running Avg Loss: 4.65288
2025-03-09 19:13:45,595 - INFO - Batch 62701, Running Avg Loss: 4.65282
2025-03-09 19:13:45,610 - INFO - Batch 62700 finished
2025-03-09 19:13:45,611 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:14:03,055 - INFO - Batch 62726, Running Avg Loss: 4.65282
2025-03-09 19:14:20,565 - INFO - Batch 62751, Running Avg Loss: 4.65277
2025-03-09 19:14:38,135 - INFO - Batch 62776, Running Avg Loss: 4.65273
2025-03-09 19:14:55,675 - INFO - Batch 62801, Running Avg Loss: 4.65266
2025-03-09 19:14:55,693 - INFO - Batch 62800 finished
2025-03-09 19:14:55,693 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:15:13,193 - INFO - Batch 62826, Running Avg Loss: 4.65260
2025-03-09 19:15:30,871 - INFO - Batch 62851, Running Avg Loss: 4.65256
2025-03-09 19:15:48,371 - INFO - Batch 62876, Running Avg Loss: 4.65253
2025-03-09 19:16:05,932 - INFO - Batch 62901, Running Avg Loss: 4.65250
2025-03-09 19:16:05,950 - INFO - Batch 62900 finished
2025-03-09 19:16:05,950 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:16:23,496 - INFO - Batch 62926, Running Avg Loss: 4.65248
2025-03-09 19:16:41,038 - INFO - Batch 62951, Running Avg Loss: 4.65243
2025-03-09 19:16:58,575 - INFO - Batch 62976, Running Avg Loss: 4.65236
2025-03-09 19:17:16,238 - INFO - Batch 63001, Running Avg Loss: 4.65234
2025-03-09 19:17:16,255 - INFO - Batch 63000 finished
2025-03-09 19:17:16,255 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:17:33,786 - INFO - Batch 63026, Running Avg Loss: 4.65231
2025-03-09 19:17:51,325 - INFO - Batch 63051, Running Avg Loss: 4.65229
2025-03-09 19:18:08,897 - INFO - Batch 63076, Running Avg Loss: 4.65229
2025-03-09 19:18:26,418 - INFO - Batch 63101, Running Avg Loss: 4.65224
2025-03-09 19:18:26,435 - INFO - Batch 63100 finished
2025-03-09 19:18:26,436 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:18:43,954 - INFO - Batch 63126, Running Avg Loss: 4.65220
2025-03-09 19:19:01,618 - INFO - Batch 63151, Running Avg Loss: 4.65214
2025-03-09 19:19:19,086 - INFO - Batch 63176, Running Avg Loss: 4.65208
2025-03-09 19:19:36,565 - INFO - Batch 63201, Running Avg Loss: 4.65202
2025-03-09 19:19:36,581 - INFO - Batch 63200 finished
2025-03-09 19:19:36,582 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:19:54,042 - INFO - Batch 63226, Running Avg Loss: 4.65201
2025-03-09 19:20:11,512 - INFO - Batch 63251, Running Avg Loss: 4.65198
2025-03-09 19:20:29,001 - INFO - Batch 63276, Running Avg Loss: 4.65195
2025-03-09 19:20:46,454 - INFO - Batch 63301, Running Avg Loss: 4.65189
2025-03-09 19:20:46,472 - INFO - Batch 63300 finished
2025-03-09 19:20:46,472 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:21:03,711 - INFO - Batch 63326, Running Avg Loss: 4.65188
2025-03-09 19:21:21,061 - INFO - Batch 63351, Running Avg Loss: 4.65182
2025-03-09 19:21:38,505 - INFO - Batch 63376, Running Avg Loss: 4.65179
2025-03-09 19:21:55,962 - INFO - Batch 63401, Running Avg Loss: 4.65173
2025-03-09 19:21:55,980 - INFO - Batch 63400 finished
2025-03-09 19:21:55,980 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:22:13,579 - INFO - Batch 63426, Running Avg Loss: 4.65168
2025-03-09 19:22:31,093 - INFO - Batch 63451, Running Avg Loss: 4.65167
2025-03-09 19:22:48,631 - INFO - Batch 63476, Running Avg Loss: 4.65164
2025-03-09 19:23:06,187 - INFO - Batch 63501, Running Avg Loss: 4.65160
2025-03-09 19:23:06,203 - INFO - Batch 63500 finished
2025-03-09 19:23:06,203 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:23:23,747 - INFO - Batch 63526, Running Avg Loss: 4.65156
2025-03-09 19:23:41,238 - INFO - Batch 63551, Running Avg Loss: 4.65154
2025-03-09 19:23:58,906 - INFO - Batch 63576, Running Avg Loss: 4.65151
2025-03-09 19:24:16,396 - INFO - Batch 63601, Running Avg Loss: 4.65145
2025-03-09 19:24:16,414 - INFO - Batch 63600 finished
2025-03-09 19:24:16,415 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:24:33,957 - INFO - Batch 63626, Running Avg Loss: 4.65141
2025-03-09 19:24:51,559 - INFO - Batch 63651, Running Avg Loss: 4.65137
2025-03-09 19:25:09,208 - INFO - Batch 63676, Running Avg Loss: 4.65132
2025-03-09 19:25:27,014 - INFO - Batch 63701, Running Avg Loss: 4.65127
2025-03-09 19:25:27,031 - INFO - Batch 63700 finished
2025-03-09 19:25:27,031 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:25:44,605 - INFO - Batch 63726, Running Avg Loss: 4.65122
2025-03-09 19:26:02,254 - INFO - Batch 63751, Running Avg Loss: 4.65120
2025-03-09 19:26:19,879 - INFO - Batch 63776, Running Avg Loss: 4.65116
2025-03-09 19:26:37,483 - INFO - Batch 63801, Running Avg Loss: 4.65115
2025-03-09 19:26:37,502 - INFO - Batch 63800 finished
2025-03-09 19:26:37,503 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:26:55,070 - INFO - Batch 63826, Running Avg Loss: 4.65109
2025-03-09 19:27:12,622 - INFO - Batch 63851, Running Avg Loss: 4.65106
2025-03-09 19:27:30,185 - INFO - Batch 63876, Running Avg Loss: 4.65105
2025-03-09 19:27:47,743 - INFO - Batch 63901, Running Avg Loss: 4.65105
2025-03-09 19:27:47,759 - INFO - Batch 63900 finished
2025-03-09 19:27:47,760 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:28:05,273 - INFO - Batch 63926, Running Avg Loss: 4.65103
2025-03-09 19:28:22,813 - INFO - Batch 63951, Running Avg Loss: 4.65100
2025-03-09 19:28:40,488 - INFO - Batch 63976, Running Avg Loss: 4.65098
2025-03-09 19:28:58,030 - INFO - Batch 64001, Running Avg Loss: 4.65095
2025-03-09 19:28:58,046 - INFO - Batch 64000 finished
2025-03-09 19:28:58,047 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:29:15,577 - INFO - Batch 64026, Running Avg Loss: 4.65093
2025-03-09 19:29:33,123 - INFO - Batch 64051, Running Avg Loss: 4.65092
2025-03-09 19:29:50,698 - INFO - Batch 64076, Running Avg Loss: 4.65088
2025-03-09 19:30:08,238 - INFO - Batch 64101, Running Avg Loss: 4.65087
2025-03-09 19:30:08,253 - INFO - Batch 64100 finished
2025-03-09 19:30:08,253 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:30:25,904 - INFO - Batch 64126, Running Avg Loss: 4.65084
2025-03-09 19:30:43,283 - INFO - Batch 64151, Running Avg Loss: 4.65082
2025-03-09 19:31:00,485 - INFO - Batch 64176, Running Avg Loss: 4.65085
2025-03-09 19:31:17,839 - INFO - Batch 64201, Running Avg Loss: 4.65077
2025-03-09 19:31:17,854 - INFO - Batch 64200 finished
2025-03-09 19:31:17,854 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:31:35,373 - INFO - Batch 64226, Running Avg Loss: 4.65072
2025-03-09 19:31:52,933 - INFO - Batch 64251, Running Avg Loss: 4.65072
2025-03-09 19:32:10,662 - INFO - Batch 64276, Running Avg Loss: 4.65067
2025-03-09 19:32:28,221 - INFO - Batch 64301, Running Avg Loss: 4.65063
2025-03-09 19:32:28,235 - INFO - Batch 64300 finished
2025-03-09 19:32:28,236 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:32:45,708 - INFO - Batch 64326, Running Avg Loss: 4.65058
2025-03-09 19:33:03,300 - INFO - Batch 64351, Running Avg Loss: 4.65054
2025-03-09 19:33:20,817 - INFO - Batch 64376, Running Avg Loss: 4.65050
2025-03-09 19:33:38,355 - INFO - Batch 64401, Running Avg Loss: 4.65046
2025-03-09 19:33:38,374 - INFO - Batch 64400 finished
2025-03-09 19:33:38,374 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:33:55,943 - INFO - Batch 64426, Running Avg Loss: 4.65042
2025-03-09 19:34:13,555 - INFO - Batch 64451, Running Avg Loss: 4.65039
2025-03-09 19:34:31,151 - INFO - Batch 64476, Running Avg Loss: 4.65035
2025-03-09 19:34:48,686 - INFO - Batch 64501, Running Avg Loss: 4.65031
2025-03-09 19:34:48,702 - INFO - Batch 64500 finished
2025-03-09 19:34:48,702 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:35:06,251 - INFO - Batch 64526, Running Avg Loss: 4.65025
2025-03-09 19:35:23,955 - INFO - Batch 64551, Running Avg Loss: 4.65022
2025-03-09 19:35:41,521 - INFO - Batch 64576, Running Avg Loss: 4.65019
2025-03-09 19:35:59,128 - INFO - Batch 64601, Running Avg Loss: 4.65018
2025-03-09 19:35:59,145 - INFO - Batch 64600 finished
2025-03-09 19:35:59,145 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:36:16,735 - INFO - Batch 64626, Running Avg Loss: 4.65014
2025-03-09 19:36:34,319 - INFO - Batch 64651, Running Avg Loss: 4.65010
2025-03-09 19:36:51,889 - INFO - Batch 64676, Running Avg Loss: 4.65010
2025-03-09 19:37:09,603 - INFO - Batch 64701, Running Avg Loss: 4.65003
2025-03-09 19:37:09,617 - INFO - Batch 64700 finished
2025-03-09 19:37:09,617 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:37:27,146 - INFO - Batch 64726, Running Avg Loss: 4.64999
2025-03-09 19:37:44,681 - INFO - Batch 64751, Running Avg Loss: 4.64997
2025-03-09 19:38:02,212 - INFO - Batch 64776, Running Avg Loss: 4.64991
2025-03-09 19:38:19,935 - INFO - Batch 64801, Running Avg Loss: 4.64987
2025-03-09 19:38:19,950 - INFO - Batch 64800 finished
2025-03-09 19:38:19,951 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:38:37,579 - INFO - Batch 64826, Running Avg Loss: 4.64984
2025-03-09 19:38:55,194 - INFO - Batch 64851, Running Avg Loss: 4.64982
2025-03-09 19:39:12,784 - INFO - Batch 64876, Running Avg Loss: 4.64979
2025-03-09 19:39:30,385 - INFO - Batch 64901, Running Avg Loss: 4.64979
2025-03-09 19:39:30,401 - INFO - Batch 64900 finished
2025-03-09 19:39:30,402 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:39:47,867 - INFO - Batch 64926, Running Avg Loss: 4.64976
2025-03-09 19:40:05,366 - INFO - Batch 64951, Running Avg Loss: 4.64971
2025-03-09 19:40:22,883 - INFO - Batch 64976, Running Avg Loss: 4.64969
2025-03-09 19:40:40,137 - INFO - Batch 65001, Running Avg Loss: 4.64967
2025-03-09 19:40:40,149 - INFO - 
GPU Memory Stats at step 65000:
2025-03-09 19:40:40,150 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 19:40:40,150 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 19:40:40,150 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 19:40:40,150 - INFO - learning rate: 0.00000001
2025-03-09 19:40:40,150 - INFO - Ep 1 (Step 065000): Avg loss 4.650 | 266244096 tokens seen
2025-03-09 19:40:40,150 - INFO - optimizer lr: 0.00000001
2025-03-09 19:40:40,150 - INFO - scheduler lr: 0.00000001
2025-03-09 19:40:40,150 - INFO - Selected prompt: Introduction: The Art of Crafting Vegan Sandwich Delights Sandwiches occupy a unique space in
2025-03-09 19:40:40,150 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:40:40,152 - INFO - random_topk: 8
2025-03-09 19:40:40,152 - INFO - random_temperature: 0.8621763193425427
2025-03-09 19:40:40,152 - INFO - global step 65000 , batch_idx 65000 => generating text
2025-03-09 19:40:40,152 - INFO - Generating on device cuda
2025-03-09 19:41:12,591 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:41:12,591 - INFO - Introduction: The Art of Crafting Vegan Sandwich Delights Sandwiches occupy a unique space in the world and the early 19th century. It was a fascinating place in ancient civilizations, with a unique blend of artistic expression and cultural significance in the context of the Middle Ages. This chapter will delve into the rich history, artistic traditions, and how it shaped its cultural roots.

Section 1: What Are the Life of the Al-Aagas?

Before diving into the vast landscape of the late 1960s, it is essential first to examine what constitutes the ancient Chinese-white culture. At the heart of the Middle Eastern history, it is the most influential figure among the American African Church, where it evolved into a particular place of people living in the United States. By the 20th century, it became known as the "The B.S., the 19th century, the American Civil War in the 1980s. These stories often feature the historical context, cultural practices, and cultural traditions that shaped our cultural heritage while also connecting the world to the world.

Section 2: Key Elements of the History of the Middle East and its Impact on the 19th century

The roots of the British era was the first European Union, where the early 1
2025-03-09 19:41:12,591 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:41:41,076 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_65000_steps_avg_loss_4.64967_optimizer_lr_0.00000001.pth
2025-03-09 19:41:41,322 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 19:41:41,322 - INFO - Batch 65000 finished
2025-03-09 19:41:41,322 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:41:58,678 - INFO - Batch 65026, Running Avg Loss: 4.64962
2025-03-09 19:42:16,145 - INFO - Batch 65051, Running Avg Loss: 4.64958
2025-03-09 19:42:33,723 - INFO - Batch 65076, Running Avg Loss: 4.64953
2025-03-09 19:42:51,257 - INFO - Batch 65101, Running Avg Loss: 4.64950
2025-03-09 19:42:51,278 - INFO - Batch 65100 finished
2025-03-09 19:42:51,279 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:43:08,788 - INFO - Batch 65126, Running Avg Loss: 4.64949
2025-03-09 19:43:26,483 - INFO - Batch 65151, Running Avg Loss: 4.64948
2025-03-09 19:43:43,976 - INFO - Batch 65176, Running Avg Loss: 4.64945
2025-03-09 19:44:01,500 - INFO - Batch 65201, Running Avg Loss: 4.64941
2025-03-09 19:44:01,516 - INFO - Batch 65200 finished
2025-03-09 19:44:01,516 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:44:19,035 - INFO - Batch 65226, Running Avg Loss: 4.64937
2025-03-09 19:44:36,727 - INFO - Batch 65251, Running Avg Loss: 4.64934
2025-03-09 19:44:54,232 - INFO - Batch 65276, Running Avg Loss: 4.64930
2025-03-09 19:45:11,775 - INFO - Batch 65301, Running Avg Loss: 4.64928
2025-03-09 19:45:11,793 - INFO - Batch 65300 finished
2025-03-09 19:45:11,794 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:45:29,343 - INFO - Batch 65326, Running Avg Loss: 4.64925
2025-03-09 19:45:46,896 - INFO - Batch 65351, Running Avg Loss: 4.64918
2025-03-09 19:46:04,428 - INFO - Batch 65376, Running Avg Loss: 4.64916
2025-03-09 19:46:22,008 - INFO - Batch 65401, Running Avg Loss: 4.64914
2025-03-09 19:46:22,024 - INFO - Batch 65400 finished
2025-03-09 19:46:22,025 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:46:39,590 - INFO - Batch 65426, Running Avg Loss: 4.64910
2025-03-09 19:46:57,120 - INFO - Batch 65451, Running Avg Loss: 4.64909
2025-03-09 19:47:14,693 - INFO - Batch 65476, Running Avg Loss: 4.64905
2025-03-09 19:47:32,252 - INFO - Batch 65501, Running Avg Loss: 4.64905
2025-03-09 19:47:32,269 - INFO - Batch 65500 finished
2025-03-09 19:47:32,270 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:47:49,780 - INFO - Batch 65526, Running Avg Loss: 4.64901
2025-03-09 19:48:07,471 - INFO - Batch 65551, Running Avg Loss: 4.64899
2025-03-09 19:48:25,002 - INFO - Batch 65576, Running Avg Loss: 4.64893
2025-03-09 19:48:42,495 - INFO - Batch 65601, Running Avg Loss: 4.64890
2025-03-09 19:48:42,511 - INFO - Batch 65600 finished
2025-03-09 19:48:42,512 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:49:00,010 - INFO - Batch 65626, Running Avg Loss: 4.64888
2025-03-09 19:49:17,537 - INFO - Batch 65651, Running Avg Loss: 4.64880
2025-03-09 19:49:35,145 - INFO - Batch 65676, Running Avg Loss: 4.64875
2025-03-09 19:49:52,672 - INFO - Batch 65701, Running Avg Loss: 4.64871
2025-03-09 19:49:52,687 - INFO - Batch 65700 finished
2025-03-09 19:49:52,688 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:50:10,212 - INFO - Batch 65726, Running Avg Loss: 4.64869
2025-03-09 19:50:27,764 - INFO - Batch 65751, Running Avg Loss: 4.64864
2025-03-09 19:50:45,260 - INFO - Batch 65776, Running Avg Loss: 4.64858
2025-03-09 19:51:02,734 - INFO - Batch 65801, Running Avg Loss: 4.64851
2025-03-09 19:51:02,750 - INFO - Batch 65800 finished
2025-03-09 19:51:02,750 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:51:20,377 - INFO - Batch 65826, Running Avg Loss: 4.64847
2025-03-09 19:51:37,905 - INFO - Batch 65851, Running Avg Loss: 4.64844
2025-03-09 19:51:55,430 - INFO - Batch 65876, Running Avg Loss: 4.64840
2025-03-09 19:52:12,971 - INFO - Batch 65901, Running Avg Loss: 4.64840
2025-03-09 19:52:12,989 - INFO - Batch 65900 finished
2025-03-09 19:52:12,989 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:52:30,534 - INFO - Batch 65926, Running Avg Loss: 4.64839
2025-03-09 19:52:48,082 - INFO - Batch 65951, Running Avg Loss: 4.64836
2025-03-09 19:53:05,584 - INFO - Batch 65976, Running Avg Loss: 4.64830
2025-03-09 19:53:23,094 - INFO - Batch 66001, Running Avg Loss: 4.64830
2025-03-09 19:53:23,110 - INFO - Batch 66000 finished
2025-03-09 19:53:23,111 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:53:40,622 - INFO - Batch 66026, Running Avg Loss: 4.64828
2025-03-09 19:53:58,089 - INFO - Batch 66051, Running Avg Loss: 4.64827
2025-03-09 19:54:15,575 - INFO - Batch 66076, Running Avg Loss: 4.64822
2025-03-09 19:54:33,059 - INFO - Batch 66101, Running Avg Loss: 4.64824
2025-03-09 19:54:33,076 - INFO - Batch 66100 finished
2025-03-09 19:54:33,076 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:54:50,678 - INFO - Batch 66126, Running Avg Loss: 4.64817
2025-03-09 19:55:08,159 - INFO - Batch 66151, Running Avg Loss: 4.64813
2025-03-09 19:55:25,648 - INFO - Batch 66176, Running Avg Loss: 4.64807
2025-03-09 19:55:43,145 - INFO - Batch 66201, Running Avg Loss: 4.64805
2025-03-09 19:55:43,162 - INFO - Batch 66200 finished
2025-03-09 19:55:43,163 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:56:00,612 - INFO - Batch 66226, Running Avg Loss: 4.64801
2025-03-09 19:56:18,125 - INFO - Batch 66251, Running Avg Loss: 4.64797
2025-03-09 19:56:35,784 - INFO - Batch 66276, Running Avg Loss: 4.64792
2025-03-09 19:56:53,261 - INFO - Batch 66301, Running Avg Loss: 4.64790
2025-03-09 19:56:53,278 - INFO - Batch 66300 finished
2025-03-09 19:56:53,279 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:57:10,784 - INFO - Batch 66326, Running Avg Loss: 4.64788
2025-03-09 19:57:28,304 - INFO - Batch 66351, Running Avg Loss: 4.64785
2025-03-09 19:57:45,985 - INFO - Batch 66376, Running Avg Loss: 4.64783
2025-03-09 19:58:03,474 - INFO - Batch 66401, Running Avg Loss: 4.64779
2025-03-09 19:58:03,493 - INFO - Batch 66400 finished
2025-03-09 19:58:03,494 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:58:20,989 - INFO - Batch 66426, Running Avg Loss: 4.64774
2025-03-09 19:58:38,501 - INFO - Batch 66451, Running Avg Loss: 4.64771
2025-03-09 19:58:56,071 - INFO - Batch 66476, Running Avg Loss: 4.64770
2025-03-09 19:59:13,640 - INFO - Batch 66501, Running Avg Loss: 4.64767
2025-03-09 19:59:13,658 - INFO - Batch 66500 finished
2025-03-09 19:59:13,659 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 19:59:31,176 - INFO - Batch 66526, Running Avg Loss: 4.64762
2025-03-09 19:59:48,711 - INFO - Batch 66551, Running Avg Loss: 4.64758
2025-03-09 20:00:06,216 - INFO - Batch 66576, Running Avg Loss: 4.64758
2025-03-09 20:00:23,693 - INFO - Batch 66601, Running Avg Loss: 4.64753
2025-03-09 20:00:23,709 - INFO - Batch 66600 finished
2025-03-09 20:00:23,709 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:00:41,066 - INFO - Batch 66626, Running Avg Loss: 4.64750
2025-03-09 20:00:58,573 - INFO - Batch 66651, Running Avg Loss: 4.64749
2025-03-09 20:01:15,627 - INFO - Batch 66676, Running Avg Loss: 4.64746
2025-03-09 20:01:32,961 - INFO - Batch 66701, Running Avg Loss: 4.64747
2025-03-09 20:01:32,978 - INFO - Batch 66700 finished
2025-03-09 20:01:32,979 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:01:50,564 - INFO - Batch 66726, Running Avg Loss: 4.64746
2025-03-09 20:02:08,040 - INFO - Batch 66751, Running Avg Loss: 4.64745
2025-03-09 20:02:25,480 - INFO - Batch 66776, Running Avg Loss: 4.64743
2025-03-09 20:02:42,980 - INFO - Batch 66801, Running Avg Loss: 4.64741
2025-03-09 20:02:42,998 - INFO - Batch 66800 finished
2025-03-09 20:02:42,998 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:03:00,419 - INFO - Batch 66826, Running Avg Loss: 4.64736
2025-03-09 20:03:17,979 - INFO - Batch 66851, Running Avg Loss: 4.64733
2025-03-09 20:03:35,534 - INFO - Batch 66876, Running Avg Loss: 4.64729
2025-03-09 20:03:53,082 - INFO - Batch 66901, Running Avg Loss: 4.64726
2025-03-09 20:03:53,098 - INFO - Batch 66900 finished
2025-03-09 20:03:53,098 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:04:10,661 - INFO - Batch 66926, Running Avg Loss: 4.64724
2025-03-09 20:04:28,376 - INFO - Batch 66951, Running Avg Loss: 4.64722
2025-03-09 20:04:45,873 - INFO - Batch 66976, Running Avg Loss: 4.64723
2025-03-09 20:05:03,383 - INFO - Batch 67001, Running Avg Loss: 4.64720
2025-03-09 20:05:03,399 - INFO - Batch 67000 finished
2025-03-09 20:05:03,399 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:05:20,969 - INFO - Batch 67026, Running Avg Loss: 4.64717
2025-03-09 20:05:38,575 - INFO - Batch 67051, Running Avg Loss: 4.64715
2025-03-09 20:05:55,991 - INFO - Batch 67076, Running Avg Loss: 4.64712
2025-03-09 20:06:13,379 - INFO - Batch 67101, Running Avg Loss: 4.64707
2025-03-09 20:06:13,396 - INFO - Batch 67100 finished
2025-03-09 20:06:13,396 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:06:30,730 - INFO - Batch 67126, Running Avg Loss: 4.64703
2025-03-09 20:06:48,190 - INFO - Batch 67151, Running Avg Loss: 4.64700
2025-03-09 20:07:05,732 - INFO - Batch 67176, Running Avg Loss: 4.64697
2025-03-09 20:07:23,205 - INFO - Batch 67201, Running Avg Loss: 4.64694
2025-03-09 20:07:23,222 - INFO - Batch 67200 finished
2025-03-09 20:07:23,222 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:07:40,805 - INFO - Batch 67226, Running Avg Loss: 4.64690
2025-03-09 20:07:58,277 - INFO - Batch 67251, Running Avg Loss: 4.64687
2025-03-09 20:08:15,813 - INFO - Batch 67276, Running Avg Loss: 4.64684
2025-03-09 20:08:33,235 - INFO - Batch 67301, Running Avg Loss: 4.64683
2025-03-09 20:08:33,254 - INFO - Batch 67300 finished
2025-03-09 20:08:33,254 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:08:50,619 - INFO - Batch 67326, Running Avg Loss: 4.64678
2025-03-09 20:09:08,061 - INFO - Batch 67351, Running Avg Loss: 4.64673
2025-03-09 20:09:25,514 - INFO - Batch 67376, Running Avg Loss: 4.64669
2025-03-09 20:09:43,230 - INFO - Batch 67401, Running Avg Loss: 4.64668
2025-03-09 20:09:43,244 - INFO - Batch 67400 finished
2025-03-09 20:09:43,245 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:10:00,593 - INFO - Batch 67426, Running Avg Loss: 4.64665
2025-03-09 20:10:17,913 - INFO - Batch 67451, Running Avg Loss: 4.64662
2025-03-09 20:10:35,199 - INFO - Batch 67476, Running Avg Loss: 4.64658
2025-03-09 20:10:52,440 - INFO - Batch 67501, Running Avg Loss: 4.64656
2025-03-09 20:10:52,453 - INFO - 
GPU Memory Stats at step 67500:
2025-03-09 20:10:52,453 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 20:10:52,454 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 20:10:52,454 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 20:10:52,454 - INFO - learning rate: 0.00000001
2025-03-09 20:10:52,454 - INFO - Ep 1 (Step 067500): Avg loss 4.647 | 276484096 tokens seen
2025-03-09 20:10:52,454 - INFO - optimizer lr: 0.00000001
2025-03-09 20:10:52,454 - INFO - scheduler lr: 0.00000001
2025-03-09 20:10:52,454 - INFO - Selected prompt: Introduction: The Art of Crafting Vegan Sandwich Delights Sandwiches occupy a unique space in
2025-03-09 20:10:52,454 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:10:52,454 - INFO - random_topk: 4
2025-03-09 20:10:52,455 - INFO - random_temperature: 0.803009999453815
2025-03-09 20:10:52,455 - INFO - global step 67500 , batch_idx 67500 => generating text
2025-03-09 20:10:52,455 - INFO - Generating on device cuda
2025-03-09 20:11:25,220 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:11:25,221 - INFO - Introduction: The Art of Crafting Vegan Sandwich Delights Sandwiches occupy a unique space in ancient civilizations. It was a fascinating period in ancient civilizations, where artists like the "Ale of the United States," and it was a significant aspect of this region.

Imagine you have a big, beautiful forest filled with colorful flowers, and the water was just a few years ago! That's what we now call "The Gira" and "The Little Mitz," which was a group of animals who lived thousands of years ago. They started using the art of the United States, which was a powerful tool for many people.

One famous example of the most famous people who lived in the 19th century. He was born into the Middle East, which took over 1000 years ago. She had many different parts of the country's history, including the "The Gira" in 2028.

One day, while walking into a beautiful forest, he noticed something unusual - the people who lived near the country. He would gather around the water and create the beautiful, colorful flowers. This was the perfect, but it wasn't just about the land of the sea.

Now, imagine you're living in your neighborhood and wanted to share your friends with the people. That would be pretty cool,
2025-03-09 20:11:25,221 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:11:49,572 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_67500_steps_avg_loss_4.64656_optimizer_lr_0.00000001.pth
2025-03-09 20:11:49,807 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 20:11:49,807 - INFO - Batch 67500 finished
2025-03-09 20:11:49,807 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:12:06,955 - INFO - Batch 67526, Running Avg Loss: 4.64654
2025-03-09 20:12:24,315 - INFO - Batch 67551, Running Avg Loss: 4.64648
2025-03-09 20:12:41,975 - INFO - Batch 67576, Running Avg Loss: 4.64646
2025-03-09 20:12:59,491 - INFO - Batch 67601, Running Avg Loss: 4.64645
2025-03-09 20:12:59,507 - INFO - Batch 67600 finished
2025-03-09 20:12:59,508 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:13:17,096 - INFO - Batch 67626, Running Avg Loss: 4.64643
2025-03-09 20:13:34,687 - INFO - Batch 67651, Running Avg Loss: 4.64640
2025-03-09 20:13:52,296 - INFO - Batch 67676, Running Avg Loss: 4.64642
2025-03-09 20:14:09,835 - INFO - Batch 67701, Running Avg Loss: 4.64639
2025-03-09 20:14:09,852 - INFO - Batch 67700 finished
2025-03-09 20:14:09,852 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:14:27,360 - INFO - Batch 67726, Running Avg Loss: 4.64635
2025-03-09 20:14:44,949 - INFO - Batch 67751, Running Avg Loss: 4.64631
2025-03-09 20:15:02,561 - INFO - Batch 67776, Running Avg Loss: 4.64626
2025-03-09 20:15:20,037 - INFO - Batch 67801, Running Avg Loss: 4.64625
2025-03-09 20:15:20,054 - INFO - Batch 67800 finished
2025-03-09 20:15:20,054 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:15:37,520 - INFO - Batch 67826, Running Avg Loss: 4.64620
2025-03-09 20:15:55,197 - INFO - Batch 67851, Running Avg Loss: 4.64615
2025-03-09 20:16:12,760 - INFO - Batch 67876, Running Avg Loss: 4.64614
2025-03-09 20:16:30,330 - INFO - Batch 67901, Running Avg Loss: 4.64611
2025-03-09 20:16:30,348 - INFO - Batch 67900 finished
2025-03-09 20:16:30,348 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:16:47,819 - INFO - Batch 67926, Running Avg Loss: 4.64610
2025-03-09 20:17:05,286 - INFO - Batch 67951, Running Avg Loss: 4.64609
2025-03-09 20:17:22,748 - INFO - Batch 67976, Running Avg Loss: 4.64607
2025-03-09 20:17:40,356 - INFO - Batch 68001, Running Avg Loss: 4.64607
2025-03-09 20:17:40,373 - INFO - Batch 68000 finished
2025-03-09 20:17:40,374 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:17:57,932 - INFO - Batch 68026, Running Avg Loss: 4.64607
2025-03-09 20:18:15,578 - INFO - Batch 68051, Running Avg Loss: 4.64604
2025-03-09 20:18:33,150 - INFO - Batch 68076, Running Avg Loss: 4.64599
2025-03-09 20:18:50,661 - INFO - Batch 68101, Running Avg Loss: 4.64596
2025-03-09 20:18:50,677 - INFO - Batch 68100 finished
2025-03-09 20:18:50,678 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:19:08,249 - INFO - Batch 68126, Running Avg Loss: 4.64594
2025-03-09 20:19:25,998 - INFO - Batch 68151, Running Avg Loss: 4.64591
2025-03-09 20:19:43,545 - INFO - Batch 68176, Running Avg Loss: 4.64588
2025-03-09 20:20:01,061 - INFO - Batch 68201, Running Avg Loss: 4.64589
2025-03-09 20:20:01,079 - INFO - Batch 68200 finished
2025-03-09 20:20:01,079 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:20:18,595 - INFO - Batch 68226, Running Avg Loss: 4.64588
2025-03-09 20:20:36,074 - INFO - Batch 68251, Running Avg Loss: 4.64586
2025-03-09 20:20:53,278 - INFO - Batch 68276, Running Avg Loss: 4.64584
2025-03-09 20:21:10,568 - INFO - Batch 68301, Running Avg Loss: 4.64581
2025-03-09 20:21:10,582 - INFO - Batch 68300 finished
2025-03-09 20:21:10,583 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:21:27,883 - INFO - Batch 68326, Running Avg Loss: 4.64577
2025-03-09 20:21:45,398 - INFO - Batch 68351, Running Avg Loss: 4.64575
2025-03-09 20:22:02,913 - INFO - Batch 68376, Running Avg Loss: 4.64570
2025-03-09 20:22:20,455 - INFO - Batch 68401, Running Avg Loss: 4.64569
2025-03-09 20:22:20,472 - INFO - Batch 68400 finished
2025-03-09 20:22:20,472 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:22:38,195 - INFO - Batch 68426, Running Avg Loss: 4.64565
2025-03-09 20:22:55,727 - INFO - Batch 68451, Running Avg Loss: 4.64558
2025-03-09 20:23:13,224 - INFO - Batch 68476, Running Avg Loss: 4.64551
2025-03-09 20:23:30,758 - INFO - Batch 68501, Running Avg Loss: 4.64546
2025-03-09 20:23:30,774 - INFO - Batch 68500 finished
2025-03-09 20:23:30,774 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:23:48,313 - INFO - Batch 68526, Running Avg Loss: 4.64541
2025-03-09 20:24:05,886 - INFO - Batch 68551, Running Avg Loss: 4.64536
2025-03-09 20:24:23,614 - INFO - Batch 68576, Running Avg Loss: 4.64532
2025-03-09 20:24:41,187 - INFO - Batch 68601, Running Avg Loss: 4.64532
2025-03-09 20:24:41,201 - INFO - Batch 68600 finished
2025-03-09 20:24:41,202 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:24:58,717 - INFO - Batch 68626, Running Avg Loss: 4.64528
2025-03-09 20:25:16,236 - INFO - Batch 68651, Running Avg Loss: 4.64523
2025-03-09 20:25:33,760 - INFO - Batch 68676, Running Avg Loss: 4.64518
2025-03-09 20:25:51,521 - INFO - Batch 68701, Running Avg Loss: 4.64517
2025-03-09 20:25:51,538 - INFO - Batch 68700 finished
2025-03-09 20:25:51,539 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:26:09,054 - INFO - Batch 68726, Running Avg Loss: 4.64517
2025-03-09 20:26:26,554 - INFO - Batch 68751, Running Avg Loss: 4.64514
2025-03-09 20:26:44,139 - INFO - Batch 68776, Running Avg Loss: 4.64512
2025-03-09 20:27:01,695 - INFO - Batch 68801, Running Avg Loss: 4.64510
2025-03-09 20:27:01,714 - INFO - Batch 68800 finished
2025-03-09 20:27:01,715 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:27:19,300 - INFO - Batch 68826, Running Avg Loss: 4.64507
2025-03-09 20:27:36,918 - INFO - Batch 68851, Running Avg Loss: 4.64503
2025-03-09 20:27:54,414 - INFO - Batch 68876, Running Avg Loss: 4.64501
2025-03-09 20:28:11,967 - INFO - Batch 68901, Running Avg Loss: 4.64498
2025-03-09 20:28:11,983 - INFO - Batch 68900 finished
2025-03-09 20:28:11,983 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:28:29,473 - INFO - Batch 68926, Running Avg Loss: 4.64494
2025-03-09 20:28:47,058 - INFO - Batch 68951, Running Avg Loss: 4.64490
2025-03-09 20:29:04,850 - INFO - Batch 68976, Running Avg Loss: 4.64487
2025-03-09 20:29:22,495 - INFO - Batch 69001, Running Avg Loss: 4.64487
2025-03-09 20:29:22,511 - INFO - Batch 69000 finished
2025-03-09 20:29:22,512 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:29:40,162 - INFO - Batch 69026, Running Avg Loss: 4.64484
2025-03-09 20:29:57,679 - INFO - Batch 69051, Running Avg Loss: 4.64480
2025-03-09 20:30:15,159 - INFO - Batch 69076, Running Avg Loss: 4.64476
2025-03-09 20:30:32,645 - INFO - Batch 69101, Running Avg Loss: 4.64471
2025-03-09 20:30:32,661 - INFO - Batch 69100 finished
2025-03-09 20:30:32,662 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:30:50,227 - INFO - Batch 69126, Running Avg Loss: 4.64470
2025-03-09 20:31:07,465 - INFO - Batch 69151, Running Avg Loss: 4.64469
2025-03-09 20:31:24,866 - INFO - Batch 69176, Running Avg Loss: 4.64465
2025-03-09 20:31:42,273 - INFO - Batch 69201, Running Avg Loss: 4.64461
2025-03-09 20:31:42,291 - INFO - Batch 69200 finished
2025-03-09 20:31:42,292 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:31:59,755 - INFO - Batch 69226, Running Avg Loss: 4.64456
2025-03-09 20:32:17,369 - INFO - Batch 69251, Running Avg Loss: 4.64453
2025-03-09 20:32:35,100 - INFO - Batch 69276, Running Avg Loss: 4.64453
2025-03-09 20:32:52,546 - INFO - Batch 69301, Running Avg Loss: 4.64453
2025-03-09 20:32:52,567 - INFO - Batch 69300 finished
2025-03-09 20:32:52,568 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:33:10,007 - INFO - Batch 69326, Running Avg Loss: 4.64450
2025-03-09 20:33:27,495 - INFO - Batch 69351, Running Avg Loss: 4.64446
2025-03-09 20:33:44,993 - INFO - Batch 69376, Running Avg Loss: 4.64440
2025-03-09 20:34:02,551 - INFO - Batch 69401, Running Avg Loss: 4.64437
2025-03-09 20:34:02,568 - INFO - Batch 69400 finished
2025-03-09 20:34:02,568 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:34:20,150 - INFO - Batch 69426, Running Avg Loss: 4.64433
2025-03-09 20:34:37,684 - INFO - Batch 69451, Running Avg Loss: 4.64429
2025-03-09 20:34:55,265 - INFO - Batch 69476, Running Avg Loss: 4.64426
2025-03-09 20:35:12,885 - INFO - Batch 69501, Running Avg Loss: 4.64422
2025-03-09 20:35:12,904 - INFO - Batch 69500 finished
2025-03-09 20:35:12,905 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:35:30,541 - INFO - Batch 69526, Running Avg Loss: 4.64418
2025-03-09 20:35:48,367 - INFO - Batch 69551, Running Avg Loss: 4.64417
2025-03-09 20:36:05,883 - INFO - Batch 69576, Running Avg Loss: 4.64413
2025-03-09 20:36:23,415 - INFO - Batch 69601, Running Avg Loss: 4.64410
2025-03-09 20:36:23,435 - INFO - Batch 69600 finished
2025-03-09 20:36:23,436 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:36:40,912 - INFO - Batch 69626, Running Avg Loss: 4.64407
2025-03-09 20:36:58,551 - INFO - Batch 69651, Running Avg Loss: 4.64404
2025-03-09 20:37:16,185 - INFO - Batch 69676, Running Avg Loss: 4.64403
2025-03-09 20:37:33,934 - INFO - Batch 69701, Running Avg Loss: 4.64399
2025-03-09 20:37:33,952 - INFO - Batch 69700 finished
2025-03-09 20:37:33,953 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:37:51,514 - INFO - Batch 69726, Running Avg Loss: 4.64398
2025-03-09 20:38:09,017 - INFO - Batch 69751, Running Avg Loss: 4.64396
2025-03-09 20:38:26,550 - INFO - Batch 69776, Running Avg Loss: 4.64397
2025-03-09 20:38:44,386 - INFO - Batch 69801, Running Avg Loss: 4.64394
2025-03-09 20:38:44,405 - INFO - Batch 69800 finished
2025-03-09 20:38:44,406 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:39:02,009 - INFO - Batch 69826, Running Avg Loss: 4.64390
2025-03-09 20:39:19,586 - INFO - Batch 69851, Running Avg Loss: 4.64388
2025-03-09 20:39:37,207 - INFO - Batch 69876, Running Avg Loss: 4.64384
2025-03-09 20:39:54,866 - INFO - Batch 69901, Running Avg Loss: 4.64379
2025-03-09 20:39:54,881 - INFO - Batch 69900 finished
2025-03-09 20:39:54,882 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:40:12,500 - INFO - Batch 69926, Running Avg Loss: 4.64376
2025-03-09 20:40:30,141 - INFO - Batch 69951, Running Avg Loss: 4.64374
2025-03-09 20:40:47,439 - INFO - Batch 69976, Running Avg Loss: 4.64373
2025-03-09 20:41:04,694 - INFO - Batch 70001, Running Avg Loss: 4.64369
2025-03-09 20:41:04,707 - INFO - 
GPU Memory Stats at step 70000:
2025-03-09 20:41:04,707 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 20:41:04,707 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 20:41:04,707 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 20:41:04,707 - INFO - learning rate: 0.00000001
2025-03-09 20:41:04,707 - INFO - Ep 1 (Step 070000): Avg loss 4.644 | 286724096 tokens seen
2025-03-09 20:41:04,708 - INFO - optimizer lr: 0.00000001
2025-03-09 20:41:04,708 - INFO - scheduler lr: 0.00000001
2025-03-09 20:41:04,708 - INFO - Selected prompt: Correctly identifying what is causing a problem is the most important step in pest control.
2025-03-09 20:41:04,708 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:41:04,708 - INFO - random_topk: 4
2025-03-09 20:41:04,708 - INFO - random_temperature: 0.8317449401715173
2025-03-09 20:41:04,708 - INFO - global step 70000 , batch_idx 70000 => generating text
2025-03-09 20:41:04,708 - INFO - Generating on device cuda
2025-03-09 20:41:38,469 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:41:38,470 - INFO - Correctly identifying what is causing a problem is the most important step in pest control. It involves using a device that allows you to move, share, and feel more comfortable and healthy. However, this is the study of a study that allows us to explore various aspects of the body, including the way we can make, learn, and grow in our own. In this chapter, we will delve into the concept of 'ditor-based and 'A-year-old, which has been shown to be a way to help us understand the complex relationship between our thoughts and feelings. We will also explore the role of the study of our own thoughts, feelings, and experiences within the context of the environment and the role they face in our own lives.

Firstly, let's talk about some key terms. A well-designed and emotional intelligence refers to the idea that the brain is to make the difference between the same language and the body. This means that our thoughts, feelings, and behaviors can be difficult to be more likely to be more accessible than others. For instance, if we are to think about how we are, we may feel more comfortable and more comfortable, but they can also be more challenging to us.

Now let's dive into the concept of 't' and 's-the-sexism.' This concept is known as
2025-03-09 20:41:38,470 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:42:03,427 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_70000_steps_avg_loss_4.64369_optimizer_lr_0.00000001.pth
2025-03-09 20:42:03,605 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 20:42:03,605 - INFO - Batch 70000 finished
2025-03-09 20:42:03,605 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:42:20,974 - INFO - Batch 70026, Running Avg Loss: 4.64367
2025-03-09 20:42:38,456 - INFO - Batch 70051, Running Avg Loss: 4.64362
2025-03-09 20:42:55,980 - INFO - Batch 70076, Running Avg Loss: 4.64360
2025-03-09 20:43:13,531 - INFO - Batch 70101, Running Avg Loss: 4.64356
2025-03-09 20:43:13,547 - INFO - Batch 70100 finished
2025-03-09 20:43:13,547 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:43:31,092 - INFO - Batch 70126, Running Avg Loss: 4.64354
2025-03-09 20:43:48,769 - INFO - Batch 70151, Running Avg Loss: 4.64349
2025-03-09 20:44:06,256 - INFO - Batch 70176, Running Avg Loss: 4.64347
2025-03-09 20:44:23,660 - INFO - Batch 70201, Running Avg Loss: 4.64342
2025-03-09 20:44:23,675 - INFO - Batch 70200 finished
2025-03-09 20:44:23,676 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:44:41,089 - INFO - Batch 70226, Running Avg Loss: 4.64339
2025-03-09 20:44:58,737 - INFO - Batch 70251, Running Avg Loss: 4.64337
2025-03-09 20:45:16,288 - INFO - Batch 70276, Running Avg Loss: 4.64333
2025-03-09 20:45:33,856 - INFO - Batch 70301, Running Avg Loss: 4.64334
2025-03-09 20:45:33,873 - INFO - Batch 70300 finished
2025-03-09 20:45:33,873 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:45:51,422 - INFO - Batch 70326, Running Avg Loss: 4.64331
2025-03-09 20:46:08,974 - INFO - Batch 70351, Running Avg Loss: 4.64327
2025-03-09 20:46:26,538 - INFO - Batch 70376, Running Avg Loss: 4.64325
2025-03-09 20:46:44,085 - INFO - Batch 70401, Running Avg Loss: 4.64324
2025-03-09 20:46:44,101 - INFO - Batch 70400 finished
2025-03-09 20:46:44,101 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:47:01,644 - INFO - Batch 70426, Running Avg Loss: 4.64318
2025-03-09 20:47:19,187 - INFO - Batch 70451, Running Avg Loss: 4.64315
2025-03-09 20:47:36,695 - INFO - Batch 70476, Running Avg Loss: 4.64314
2025-03-09 20:47:54,260 - INFO - Batch 70501, Running Avg Loss: 4.64310
2025-03-09 20:47:54,277 - INFO - Batch 70500 finished
2025-03-09 20:47:54,277 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:48:11,815 - INFO - Batch 70526, Running Avg Loss: 4.64305
2025-03-09 20:48:29,532 - INFO - Batch 70551, Running Avg Loss: 4.64303
2025-03-09 20:48:47,089 - INFO - Batch 70576, Running Avg Loss: 4.64299
2025-03-09 20:49:04,618 - INFO - Batch 70601, Running Avg Loss: 4.64295
2025-03-09 20:49:04,635 - INFO - Batch 70600 finished
2025-03-09 20:49:04,635 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:49:22,176 - INFO - Batch 70626, Running Avg Loss: 4.64292
2025-03-09 20:49:39,790 - INFO - Batch 70651, Running Avg Loss: 4.64290
2025-03-09 20:49:57,485 - INFO - Batch 70676, Running Avg Loss: 4.64289
2025-03-09 20:50:15,052 - INFO - Batch 70701, Running Avg Loss: 4.64284
2025-03-09 20:50:15,070 - INFO - Batch 70700 finished
2025-03-09 20:50:15,071 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:50:32,584 - INFO - Batch 70726, Running Avg Loss: 4.64282
2025-03-09 20:50:50,046 - INFO - Batch 70751, Running Avg Loss: 4.64280
2025-03-09 20:51:07,263 - INFO - Batch 70776, Running Avg Loss: 4.64277
2025-03-09 20:51:24,754 - INFO - Batch 70801, Running Avg Loss: 4.64275
2025-03-09 20:51:24,769 - INFO - Batch 70800 finished
2025-03-09 20:51:24,770 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:51:42,491 - INFO - Batch 70826, Running Avg Loss: 4.64271
2025-03-09 20:52:00,052 - INFO - Batch 70851, Running Avg Loss: 4.64268
2025-03-09 20:52:17,627 - INFO - Batch 70876, Running Avg Loss: 4.64266
2025-03-09 20:52:35,218 - INFO - Batch 70901, Running Avg Loss: 4.64259
2025-03-09 20:52:35,235 - INFO - Batch 70900 finished
2025-03-09 20:52:35,235 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:52:52,803 - INFO - Batch 70926, Running Avg Loss: 4.64257
2025-03-09 20:53:10,415 - INFO - Batch 70951, Running Avg Loss: 4.64254
2025-03-09 20:53:28,048 - INFO - Batch 70976, Running Avg Loss: 4.64249
2025-03-09 20:53:45,599 - INFO - Batch 71001, Running Avg Loss: 4.64247
2025-03-09 20:53:45,616 - INFO - Batch 71000 finished
2025-03-09 20:53:45,616 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:54:03,122 - INFO - Batch 71026, Running Avg Loss: 4.64246
2025-03-09 20:54:20,681 - INFO - Batch 71051, Running Avg Loss: 4.64242
2025-03-09 20:54:38,291 - INFO - Batch 71076, Running Avg Loss: 4.64238
2025-03-09 20:54:55,858 - INFO - Batch 71101, Running Avg Loss: 4.64232
2025-03-09 20:54:55,876 - INFO - Batch 71100 finished
2025-03-09 20:54:55,876 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:55:13,646 - INFO - Batch 71126, Running Avg Loss: 4.64229
2025-03-09 20:55:31,183 - INFO - Batch 71151, Running Avg Loss: 4.64223
2025-03-09 20:55:48,713 - INFO - Batch 71176, Running Avg Loss: 4.64218
2025-03-09 20:56:06,190 - INFO - Batch 71201, Running Avg Loss: 4.64215
2025-03-09 20:56:06,205 - INFO - Batch 71200 finished
2025-03-09 20:56:06,206 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:56:23,698 - INFO - Batch 71226, Running Avg Loss: 4.64211
2025-03-09 20:56:41,199 - INFO - Batch 71251, Running Avg Loss: 4.64210
2025-03-09 20:56:58,891 - INFO - Batch 71276, Running Avg Loss: 4.64203
2025-03-09 20:57:16,449 - INFO - Batch 71301, Running Avg Loss: 4.64198
2025-03-09 20:57:16,466 - INFO - Batch 71300 finished
2025-03-09 20:57:16,466 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:57:33,896 - INFO - Batch 71326, Running Avg Loss: 4.64195
2025-03-09 20:57:51,427 - INFO - Batch 71351, Running Avg Loss: 4.64193
2025-03-09 20:58:09,206 - INFO - Batch 71376, Running Avg Loss: 4.64188
2025-03-09 20:58:26,767 - INFO - Batch 71401, Running Avg Loss: 4.64183
2025-03-09 20:58:26,782 - INFO - Batch 71400 finished
2025-03-09 20:58:26,783 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:58:44,229 - INFO - Batch 71426, Running Avg Loss: 4.64182
2025-03-09 20:59:01,697 - INFO - Batch 71451, Running Avg Loss: 4.64176
2025-03-09 20:59:19,197 - INFO - Batch 71476, Running Avg Loss: 4.64174
2025-03-09 20:59:36,723 - INFO - Batch 71501, Running Avg Loss: 4.64174
2025-03-09 20:59:36,739 - INFO - Batch 71500 finished
2025-03-09 20:59:36,740 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 20:59:54,291 - INFO - Batch 71526, Running Avg Loss: 4.64171
2025-03-09 21:00:11,843 - INFO - Batch 71551, Running Avg Loss: 4.64167
2025-03-09 21:00:29,248 - INFO - Batch 71576, Running Avg Loss: 4.64166
2025-03-09 21:00:46,431 - INFO - Batch 71601, Running Avg Loss: 4.64163
2025-03-09 21:00:46,444 - INFO - Batch 71600 finished
2025-03-09 21:00:46,444 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:01:03,497 - INFO - Batch 71626, Running Avg Loss: 4.64158
2025-03-09 21:01:20,800 - INFO - Batch 71651, Running Avg Loss: 4.64154
2025-03-09 21:01:38,281 - INFO - Batch 71676, Running Avg Loss: 4.64151
2025-03-09 21:01:55,809 - INFO - Batch 71701, Running Avg Loss: 4.64148
2025-03-09 21:01:55,826 - INFO - Batch 71700 finished
2025-03-09 21:01:55,826 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:02:13,288 - INFO - Batch 71726, Running Avg Loss: 4.64144
2025-03-09 21:02:30,781 - INFO - Batch 71751, Running Avg Loss: 4.64143
2025-03-09 21:02:48,223 - INFO - Batch 71776, Running Avg Loss: 4.64142
2025-03-09 21:03:05,772 - INFO - Batch 71801, Running Avg Loss: 4.64138
2025-03-09 21:03:05,789 - INFO - Batch 71800 finished
2025-03-09 21:03:05,789 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:03:23,266 - INFO - Batch 71826, Running Avg Loss: 4.64141
2025-03-09 21:03:40,828 - INFO - Batch 71851, Running Avg Loss: 4.64138
2025-03-09 21:03:58,337 - INFO - Batch 71876, Running Avg Loss: 4.64137
2025-03-09 21:04:15,944 - INFO - Batch 71901, Running Avg Loss: 4.64132
2025-03-09 21:04:15,962 - INFO - Batch 71900 finished
2025-03-09 21:04:15,962 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:04:33,532 - INFO - Batch 71926, Running Avg Loss: 4.64127
2025-03-09 21:04:51,261 - INFO - Batch 71951, Running Avg Loss: 4.64123
2025-03-09 21:05:08,807 - INFO - Batch 71976, Running Avg Loss: 4.64118
2025-03-09 21:05:26,342 - INFO - Batch 72001, Running Avg Loss: 4.64115
2025-03-09 21:05:26,356 - INFO - Batch 72000 finished
2025-03-09 21:05:26,357 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:05:43,931 - INFO - Batch 72026, Running Avg Loss: 4.64112
2025-03-09 21:06:01,519 - INFO - Batch 72051, Running Avg Loss: 4.64107
2025-03-09 21:06:19,094 - INFO - Batch 72076, Running Avg Loss: 4.64104
2025-03-09 21:06:36,590 - INFO - Batch 72101, Running Avg Loss: 4.64104
2025-03-09 21:06:36,607 - INFO - Batch 72100 finished
2025-03-09 21:06:36,608 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:06:54,082 - INFO - Batch 72126, Running Avg Loss: 4.64105
2025-03-09 21:07:11,566 - INFO - Batch 72151, Running Avg Loss: 4.64099
2025-03-09 21:07:29,099 - INFO - Batch 72176, Running Avg Loss: 4.64095
2025-03-09 21:07:46,653 - INFO - Batch 72201, Running Avg Loss: 4.64093
2025-03-09 21:07:46,671 - INFO - Batch 72200 finished
2025-03-09 21:07:46,671 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:08:04,398 - INFO - Batch 72226, Running Avg Loss: 4.64093
2025-03-09 21:08:21,844 - INFO - Batch 72251, Running Avg Loss: 4.64089
2025-03-09 21:08:39,264 - INFO - Batch 72276, Running Avg Loss: 4.64085
2025-03-09 21:08:56,734 - INFO - Batch 72301, Running Avg Loss: 4.64082
2025-03-09 21:08:56,752 - INFO - Batch 72300 finished
2025-03-09 21:08:56,752 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:09:14,218 - INFO - Batch 72326, Running Avg Loss: 4.64079
2025-03-09 21:09:31,703 - INFO - Batch 72351, Running Avg Loss: 4.64075
2025-03-09 21:09:49,179 - INFO - Batch 72376, Running Avg Loss: 4.64073
2025-03-09 21:10:06,834 - INFO - Batch 72401, Running Avg Loss: 4.64072
2025-03-09 21:10:06,849 - INFO - Batch 72400 finished
2025-03-09 21:10:06,849 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:10:24,269 - INFO - Batch 72426, Running Avg Loss: 4.64070
2025-03-09 21:10:41,519 - INFO - Batch 72451, Running Avg Loss: 4.64069
2025-03-09 21:10:59,030 - INFO - Batch 72476, Running Avg Loss: 4.64067
2025-03-09 21:11:16,588 - INFO - Batch 72501, Running Avg Loss: 4.64063
2025-03-09 21:11:16,603 - INFO - 
GPU Memory Stats at step 72500:
2025-03-09 21:11:16,603 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 21:11:16,603 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 21:11:16,604 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 21:11:16,604 - INFO - learning rate: 0.00000001
2025-03-09 21:11:16,604 - INFO - Ep 1 (Step 072500): Avg loss 4.641 | 296964096 tokens seen
2025-03-09 21:11:16,604 - INFO - optimizer lr: 0.00000001
2025-03-09 21:11:16,604 - INFO - scheduler lr: 0.00000001
2025-03-09 21:11:16,604 - INFO - Selected prompt: Bees are vital for pollination. You can buy leafcutter bee houses to attract 
2025-03-09 21:11:16,604 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:11:16,604 - INFO - random_topk: 1
2025-03-09 21:11:16,604 - INFO - random_temperature: 0.7568487495523715
2025-03-09 21:11:16,604 - INFO - global step 72500 , batch_idx 72500 => generating text
2025-03-09 21:11:16,605 - INFO - Generating on device cuda
2025-03-09 21:11:49,802 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:11:49,803 - INFO - Bees are vital for pollination. You can buy leafcutter bee houses to attract 10000. This way, you can create a more inclusive environment for your family and family.

**Step 1: Understand What a Wyna Is**

Before we dive into the world of food, let's talk about what makes a good food. A good food is like a big, beautiful, and beautiful food that can be found in a place called a "The Gira". It's a type of food that has been used for thousands of years, including the United States.

**Step 2: Understand the World of the World**

* **What is a "t'?"
* **What is a "t'?"
* **Sally**: A small, small, and small, and a small, small, and small, it's a way to make a good life.
* **What is a "the term 's'?" or "The term "tap" is a type of food that has its own unique taste and style.
* **Sally**: A small, beautiful, and beautiful, and it's a special place where you can enjoy a new home.
* **Sally**: A small, beautiful, and beautiful, and a small, beautiful
2025-03-09 21:11:49,803 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:12:14,417 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_72500_steps_avg_loss_4.64063_optimizer_lr_0.00000001.pth
2025-03-09 21:12:14,704 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 21:12:14,704 - INFO - Batch 72500 finished
2025-03-09 21:12:14,705 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:12:32,108 - INFO - Batch 72526, Running Avg Loss: 4.64061
2025-03-09 21:12:49,673 - INFO - Batch 72551, Running Avg Loss: 4.64058
2025-03-09 21:13:07,353 - INFO - Batch 72576, Running Avg Loss: 4.64054
2025-03-09 21:13:24,812 - INFO - Batch 72601, Running Avg Loss: 4.64050
2025-03-09 21:13:24,831 - INFO - Batch 72600 finished
2025-03-09 21:13:24,831 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:13:42,274 - INFO - Batch 72626, Running Avg Loss: 4.64047
2025-03-09 21:13:59,760 - INFO - Batch 72651, Running Avg Loss: 4.64043
2025-03-09 21:14:17,248 - INFO - Batch 72676, Running Avg Loss: 4.64039
2025-03-09 21:14:34,690 - INFO - Batch 72701, Running Avg Loss: 4.64036
2025-03-09 21:14:34,709 - INFO - Batch 72700 finished
2025-03-09 21:14:34,709 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:14:52,134 - INFO - Batch 72726, Running Avg Loss: 4.64034
2025-03-09 21:15:09,572 - INFO - Batch 72751, Running Avg Loss: 4.64031
2025-03-09 21:15:27,087 - INFO - Batch 72776, Running Avg Loss: 4.64030
2025-03-09 21:15:44,575 - INFO - Batch 72801, Running Avg Loss: 4.64026
2025-03-09 21:15:44,590 - INFO - Batch 72800 finished
2025-03-09 21:15:44,590 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:16:02,050 - INFO - Batch 72826, Running Avg Loss: 4.64030
2025-03-09 21:16:19,671 - INFO - Batch 72851, Running Avg Loss: 4.64026
2025-03-09 21:16:37,157 - INFO - Batch 72876, Running Avg Loss: 4.64022
2025-03-09 21:16:54,650 - INFO - Batch 72901, Running Avg Loss: 4.64016
2025-03-09 21:16:54,667 - INFO - Batch 72900 finished
2025-03-09 21:16:54,667 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:17:12,105 - INFO - Batch 72926, Running Avg Loss: 4.64016
2025-03-09 21:17:29,585 - INFO - Batch 72951, Running Avg Loss: 4.64014
2025-03-09 21:17:47,055 - INFO - Batch 72976, Running Avg Loss: 4.64012
2025-03-09 21:18:04,562 - INFO - Batch 73001, Running Avg Loss: 4.64006
2025-03-09 21:18:04,578 - INFO - Batch 73000 finished
2025-03-09 21:18:04,578 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:18:22,062 - INFO - Batch 73026, Running Avg Loss: 4.64000
2025-03-09 21:18:39,579 - INFO - Batch 73051, Running Avg Loss: 4.63992
2025-03-09 21:18:57,015 - INFO - Batch 73076, Running Avg Loss: 4.63990
2025-03-09 21:19:14,548 - INFO - Batch 73101, Running Avg Loss: 4.63988
2025-03-09 21:19:14,566 - INFO - Batch 73100 finished
2025-03-09 21:19:14,567 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:19:32,011 - INFO - Batch 73126, Running Avg Loss: 4.63985
2025-03-09 21:19:49,591 - INFO - Batch 73151, Running Avg Loss: 4.63982
2025-03-09 21:20:07,027 - INFO - Batch 73176, Running Avg Loss: 4.63980
2025-03-09 21:20:24,454 - INFO - Batch 73201, Running Avg Loss: 4.63978
2025-03-09 21:20:24,469 - INFO - Batch 73200 finished
2025-03-09 21:20:24,470 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:20:41,820 - INFO - Batch 73226, Running Avg Loss: 4.63977
2025-03-09 21:20:58,962 - INFO - Batch 73251, Running Avg Loss: 4.63975
2025-03-09 21:21:16,228 - INFO - Batch 73276, Running Avg Loss: 4.63971
2025-03-09 21:21:33,708 - INFO - Batch 73301, Running Avg Loss: 4.63970
2025-03-09 21:21:33,723 - INFO - Batch 73300 finished
2025-03-09 21:21:33,723 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:21:51,157 - INFO - Batch 73326, Running Avg Loss: 4.63966
2025-03-09 21:22:08,627 - INFO - Batch 73351, Running Avg Loss: 4.63961
2025-03-09 21:22:26,161 - INFO - Batch 73376, Running Avg Loss: 4.63957
2025-03-09 21:22:43,595 - INFO - Batch 73401, Running Avg Loss: 4.63956
2025-03-09 21:22:43,613 - INFO - Batch 73400 finished
2025-03-09 21:22:43,613 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:23:01,223 - INFO - Batch 73426, Running Avg Loss: 4.63956
2025-03-09 21:23:18,684 - INFO - Batch 73451, Running Avg Loss: 4.63950
2025-03-09 21:23:36,117 - INFO - Batch 73476, Running Avg Loss: 4.63949
2025-03-09 21:23:53,558 - INFO - Batch 73501, Running Avg Loss: 4.63946
2025-03-09 21:23:53,575 - INFO - Batch 73500 finished
2025-03-09 21:23:53,576 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:24:11,026 - INFO - Batch 73526, Running Avg Loss: 4.63943
2025-03-09 21:24:28,466 - INFO - Batch 73551, Running Avg Loss: 4.63944
2025-03-09 21:24:45,963 - INFO - Batch 73576, Running Avg Loss: 4.63943
2025-03-09 21:25:03,282 - INFO - Batch 73601, Running Avg Loss: 4.63937
2025-03-09 21:25:03,299 - INFO - Batch 73600 finished
2025-03-09 21:25:03,299 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:25:20,648 - INFO - Batch 73626, Running Avg Loss: 4.63934
2025-03-09 21:25:38,109 - INFO - Batch 73651, Running Avg Loss: 4.63929
2025-03-09 21:25:55,636 - INFO - Batch 73676, Running Avg Loss: 4.63924
2025-03-09 21:26:13,223 - INFO - Batch 73701, Running Avg Loss: 4.63919
2025-03-09 21:26:13,238 - INFO - Batch 73700 finished
2025-03-09 21:26:13,239 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:26:30,622 - INFO - Batch 73726, Running Avg Loss: 4.63917
2025-03-09 21:26:48,069 - INFO - Batch 73751, Running Avg Loss: 4.63913
2025-03-09 21:27:05,578 - INFO - Batch 73776, Running Avg Loss: 4.63910
2025-03-09 21:27:23,135 - INFO - Batch 73801, Running Avg Loss: 4.63909
2025-03-09 21:27:23,151 - INFO - Batch 73800 finished
2025-03-09 21:27:23,151 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:27:40,756 - INFO - Batch 73826, Running Avg Loss: 4.63906
2025-03-09 21:27:58,437 - INFO - Batch 73851, Running Avg Loss: 4.63904
2025-03-09 21:28:16,047 - INFO - Batch 73876, Running Avg Loss: 4.63901
2025-03-09 21:28:33,480 - INFO - Batch 73901, Running Avg Loss: 4.63898
2025-03-09 21:28:33,497 - INFO - Batch 73900 finished
2025-03-09 21:28:33,498 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:28:50,997 - INFO - Batch 73926, Running Avg Loss: 4.63894
2025-03-09 21:29:08,556 - INFO - Batch 73951, Running Avg Loss: 4.63890
2025-03-09 21:29:26,164 - INFO - Batch 73976, Running Avg Loss: 4.63886
2025-03-09 21:29:43,741 - INFO - Batch 74001, Running Avg Loss: 4.63882
2025-03-09 21:29:43,758 - INFO - Batch 74000 finished
2025-03-09 21:29:43,758 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:30:01,215 - INFO - Batch 74026, Running Avg Loss: 4.63879
2025-03-09 21:30:18,634 - INFO - Batch 74051, Running Avg Loss: 4.63878
2025-03-09 21:30:36,092 - INFO - Batch 74076, Running Avg Loss: 4.63872
2025-03-09 21:30:53,663 - INFO - Batch 74101, Running Avg Loss: 4.63867
2025-03-09 21:30:53,680 - INFO - Batch 74100 finished
2025-03-09 21:30:53,680 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:31:11,213 - INFO - Batch 74126, Running Avg Loss: 4.63866
2025-03-09 21:31:28,683 - INFO - Batch 74151, Running Avg Loss: 4.63862
2025-03-09 21:31:46,168 - INFO - Batch 74176, Running Avg Loss: 4.63861
2025-03-09 21:32:03,637 - INFO - Batch 74201, Running Avg Loss: 4.63859
2025-03-09 21:32:03,653 - INFO - Batch 74200 finished
2025-03-09 21:32:03,653 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:32:21,248 - INFO - Batch 74226, Running Avg Loss: 4.63857
2025-03-09 21:32:38,790 - INFO - Batch 74251, Running Avg Loss: 4.63854
2025-03-09 21:32:56,416 - INFO - Batch 74276, Running Avg Loss: 4.63854
2025-03-09 21:33:13,880 - INFO - Batch 74301, Running Avg Loss: 4.63850
2025-03-09 21:33:13,899 - INFO - Batch 74300 finished
2025-03-09 21:33:13,900 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:33:31,436 - INFO - Batch 74326, Running Avg Loss: 4.63843
2025-03-09 21:33:49,000 - INFO - Batch 74351, Running Avg Loss: 4.63839
2025-03-09 21:34:06,541 - INFO - Batch 74376, Running Avg Loss: 4.63837
2025-03-09 21:34:23,991 - INFO - Batch 74401, Running Avg Loss: 4.63836
2025-03-09 21:34:24,006 - INFO - Batch 74400 finished
2025-03-09 21:34:24,007 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:34:41,407 - INFO - Batch 74426, Running Avg Loss: 4.63834
2025-03-09 21:34:58,851 - INFO - Batch 74451, Running Avg Loss: 4.63830
2025-03-09 21:35:16,530 - INFO - Batch 74476, Running Avg Loss: 4.63825
2025-03-09 21:35:34,093 - INFO - Batch 74501, Running Avg Loss: 4.63819
2025-03-09 21:35:34,107 - INFO - Batch 74500 finished
2025-03-09 21:35:34,108 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:35:51,502 - INFO - Batch 74526, Running Avg Loss: 4.63815
2025-03-09 21:36:09,026 - INFO - Batch 74551, Running Avg Loss: 4.63813
2025-03-09 21:36:26,441 - INFO - Batch 74576, Running Avg Loss: 4.63814
2025-03-09 21:36:43,884 - INFO - Batch 74601, Running Avg Loss: 4.63811
2025-03-09 21:36:43,900 - INFO - Batch 74600 finished
2025-03-09 21:36:43,900 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:37:01,445 - INFO - Batch 74626, Running Avg Loss: 4.63805
2025-03-09 21:37:18,879 - INFO - Batch 74651, Running Avg Loss: 4.63801
2025-03-09 21:37:36,299 - INFO - Batch 74676, Running Avg Loss: 4.63797
2025-03-09 21:37:53,965 - INFO - Batch 74701, Running Avg Loss: 4.63794
2025-03-09 21:37:53,981 - INFO - Batch 74700 finished
2025-03-09 21:37:53,982 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:38:11,429 - INFO - Batch 74726, Running Avg Loss: 4.63793
2025-03-09 21:38:28,953 - INFO - Batch 74751, Running Avg Loss: 4.63792
2025-03-09 21:38:46,381 - INFO - Batch 74776, Running Avg Loss: 4.63787
2025-03-09 21:39:03,975 - INFO - Batch 74801, Running Avg Loss: 4.63786
2025-03-09 21:39:03,993 - INFO - Batch 74800 finished
2025-03-09 21:39:03,994 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:39:21,488 - INFO - Batch 74826, Running Avg Loss: 4.63783
2025-03-09 21:39:39,025 - INFO - Batch 74851, Running Avg Loss: 4.63783
2025-03-09 21:39:56,585 - INFO - Batch 74876, Running Avg Loss: 4.63781
2025-03-09 21:40:14,042 - INFO - Batch 74901, Running Avg Loss: 4.63780
2025-03-09 21:40:14,059 - INFO - Batch 74900 finished
2025-03-09 21:40:14,059 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:40:31,419 - INFO - Batch 74926, Running Avg Loss: 4.63774
2025-03-09 21:40:48,568 - INFO - Batch 74951, Running Avg Loss: 4.63769
2025-03-09 21:41:05,749 - INFO - Batch 74976, Running Avg Loss: 4.63767
2025-03-09 21:41:23,119 - INFO - Batch 75001, Running Avg Loss: 4.63766
2025-03-09 21:41:23,135 - INFO - 
GPU Memory Stats at step 75000:
2025-03-09 21:41:23,136 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 21:41:23,136 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 21:41:23,136 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 21:41:23,136 - INFO - learning rate: 0.00000002
2025-03-09 21:41:23,136 - INFO - Ep 1 (Step 075000): Avg loss 4.638 | 307204096 tokens seen
2025-03-09 21:41:23,136 - INFO - optimizer lr: 0.00000002
2025-03-09 21:41:23,136 - INFO - scheduler lr: 0.00000002
2025-03-09 21:41:23,136 - INFO - Selected prompt: Once upon a time, in a colorful town called Popville, 
2025-03-09 21:41:23,136 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:41:23,137 - INFO - random_topk: 1
2025-03-09 21:41:23,137 - INFO - random_temperature: 0.8992553080055317
2025-03-09 21:41:23,137 - INFO - global step 75000 , batch_idx 75000 => generating text
2025-03-09 21:41:23,137 - INFO - Generating on device cuda
2025-03-09 21:41:56,544 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:41:56,545 - INFO - Once upon a time, in a colorful town called Popville, 1900, there lived many different kinds of animals who lived in the world. One day, they decided to learn about something called "The Gira" - the "The Gira" and the "The Gira" in the 19th century.

One day, while playing with a big, beautiful city named the Squirrel, they stumbled upon a big, beautiful city called the "The Gira" and the "The Gira" in the 19th century. This made them think of it like a big, beautiful city filled with beautiful buildings, and sometimes even a big, beautiful city!

One day, a group of people came from a big city called the "The Gira" and the "The Gira" in the 19th century. He was known for his love for his friends, but he decided to take a try!

"What does it mean to be a big, dear?" asked his mom, "What does it mean to be a big, but do you know how to make your friends?"

"Well, I see. It's a big, beautiful city where people lived in the world. They wanted to learn about the world around us, like the "The
2025-03-09 21:41:56,545 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:42:24,915 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_75000_steps_avg_loss_4.63766_optimizer_lr_0.00000002.pth
2025-03-09 21:42:25,164 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 21:42:25,164 - INFO - Batch 75000 finished
2025-03-09 21:42:25,165 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:42:42,642 - INFO - Batch 75026, Running Avg Loss: 4.63763
2025-03-09 21:43:00,149 - INFO - Batch 75051, Running Avg Loss: 4.63763
2025-03-09 21:43:17,615 - INFO - Batch 75076, Running Avg Loss: 4.63760
2025-03-09 21:43:35,174 - INFO - Batch 75101, Running Avg Loss: 4.63758
2025-03-09 21:43:35,191 - INFO - Batch 75100 finished
2025-03-09 21:43:35,191 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:43:52,698 - INFO - Batch 75126, Running Avg Loss: 4.63760
2025-03-09 21:44:10,344 - INFO - Batch 75151, Running Avg Loss: 4.63757
2025-03-09 21:44:27,719 - INFO - Batch 75176, Running Avg Loss: 4.63755
2025-03-09 21:44:45,109 - INFO - Batch 75201, Running Avg Loss: 4.63751
2025-03-09 21:44:45,128 - INFO - Batch 75200 finished
2025-03-09 21:44:45,128 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:45:02,660 - INFO - Batch 75226, Running Avg Loss: 4.63750
2025-03-09 21:45:20,452 - INFO - Batch 75251, Running Avg Loss: 4.63745
2025-03-09 21:45:38,018 - INFO - Batch 75276, Running Avg Loss: 4.63742
2025-03-09 21:45:55,511 - INFO - Batch 75301, Running Avg Loss: 4.63741
2025-03-09 21:45:55,529 - INFO - Batch 75300 finished
2025-03-09 21:45:55,530 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:46:13,006 - INFO - Batch 75326, Running Avg Loss: 4.63740
2025-03-09 21:46:30,620 - INFO - Batch 75351, Running Avg Loss: 4.63739
2025-03-09 21:46:48,224 - INFO - Batch 75376, Running Avg Loss: 4.63738
2025-03-09 21:47:05,749 - INFO - Batch 75401, Running Avg Loss: 4.63733
2025-03-09 21:47:05,765 - INFO - Batch 75400 finished
2025-03-09 21:47:05,766 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:47:23,325 - INFO - Batch 75426, Running Avg Loss: 4.63731
2025-03-09 21:47:40,856 - INFO - Batch 75451, Running Avg Loss: 4.63727
2025-03-09 21:47:58,347 - INFO - Batch 75476, Running Avg Loss: 4.63723
2025-03-09 21:48:15,883 - INFO - Batch 75501, Running Avg Loss: 4.63720
2025-03-09 21:48:15,901 - INFO - Batch 75500 finished
2025-03-09 21:48:15,901 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:48:33,315 - INFO - Batch 75526, Running Avg Loss: 4.63719
2025-03-09 21:48:50,942 - INFO - Batch 75551, Running Avg Loss: 4.63717
2025-03-09 21:49:08,466 - INFO - Batch 75576, Running Avg Loss: 4.63714
2025-03-09 21:49:26,052 - INFO - Batch 75601, Running Avg Loss: 4.63714
2025-03-09 21:49:26,065 - INFO - Batch 75600 finished
2025-03-09 21:49:26,066 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:49:43,678 - INFO - Batch 75626, Running Avg Loss: 4.63710
2025-03-09 21:50:01,233 - INFO - Batch 75651, Running Avg Loss: 4.63707
2025-03-09 21:50:18,821 - INFO - Batch 75676, Running Avg Loss: 4.63705
2025-03-09 21:50:36,339 - INFO - Batch 75701, Running Avg Loss: 4.63703
2025-03-09 21:50:36,358 - INFO - Batch 75700 finished
2025-03-09 21:50:36,359 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:50:53,852 - INFO - Batch 75726, Running Avg Loss: 4.63699
2025-03-09 21:51:11,338 - INFO - Batch 75751, Running Avg Loss: 4.63700
2025-03-09 21:51:28,681 - INFO - Batch 75776, Running Avg Loss: 4.63700
2025-03-09 21:51:46,258 - INFO - Batch 75801, Running Avg Loss: 4.63696
2025-03-09 21:51:46,276 - INFO - Batch 75800 finished
2025-03-09 21:51:46,276 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:52:03,925 - INFO - Batch 75826, Running Avg Loss: 4.63694
2025-03-09 21:52:21,407 - INFO - Batch 75851, Running Avg Loss: 4.63692
2025-03-09 21:52:38,901 - INFO - Batch 75876, Running Avg Loss: 4.63693
2025-03-09 21:52:56,415 - INFO - Batch 75901, Running Avg Loss: 4.63692
2025-03-09 21:52:56,430 - INFO - Batch 75900 finished
2025-03-09 21:52:56,431 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:53:13,954 - INFO - Batch 75926, Running Avg Loss: 4.63692
2025-03-09 21:53:31,506 - INFO - Batch 75951, Running Avg Loss: 4.63688
2025-03-09 21:53:49,009 - INFO - Batch 75976, Running Avg Loss: 4.63684
2025-03-09 21:54:06,522 - INFO - Batch 76001, Running Avg Loss: 4.63683
2025-03-09 21:54:06,538 - INFO - Batch 76000 finished
2025-03-09 21:54:06,539 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:54:24,126 - INFO - Batch 76026, Running Avg Loss: 4.63679
2025-03-09 21:54:41,695 - INFO - Batch 76051, Running Avg Loss: 4.63677
2025-03-09 21:54:59,280 - INFO - Batch 76076, Running Avg Loss: 4.63675
2025-03-09 21:55:16,886 - INFO - Batch 76101, Running Avg Loss: 4.63671
2025-03-09 21:55:16,902 - INFO - Batch 76100 finished
2025-03-09 21:55:16,902 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:55:34,587 - INFO - Batch 76126, Running Avg Loss: 4.63669
2025-03-09 21:55:52,137 - INFO - Batch 76151, Running Avg Loss: 4.63665
2025-03-09 21:56:09,685 - INFO - Batch 76176, Running Avg Loss: 4.63663
2025-03-09 21:56:27,212 - INFO - Batch 76201, Running Avg Loss: 4.63664
2025-03-09 21:56:27,226 - INFO - Batch 76200 finished
2025-03-09 21:56:27,226 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:56:44,762 - INFO - Batch 76226, Running Avg Loss: 4.63664
2025-03-09 21:57:02,306 - INFO - Batch 76251, Running Avg Loss: 4.63662
2025-03-09 21:57:20,051 - INFO - Batch 76276, Running Avg Loss: 4.63658
2025-03-09 21:57:37,617 - INFO - Batch 76301, Running Avg Loss: 4.63657
2025-03-09 21:57:37,633 - INFO - Batch 76300 finished
2025-03-09 21:57:37,634 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:57:55,250 - INFO - Batch 76326, Running Avg Loss: 4.63655
2025-03-09 21:58:12,779 - INFO - Batch 76351, Running Avg Loss: 4.63652
2025-03-09 21:58:30,467 - INFO - Batch 76376, Running Avg Loss: 4.63649
2025-03-09 21:58:47,971 - INFO - Batch 76401, Running Avg Loss: 4.63648
2025-03-09 21:58:47,988 - INFO - Batch 76400 finished
2025-03-09 21:58:47,989 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 21:59:05,455 - INFO - Batch 76426, Running Avg Loss: 4.63643
2025-03-09 21:59:22,925 - INFO - Batch 76451, Running Avg Loss: 4.63641
2025-03-09 21:59:40,448 - INFO - Batch 76476, Running Avg Loss: 4.63638
2025-03-09 21:59:58,018 - INFO - Batch 76501, Running Avg Loss: 4.63634
2025-03-09 21:59:58,032 - INFO - Batch 76500 finished
2025-03-09 21:59:58,033 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:00:15,610 - INFO - Batch 76526, Running Avg Loss: 4.63631
2025-03-09 22:00:33,190 - INFO - Batch 76551, Running Avg Loss: 4.63627
2025-03-09 22:00:50,694 - INFO - Batch 76576, Running Avg Loss: 4.63625
2025-03-09 22:01:08,304 - INFO - Batch 76601, Running Avg Loss: 4.63622
2025-03-09 22:01:08,319 - INFO - Batch 76600 finished
2025-03-09 22:01:08,320 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:01:25,772 - INFO - Batch 76626, Running Avg Loss: 4.63623
2025-03-09 22:01:43,046 - INFO - Batch 76651, Running Avg Loss: 4.63618
2025-03-09 22:02:00,525 - INFO - Batch 76676, Running Avg Loss: 4.63617
2025-03-09 22:02:18,041 - INFO - Batch 76701, Running Avg Loss: 4.63617
2025-03-09 22:02:18,062 - INFO - Batch 76700 finished
2025-03-09 22:02:18,062 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:02:35,529 - INFO - Batch 76726, Running Avg Loss: 4.63615
2025-03-09 22:02:53,028 - INFO - Batch 76751, Running Avg Loss: 4.63613
2025-03-09 22:03:10,655 - INFO - Batch 76776, Running Avg Loss: 4.63610
2025-03-09 22:03:28,246 - INFO - Batch 76801, Running Avg Loss: 4.63607
2025-03-09 22:03:28,265 - INFO - Batch 76800 finished
2025-03-09 22:03:28,266 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:03:45,779 - INFO - Batch 76826, Running Avg Loss: 4.63603
2025-03-09 22:04:03,348 - INFO - Batch 76851, Running Avg Loss: 4.63600
2025-03-09 22:04:20,920 - INFO - Batch 76876, Running Avg Loss: 4.63595
2025-03-09 22:04:38,468 - INFO - Batch 76901, Running Avg Loss: 4.63592
2025-03-09 22:04:38,487 - INFO - Batch 76900 finished
2025-03-09 22:04:38,487 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:04:56,092 - INFO - Batch 76926, Running Avg Loss: 4.63588
2025-03-09 22:05:13,856 - INFO - Batch 76951, Running Avg Loss: 4.63586
2025-03-09 22:05:31,393 - INFO - Batch 76976, Running Avg Loss: 4.63582
2025-03-09 22:05:48,936 - INFO - Batch 77001, Running Avg Loss: 4.63578
2025-03-09 22:05:48,955 - INFO - Batch 77000 finished
2025-03-09 22:05:48,955 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:06:06,428 - INFO - Batch 77026, Running Avg Loss: 4.63575
2025-03-09 22:06:23,946 - INFO - Batch 77051, Running Avg Loss: 4.63572
2025-03-09 22:06:41,517 - INFO - Batch 77076, Running Avg Loss: 4.63567
2025-03-09 22:06:59,195 - INFO - Batch 77101, Running Avg Loss: 4.63562
2025-03-09 22:06:59,216 - INFO - Batch 77100 finished
2025-03-09 22:06:59,217 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:07:16,737 - INFO - Batch 77126, Running Avg Loss: 4.63561
2025-03-09 22:07:34,250 - INFO - Batch 77151, Running Avg Loss: 4.63559
2025-03-09 22:07:51,774 - INFO - Batch 77176, Running Avg Loss: 4.63556
2025-03-09 22:08:09,317 - INFO - Batch 77201, Running Avg Loss: 4.63557
2025-03-09 22:08:09,335 - INFO - Batch 77200 finished
2025-03-09 22:08:09,336 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:08:26,866 - INFO - Batch 77226, Running Avg Loss: 4.63555
2025-03-09 22:08:44,562 - INFO - Batch 77251, Running Avg Loss: 4.63553
2025-03-09 22:09:02,079 - INFO - Batch 77276, Running Avg Loss: 4.63551
2025-03-09 22:09:19,489 - INFO - Batch 77301, Running Avg Loss: 4.63550
2025-03-09 22:09:19,504 - INFO - Batch 77300 finished
2025-03-09 22:09:19,504 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:09:36,840 - INFO - Batch 77326, Running Avg Loss: 4.63550
2025-03-09 22:09:54,233 - INFO - Batch 77351, Running Avg Loss: 4.63547
2025-03-09 22:10:11,767 - INFO - Batch 77376, Running Avg Loss: 4.63544
2025-03-09 22:10:29,500 - INFO - Batch 77401, Running Avg Loss: 4.63540
2025-03-09 22:10:29,520 - INFO - Batch 77400 finished
2025-03-09 22:10:29,521 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:10:47,130 - INFO - Batch 77426, Running Avg Loss: 4.63538
2025-03-09 22:11:04,276 - INFO - Batch 77451, Running Avg Loss: 4.63537
2025-03-09 22:11:21,528 - INFO - Batch 77476, Running Avg Loss: 4.63533
2025-03-09 22:11:39,119 - INFO - Batch 77501, Running Avg Loss: 4.63529
2025-03-09 22:11:39,138 - INFO - 
GPU Memory Stats at step 77500:
2025-03-09 22:11:39,139 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 22:11:39,139 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 22:11:39,139 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 22:11:39,139 - INFO - learning rate: 0.00000002
2025-03-09 22:11:39,139 - INFO - Ep 1 (Step 077500): Avg loss 4.635 | 317444096 tokens seen
2025-03-09 22:11:39,140 - INFO - optimizer lr: 0.00000002
2025-03-09 22:11:39,140 - INFO - scheduler lr: 0.00000002
2025-03-09 22:11:39,140 - INFO - Selected prompt: A couple of years ago, I was working as an extra on the set of a low-budget British film.
2025-03-09 22:11:39,140 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:11:39,140 - INFO - random_topk: 5
2025-03-09 22:11:39,140 - INFO - random_temperature: 0.8874809329717878
2025-03-09 22:11:39,140 - INFO - global step 77500 , batch_idx 77500 => generating text
2025-03-09 22:11:39,140 - INFO - Generating on device cuda
2025-03-09 22:12:13,034 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:12:13,035 - INFO - A couple of years ago, I was working as an extra on the set of a low-budget British film. But what if we were to do with it to be an unusual piece of paper?

One day, while scrolling through the air, I stumbled upon a few of the most popular types of paper. We started by researching the following libraries:

* A 18-year-old girl
	* 1/25 (127-244)
	* A small but more powerful tool

* The first thing that would be a great idea?

**Step 2: Understanding the World of the Life

Before diving into the process, let's talk about how it works. Imagine you have two different shapes: the number of paper and a number. You might have a picture of a single piece of paper and a line, but you would want you to know what you were doing. This is where the number of numbers is 1,000.
* 4: If the other side is 1,000, then it takes a moment to the number.

Now let's see how the 10th and the 1400s. This is a powerful tool used for the 1000s and 2010, and the
2025-03-09 22:12:13,035 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:12:41,750 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_77500_steps_avg_loss_4.63529_optimizer_lr_0.00000002.pth
2025-03-09 22:12:42,044 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 22:12:42,044 - INFO - Batch 77500 finished
2025-03-09 22:12:42,044 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:12:59,715 - INFO - Batch 77526, Running Avg Loss: 4.63526
2025-03-09 22:13:17,261 - INFO - Batch 77551, Running Avg Loss: 4.63523
2025-03-09 22:13:35,027 - INFO - Batch 77576, Running Avg Loss: 4.63519
2025-03-09 22:13:52,590 - INFO - Batch 77601, Running Avg Loss: 4.63515
2025-03-09 22:13:52,605 - INFO - Batch 77600 finished
2025-03-09 22:13:52,605 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:14:10,160 - INFO - Batch 77626, Running Avg Loss: 4.63514
2025-03-09 22:14:27,707 - INFO - Batch 77651, Running Avg Loss: 4.63512
2025-03-09 22:14:45,301 - INFO - Batch 77676, Running Avg Loss: 4.63508
2025-03-09 22:15:02,864 - INFO - Batch 77701, Running Avg Loss: 4.63508
2025-03-09 22:15:02,883 - INFO - Batch 77700 finished
2025-03-09 22:15:02,884 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:15:20,430 - INFO - Batch 77726, Running Avg Loss: 4.63505
2025-03-09 22:15:38,079 - INFO - Batch 77751, Running Avg Loss: 4.63505
2025-03-09 22:15:55,686 - INFO - Batch 77776, Running Avg Loss: 4.63503
2025-03-09 22:16:13,317 - INFO - Batch 77801, Running Avg Loss: 4.63497
2025-03-09 22:16:13,332 - INFO - Batch 77800 finished
2025-03-09 22:16:13,333 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:16:30,856 - INFO - Batch 77826, Running Avg Loss: 4.63493
2025-03-09 22:16:48,440 - INFO - Batch 77851, Running Avg Loss: 4.63493
2025-03-09 22:17:05,959 - INFO - Batch 77876, Running Avg Loss: 4.63492
2025-03-09 22:17:23,511 - INFO - Batch 77901, Running Avg Loss: 4.63491
2025-03-09 22:17:23,531 - INFO - Batch 77900 finished
2025-03-09 22:17:23,532 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:17:41,108 - INFO - Batch 77926, Running Avg Loss: 4.63491
2025-03-09 22:17:58,632 - INFO - Batch 77951, Running Avg Loss: 4.63486
2025-03-09 22:18:16,102 - INFO - Batch 77976, Running Avg Loss: 4.63482
2025-03-09 22:18:33,699 - INFO - Batch 78001, Running Avg Loss: 4.63480
2025-03-09 22:18:33,716 - INFO - Batch 78000 finished
2025-03-09 22:18:33,716 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:18:51,259 - INFO - Batch 78026, Running Avg Loss: 4.63476
2025-03-09 22:19:08,819 - INFO - Batch 78051, Running Avg Loss: 4.63470
2025-03-09 22:19:26,351 - INFO - Batch 78076, Running Avg Loss: 4.63467
2025-03-09 22:19:43,835 - INFO - Batch 78101, Running Avg Loss: 4.63461
2025-03-09 22:19:43,854 - INFO - Batch 78100 finished
2025-03-09 22:19:43,855 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:20:01,380 - INFO - Batch 78126, Running Avg Loss: 4.63457
2025-03-09 22:20:19,099 - INFO - Batch 78151, Running Avg Loss: 4.63454
2025-03-09 22:20:36,571 - INFO - Batch 78176, Running Avg Loss: 4.63453
2025-03-09 22:20:54,071 - INFO - Batch 78201, Running Avg Loss: 4.63447
2025-03-09 22:20:54,088 - INFO - Batch 78200 finished
2025-03-09 22:20:54,088 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:21:11,573 - INFO - Batch 78226, Running Avg Loss: 4.63443
2025-03-09 22:21:29,057 - INFO - Batch 78251, Running Avg Loss: 4.63442
2025-03-09 22:21:46,552 - INFO - Batch 78276, Running Avg Loss: 4.63439
2025-03-09 22:22:04,047 - INFO - Batch 78301, Running Avg Loss: 4.63438
2025-03-09 22:22:04,062 - INFO - Batch 78300 finished
2025-03-09 22:22:04,062 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:22:21,617 - INFO - Batch 78326, Running Avg Loss: 4.63437
2025-03-09 22:22:39,120 - INFO - Batch 78351, Running Avg Loss: 4.63432
2025-03-09 22:22:56,776 - INFO - Batch 78376, Running Avg Loss: 4.63430
2025-03-09 22:23:14,379 - INFO - Batch 78401, Running Avg Loss: 4.63428
2025-03-09 22:23:14,399 - INFO - Batch 78400 finished
2025-03-09 22:23:14,399 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:23:32,108 - INFO - Batch 78426, Running Avg Loss: 4.63424
2025-03-09 22:23:49,671 - INFO - Batch 78451, Running Avg Loss: 4.63422
2025-03-09 22:24:07,158 - INFO - Batch 78476, Running Avg Loss: 4.63420
2025-03-09 22:24:24,623 - INFO - Batch 78501, Running Avg Loss: 4.63417
2025-03-09 22:24:24,640 - INFO - Batch 78500 finished
2025-03-09 22:24:24,640 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:24:42,140 - INFO - Batch 78526, Running Avg Loss: 4.63415
2025-03-09 22:24:59,631 - INFO - Batch 78551, Running Avg Loss: 4.63412
2025-03-09 22:25:17,339 - INFO - Batch 78576, Running Avg Loss: 4.63408
2025-03-09 22:25:34,834 - INFO - Batch 78601, Running Avg Loss: 4.63405
2025-03-09 22:25:34,850 - INFO - Batch 78600 finished
2025-03-09 22:25:34,851 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:25:52,353 - INFO - Batch 78626, Running Avg Loss: 4.63404
2025-03-09 22:26:09,924 - INFO - Batch 78651, Running Avg Loss: 4.63398
2025-03-09 22:26:27,471 - INFO - Batch 78676, Running Avg Loss: 4.63394
2025-03-09 22:26:45,069 - INFO - Batch 78701, Running Avg Loss: 4.63391
2025-03-09 22:26:45,088 - INFO - Batch 78700 finished
2025-03-09 22:26:45,089 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:27:02,599 - INFO - Batch 78726, Running Avg Loss: 4.63391
2025-03-09 22:27:20,144 - INFO - Batch 78751, Running Avg Loss: 4.63387
2025-03-09 22:27:37,636 - INFO - Batch 78776, Running Avg Loss: 4.63386
2025-03-09 22:27:55,113 - INFO - Batch 78801, Running Avg Loss: 4.63385
2025-03-09 22:27:55,131 - INFO - Batch 78800 finished
2025-03-09 22:27:55,131 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:28:12,604 - INFO - Batch 78826, Running Avg Loss: 4.63382
2025-03-09 22:28:30,149 - INFO - Batch 78851, Running Avg Loss: 4.63381
2025-03-09 22:28:47,661 - INFO - Batch 78876, Running Avg Loss: 4.63375
2025-03-09 22:29:05,148 - INFO - Batch 78901, Running Avg Loss: 4.63375
2025-03-09 22:29:05,164 - INFO - Batch 78900 finished
2025-03-09 22:29:05,165 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:29:22,723 - INFO - Batch 78926, Running Avg Loss: 4.63373
2025-03-09 22:29:40,298 - INFO - Batch 78951, Running Avg Loss: 4.63372
2025-03-09 22:29:58,003 - INFO - Batch 78976, Running Avg Loss: 4.63370
2025-03-09 22:30:15,500 - INFO - Batch 79001, Running Avg Loss: 4.63366
2025-03-09 22:30:15,519 - INFO - Batch 79000 finished
2025-03-09 22:30:15,519 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:30:33,000 - INFO - Batch 79026, Running Avg Loss: 4.63363
2025-03-09 22:30:50,516 - INFO - Batch 79051, Running Avg Loss: 4.63361
2025-03-09 22:31:08,057 - INFO - Batch 79076, Running Avg Loss: 4.63358
2025-03-09 22:31:25,563 - INFO - Batch 79101, Running Avg Loss: 4.63359
2025-03-09 22:31:25,580 - INFO - Batch 79100 finished
2025-03-09 22:31:25,581 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:31:43,136 - INFO - Batch 79126, Running Avg Loss: 4.63355
2025-03-09 22:32:00,680 - INFO - Batch 79151, Running Avg Loss: 4.63353
2025-03-09 22:32:18,149 - INFO - Batch 79176, Running Avg Loss: 4.63349
2025-03-09 22:32:35,673 - INFO - Batch 79201, Running Avg Loss: 4.63346
2025-03-09 22:32:35,690 - INFO - Batch 79200 finished
2025-03-09 22:32:35,690 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:32:53,171 - INFO - Batch 79226, Running Avg Loss: 4.63342
2025-03-09 22:33:10,667 - INFO - Batch 79251, Running Avg Loss: 4.63340
2025-03-09 22:33:28,304 - INFO - Batch 79276, Running Avg Loss: 4.63336
2025-03-09 22:33:45,857 - INFO - Batch 79301, Running Avg Loss: 4.63333
2025-03-09 22:33:45,874 - INFO - Batch 79300 finished
2025-03-09 22:33:45,875 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:34:03,350 - INFO - Batch 79326, Running Avg Loss: 4.63329
2025-03-09 22:34:20,857 - INFO - Batch 79351, Running Avg Loss: 4.63328
2025-03-09 22:34:38,402 - INFO - Batch 79376, Running Avg Loss: 4.63325
2025-03-09 22:34:55,969 - INFO - Batch 79401, Running Avg Loss: 4.63324
2025-03-09 22:34:55,985 - INFO - Batch 79400 finished
2025-03-09 22:34:55,985 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:35:13,567 - INFO - Batch 79426, Running Avg Loss: 4.63320
2025-03-09 22:35:31,126 - INFO - Batch 79451, Running Avg Loss: 4.63316
2025-03-09 22:35:48,635 - INFO - Batch 79476, Running Avg Loss: 4.63313
2025-03-09 22:36:06,221 - INFO - Batch 79501, Running Avg Loss: 4.63312
2025-03-09 22:36:06,236 - INFO - Batch 79500 finished
2025-03-09 22:36:06,236 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:36:23,708 - INFO - Batch 79526, Running Avg Loss: 4.63311
2025-03-09 22:36:41,375 - INFO - Batch 79551, Running Avg Loss: 4.63309
2025-03-09 22:36:58,871 - INFO - Batch 79576, Running Avg Loss: 4.63307
2025-03-09 22:37:16,401 - INFO - Batch 79601, Running Avg Loss: 4.63303
2025-03-09 22:37:16,418 - INFO - Batch 79600 finished
2025-03-09 22:37:16,419 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:37:33,938 - INFO - Batch 79626, Running Avg Loss: 4.63302
2025-03-09 22:37:51,412 - INFO - Batch 79651, Running Avg Loss: 4.63300
2025-03-09 22:38:08,892 - INFO - Batch 79676, Running Avg Loss: 4.63297
2025-03-09 22:38:26,579 - INFO - Batch 79701, Running Avg Loss: 4.63293
2025-03-09 22:38:26,595 - INFO - Batch 79700 finished
2025-03-09 22:38:26,596 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:38:44,174 - INFO - Batch 79726, Running Avg Loss: 4.63292
2025-03-09 22:39:01,780 - INFO - Batch 79751, Running Avg Loss: 4.63290
2025-03-09 22:39:19,340 - INFO - Batch 79776, Running Avg Loss: 4.63291
2025-03-09 22:39:37,044 - INFO - Batch 79801, Running Avg Loss: 4.63290
2025-03-09 22:39:37,062 - INFO - Batch 79800 finished
2025-03-09 22:39:37,062 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:39:54,670 - INFO - Batch 79826, Running Avg Loss: 4.63288
2025-03-09 22:40:12,186 - INFO - Batch 79851, Running Avg Loss: 4.63287
2025-03-09 22:40:29,651 - INFO - Batch 79876, Running Avg Loss: 4.63285
2025-03-09 22:40:47,114 - INFO - Batch 79901, Running Avg Loss: 4.63280
2025-03-09 22:40:47,132 - INFO - Batch 79900 finished
2025-03-09 22:40:47,132 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:41:04,368 - INFO - Batch 79926, Running Avg Loss: 4.63280
2025-03-09 22:41:21,766 - INFO - Batch 79951, Running Avg Loss: 4.63276
2025-03-09 22:41:39,220 - INFO - Batch 79976, Running Avg Loss: 4.63274
2025-03-09 22:41:56,688 - INFO - Batch 80001, Running Avg Loss: 4.63275
2025-03-09 22:41:56,703 - INFO - 
GPU Memory Stats at step 80000:
2025-03-09 22:41:56,703 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 22:41:56,703 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 22:41:56,704 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 22:41:56,704 - INFO - learning rate: 0.00000003
2025-03-09 22:41:56,704 - INFO - Ep 1 (Step 080000): Avg loss 4.633 | 327684096 tokens seen
2025-03-09 22:41:56,704 - INFO - optimizer lr: 0.00000003
2025-03-09 22:41:56,704 - INFO - scheduler lr: 0.00000003
2025-03-09 22:41:56,704 - INFO - Selected prompt: Once upon a time, there was a friendly agency called Gaudette Insurance Agency, Inc. They help
2025-03-09 22:41:56,704 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:41:56,704 - INFO - random_topk: 2
2025-03-09 22:41:56,704 - INFO - random_temperature: 0.8701751062619052
2025-03-09 22:41:56,704 - INFO - global step 80000 , batch_idx 80000 => generating text
2025-03-09 22:41:56,704 - INFO - Generating on device cuda
2025-03-09 22:42:29,923 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:42:29,924 - INFO - Once upon a time, there was a friendly agency called Gaudette Insurance Agency, Inc. They help people understand how things like how to use, and how they could help us understand the world around us. One day, they decided to learn about something called "The Gira" - The Gira of the "The Gira" in the 19th century.

As they continued exploring the world, they discovered that some people were called "A" and "A" in the 19th century. This was known as the "The Great Life of the United States," which was known for its vibrant colors and unique colors.

One day, while playing with a beautiful city, they stumbled across a big box filled with colorful colors, shapes, and shapes. The people had to create something called "The B." which was the "A" and the "The B." in the 19th century, the people who lived in the United States.

One day, a young woman named Max, a wise man named Alex, was known for his love for his love for his life. She was a big and curious, but he decided to take action to help him learn about the world around them.

As the next day, the people of the people of the world came together to learn more about the world
2025-03-09 22:42:29,924 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:42:55,050 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_80000_steps_avg_loss_4.63275_optimizer_lr_0.00000003.pth
2025-03-09 22:42:55,283 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 22:42:55,283 - INFO - Batch 80000 finished
2025-03-09 22:42:55,283 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:43:12,622 - INFO - Batch 80026, Running Avg Loss: 4.63272
2025-03-09 22:43:30,135 - INFO - Batch 80051, Running Avg Loss: 4.63271
2025-03-09 22:43:47,701 - INFO - Batch 80076, Running Avg Loss: 4.63267
2025-03-09 22:44:05,237 - INFO - Batch 80101, Running Avg Loss: 4.63265
2025-03-09 22:44:05,251 - INFO - Batch 80100 finished
2025-03-09 22:44:05,251 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:44:22,726 - INFO - Batch 80126, Running Avg Loss: 4.63263
2025-03-09 22:44:40,443 - INFO - Batch 80151, Running Avg Loss: 4.63262
2025-03-09 22:44:57,963 - INFO - Batch 80176, Running Avg Loss: 4.63260
2025-03-09 22:45:15,462 - INFO - Batch 80201, Running Avg Loss: 4.63258
2025-03-09 22:45:15,478 - INFO - Batch 80200 finished
2025-03-09 22:45:15,478 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:45:32,975 - INFO - Batch 80226, Running Avg Loss: 4.63255
2025-03-09 22:45:50,687 - INFO - Batch 80251, Running Avg Loss: 4.63252
2025-03-09 22:46:08,198 - INFO - Batch 80276, Running Avg Loss: 4.63252
2025-03-09 22:46:25,716 - INFO - Batch 80301, Running Avg Loss: 4.63249
2025-03-09 22:46:25,734 - INFO - Batch 80300 finished
2025-03-09 22:46:25,735 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:46:43,339 - INFO - Batch 80326, Running Avg Loss: 4.63248
2025-03-09 22:47:00,840 - INFO - Batch 80351, Running Avg Loss: 4.63244
2025-03-09 22:47:18,336 - INFO - Batch 80376, Running Avg Loss: 4.63240
2025-03-09 22:47:35,839 - INFO - Batch 80401, Running Avg Loss: 4.63239
2025-03-09 22:47:35,857 - INFO - Batch 80400 finished
2025-03-09 22:47:35,857 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:47:53,360 - INFO - Batch 80426, Running Avg Loss: 4.63236
2025-03-09 22:48:10,860 - INFO - Batch 80451, Running Avg Loss: 4.63237
2025-03-09 22:48:28,370 - INFO - Batch 80476, Running Avg Loss: 4.63236
2025-03-09 22:48:45,822 - INFO - Batch 80501, Running Avg Loss: 4.63233
2025-03-09 22:48:45,836 - INFO - Batch 80500 finished
2025-03-09 22:48:45,837 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:49:03,383 - INFO - Batch 80526, Running Avg Loss: 4.63231
2025-03-09 22:49:21,111 - INFO - Batch 80551, Running Avg Loss: 4.63230
2025-03-09 22:49:38,605 - INFO - Batch 80576, Running Avg Loss: 4.63228
2025-03-09 22:49:56,089 - INFO - Batch 80601, Running Avg Loss: 4.63222
2025-03-09 22:49:56,104 - INFO - Batch 80600 finished
2025-03-09 22:49:56,105 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:50:13,676 - INFO - Batch 80626, Running Avg Loss: 4.63219
2025-03-09 22:50:31,252 - INFO - Batch 80651, Running Avg Loss: 4.63217
2025-03-09 22:50:48,849 - INFO - Batch 80676, Running Avg Loss: 4.63214
2025-03-09 22:51:06,346 - INFO - Batch 80701, Running Avg Loss: 4.63209
2025-03-09 22:51:06,362 - INFO - Batch 80700 finished
2025-03-09 22:51:06,363 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:51:23,855 - INFO - Batch 80726, Running Avg Loss: 4.63208
2025-03-09 22:51:41,387 - INFO - Batch 80751, Running Avg Loss: 4.63204
2025-03-09 22:51:58,896 - INFO - Batch 80776, Running Avg Loss: 4.63201
2025-03-09 22:52:16,390 - INFO - Batch 80801, Running Avg Loss: 4.63198
2025-03-09 22:52:16,405 - INFO - Batch 80800 finished
2025-03-09 22:52:16,405 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:52:34,064 - INFO - Batch 80826, Running Avg Loss: 4.63196
2025-03-09 22:52:51,561 - INFO - Batch 80851, Running Avg Loss: 4.63196
2025-03-09 22:53:09,062 - INFO - Batch 80876, Running Avg Loss: 4.63194
2025-03-09 22:53:26,551 - INFO - Batch 80901, Running Avg Loss: 4.63190
2025-03-09 22:53:26,568 - INFO - Batch 80900 finished
2025-03-09 22:53:26,568 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:53:44,049 - INFO - Batch 80926, Running Avg Loss: 4.63185
2025-03-09 22:54:01,600 - INFO - Batch 80951, Running Avg Loss: 4.63182
2025-03-09 22:54:19,170 - INFO - Batch 80976, Running Avg Loss: 4.63181
2025-03-09 22:54:36,685 - INFO - Batch 81001, Running Avg Loss: 4.63180
2025-03-09 22:54:36,701 - INFO - Batch 81000 finished
2025-03-09 22:54:36,702 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:54:54,205 - INFO - Batch 81026, Running Avg Loss: 4.63177
2025-03-09 22:55:11,701 - INFO - Batch 81051, Running Avg Loss: 4.63173
2025-03-09 22:55:29,207 - INFO - Batch 81076, Running Avg Loss: 4.63172
2025-03-09 22:55:46,746 - INFO - Batch 81101, Running Avg Loss: 4.63171
2025-03-09 22:55:46,762 - INFO - Batch 81100 finished
2025-03-09 22:55:46,762 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:56:04,400 - INFO - Batch 81126, Running Avg Loss: 4.63169
2025-03-09 22:56:21,879 - INFO - Batch 81151, Running Avg Loss: 4.63166
2025-03-09 22:56:39,338 - INFO - Batch 81176, Running Avg Loss: 4.63166
2025-03-09 22:56:56,827 - INFO - Batch 81201, Running Avg Loss: 4.63165
2025-03-09 22:56:56,845 - INFO - Batch 81200 finished
2025-03-09 22:56:56,846 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:57:14,344 - INFO - Batch 81226, Running Avg Loss: 4.63164
2025-03-09 22:57:31,820 - INFO - Batch 81251, Running Avg Loss: 4.63164
2025-03-09 22:57:49,359 - INFO - Batch 81276, Running Avg Loss: 4.63160
2025-03-09 22:58:06,776 - INFO - Batch 81301, Running Avg Loss: 4.63157
2025-03-09 22:58:06,793 - INFO - Batch 81300 finished
2025-03-09 22:58:06,793 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:58:24,218 - INFO - Batch 81326, Running Avg Loss: 4.63153
2025-03-09 22:58:41,692 - INFO - Batch 81351, Running Avg Loss: 4.63151
2025-03-09 22:58:59,377 - INFO - Batch 81376, Running Avg Loss: 4.63148
2025-03-09 22:59:16,817 - INFO - Batch 81401, Running Avg Loss: 4.63144
2025-03-09 22:59:16,833 - INFO - Batch 81400 finished
2025-03-09 22:59:16,834 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 22:59:34,198 - INFO - Batch 81426, Running Avg Loss: 4.63143
2025-03-09 22:59:51,625 - INFO - Batch 81451, Running Avg Loss: 4.63144
2025-03-09 23:00:09,042 - INFO - Batch 81476, Running Avg Loss: 4.63142
2025-03-09 23:00:26,464 - INFO - Batch 81501, Running Avg Loss: 4.63141
2025-03-09 23:00:26,480 - INFO - Batch 81500 finished
2025-03-09 23:00:26,480 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:00:43,892 - INFO - Batch 81526, Running Avg Loss: 4.63140
2025-03-09 23:01:01,332 - INFO - Batch 81551, Running Avg Loss: 4.63137
2025-03-09 23:01:18,696 - INFO - Batch 81576, Running Avg Loss: 4.63135
2025-03-09 23:01:35,943 - INFO - Batch 81601, Running Avg Loss: 4.63131
2025-03-09 23:01:35,957 - INFO - Batch 81600 finished
2025-03-09 23:01:35,958 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:01:53,294 - INFO - Batch 81626, Running Avg Loss: 4.63128
2025-03-09 23:02:10,777 - INFO - Batch 81651, Running Avg Loss: 4.63128
2025-03-09 23:02:28,466 - INFO - Batch 81676, Running Avg Loss: 4.63127
2025-03-09 23:02:46,009 - INFO - Batch 81701, Running Avg Loss: 4.63125
2025-03-09 23:02:46,029 - INFO - Batch 81700 finished
2025-03-09 23:02:46,029 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:03:03,533 - INFO - Batch 81726, Running Avg Loss: 4.63124
2025-03-09 23:03:21,067 - INFO - Batch 81751, Running Avg Loss: 4.63121
2025-03-09 23:03:38,705 - INFO - Batch 81776, Running Avg Loss: 4.63118
2025-03-09 23:03:56,300 - INFO - Batch 81801, Running Avg Loss: 4.63119
2025-03-09 23:03:56,316 - INFO - Batch 81800 finished
2025-03-09 23:03:56,317 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:04:13,758 - INFO - Batch 81826, Running Avg Loss: 4.63116
2025-03-09 23:04:31,306 - INFO - Batch 81851, Running Avg Loss: 4.63116
2025-03-09 23:04:48,818 - INFO - Batch 81876, Running Avg Loss: 4.63112
2025-03-09 23:05:06,293 - INFO - Batch 81901, Running Avg Loss: 4.63111
2025-03-09 23:05:06,309 - INFO - Batch 81900 finished
2025-03-09 23:05:06,309 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:05:23,768 - INFO - Batch 81926, Running Avg Loss: 4.63109
2025-03-09 23:05:41,403 - INFO - Batch 81951, Running Avg Loss: 4.63108
2025-03-09 23:05:58,916 - INFO - Batch 81976, Running Avg Loss: 4.63104
2025-03-09 23:06:16,375 - INFO - Batch 82001, Running Avg Loss: 4.63102
2025-03-09 23:06:16,391 - INFO - Batch 82000 finished
2025-03-09 23:06:16,392 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:06:33,878 - INFO - Batch 82026, Running Avg Loss: 4.63101
2025-03-09 23:06:51,441 - INFO - Batch 82051, Running Avg Loss: 4.63099
2025-03-09 23:07:08,889 - INFO - Batch 82076, Running Avg Loss: 4.63101
2025-03-09 23:07:26,346 - INFO - Batch 82101, Running Avg Loss: 4.63097
2025-03-09 23:07:26,362 - INFO - Batch 82100 finished
2025-03-09 23:07:26,362 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:07:43,831 - INFO - Batch 82126, Running Avg Loss: 4.63098
2025-03-09 23:08:01,351 - INFO - Batch 82151, Running Avg Loss: 4.63097
2025-03-09 23:08:18,814 - INFO - Batch 82176, Running Avg Loss: 4.63095
2025-03-09 23:08:36,367 - INFO - Batch 82201, Running Avg Loss: 4.63094
2025-03-09 23:08:36,384 - INFO - Batch 82200 finished
2025-03-09 23:08:36,384 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:08:53,932 - INFO - Batch 82226, Running Avg Loss: 4.63093
2025-03-09 23:09:11,667 - INFO - Batch 82251, Running Avg Loss: 4.63089
2025-03-09 23:09:29,183 - INFO - Batch 82276, Running Avg Loss: 4.63087
2025-03-09 23:09:46,813 - INFO - Batch 82301, Running Avg Loss: 4.63086
2025-03-09 23:09:46,829 - INFO - Batch 82300 finished
2025-03-09 23:09:46,830 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:10:04,384 - INFO - Batch 82326, Running Avg Loss: 4.63085
2025-03-09 23:10:21,905 - INFO - Batch 82351, Running Avg Loss: 4.63083
2025-03-09 23:10:39,410 - INFO - Batch 82376, Running Avg Loss: 4.63081
2025-03-09 23:10:57,081 - INFO - Batch 82401, Running Avg Loss: 4.63082
2025-03-09 23:10:57,100 - INFO - Batch 82400 finished
2025-03-09 23:10:57,101 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:11:14,616 - INFO - Batch 82426, Running Avg Loss: 4.63082
2025-03-09 23:11:32,181 - INFO - Batch 82451, Running Avg Loss: 4.63080
2025-03-09 23:11:49,699 - INFO - Batch 82476, Running Avg Loss: 4.63077
2025-03-09 23:12:07,456 - INFO - Batch 82501, Running Avg Loss: 4.63076
2025-03-09 23:12:07,475 - INFO - 
GPU Memory Stats at step 82500:
2025-03-09 23:12:07,475 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 23:12:07,475 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 23:12:07,476 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 23:12:07,476 - INFO - learning rate: 0.00000003
2025-03-09 23:12:07,476 - INFO - Ep 1 (Step 082500): Avg loss 4.631 | 337924096 tokens seen
2025-03-09 23:12:07,476 - INFO - optimizer lr: 0.00000003
2025-03-09 23:12:07,476 - INFO - scheduler lr: 0.00000003
2025-03-09 23:12:07,476 - INFO - Selected prompt: Lobster, California spiny The California Spiny Lobster fishery is a small but locally 
2025-03-09 23:12:07,476 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:12:07,477 - INFO - random_topk: 9
2025-03-09 23:12:07,477 - INFO - random_temperature: 0.8089201821828798
2025-03-09 23:12:07,477 - INFO - global step 82500 , batch_idx 82500 => generating text
2025-03-09 23:12:07,477 - INFO - Generating on device cuda
2025-03-09 23:12:41,004 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:12:41,005 - INFO - Lobster, California spiny The California Spiny Lobster fishery is a small but locally 1990s that has been passed on its rich cultural heritage. This section will delve into the concept of creating a successful and sophisticated form of art, highlighting its historical roots, historical roots, and its impact on the world.

To begin, let us define some essential elements in modern art. A small, vibrant green, a small, and a small piece of art is characterized by its unique flavor. These elements serve as a tool for exploring various aspects of the region, including history, literature, and culture. As you explore this rich history, it becomes increasingly important to the complex world of art and science.

One of the most famous books that have played a crucial role in shaping the world of music. For instance, the "A Spir of the G," "A Spo," and "The Aest of the Gira" (1389-1999), in 1835. However, some artists have been introduced to the unique characteristics of this era, leading to the creation of their work.

In the context of the American Civil War, the United Nations Act (17501936), also known as the "The "The Al-A-Sar."
2025-03-09 23:12:41,005 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:13:09,147 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_82500_steps_avg_loss_4.63076_optimizer_lr_0.00000003.pth
2025-03-09 23:13:09,444 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 23:13:09,444 - INFO - Batch 82500 finished
2025-03-09 23:13:09,444 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:13:26,713 - INFO - Batch 82526, Running Avg Loss: 4.63073
2025-03-09 23:13:44,167 - INFO - Batch 82551, Running Avg Loss: 4.63073
2025-03-09 23:14:01,784 - INFO - Batch 82576, Running Avg Loss: 4.63070
2025-03-09 23:14:19,318 - INFO - Batch 82601, Running Avg Loss: 4.63068
2025-03-09 23:14:19,334 - INFO - Batch 82600 finished
2025-03-09 23:14:19,334 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:14:36,840 - INFO - Batch 82626, Running Avg Loss: 4.63066
2025-03-09 23:14:54,372 - INFO - Batch 82651, Running Avg Loss: 4.63065
2025-03-09 23:15:11,886 - INFO - Batch 82676, Running Avg Loss: 4.63059
2025-03-09 23:15:29,379 - INFO - Batch 82701, Running Avg Loss: 4.63059
2025-03-09 23:15:29,396 - INFO - Batch 82700 finished
2025-03-09 23:15:29,396 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:15:46,919 - INFO - Batch 82726, Running Avg Loss: 4.63058
2025-03-09 23:16:04,457 - INFO - Batch 82751, Running Avg Loss: 4.63057
2025-03-09 23:16:22,028 - INFO - Batch 82776, Running Avg Loss: 4.63056
2025-03-09 23:16:39,572 - INFO - Batch 82801, Running Avg Loss: 4.63051
2025-03-09 23:16:39,592 - INFO - Batch 82800 finished
2025-03-09 23:16:39,592 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:16:57,064 - INFO - Batch 82826, Running Avg Loss: 4.63049
2025-03-09 23:17:14,758 - INFO - Batch 82851, Running Avg Loss: 4.63048
2025-03-09 23:17:32,281 - INFO - Batch 82876, Running Avg Loss: 4.63045
2025-03-09 23:17:49,790 - INFO - Batch 82901, Running Avg Loss: 4.63041
2025-03-09 23:17:49,807 - INFO - Batch 82900 finished
2025-03-09 23:17:49,807 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:18:07,335 - INFO - Batch 82926, Running Avg Loss: 4.63035
2025-03-09 23:18:24,846 - INFO - Batch 82951, Running Avg Loss: 4.63033
2025-03-09 23:18:42,294 - INFO - Batch 82976, Running Avg Loss: 4.63030
2025-03-09 23:18:59,729 - INFO - Batch 83001, Running Avg Loss: 4.63026
2025-03-09 23:18:59,747 - INFO - Batch 83000 finished
2025-03-09 23:18:59,748 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:19:17,161 - INFO - Batch 83026, Running Avg Loss: 4.63021
2025-03-09 23:19:34,709 - INFO - Batch 83051, Running Avg Loss: 4.63017
2025-03-09 23:19:52,248 - INFO - Batch 83076, Running Avg Loss: 4.63012
2025-03-09 23:20:09,705 - INFO - Batch 83101, Running Avg Loss: 4.63009
2025-03-09 23:20:09,720 - INFO - Batch 83100 finished
2025-03-09 23:20:09,721 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:20:27,204 - INFO - Batch 83126, Running Avg Loss: 4.63009
2025-03-09 23:20:44,883 - INFO - Batch 83151, Running Avg Loss: 4.63004
2025-03-09 23:21:02,456 - INFO - Batch 83176, Running Avg Loss: 4.63003
2025-03-09 23:21:20,028 - INFO - Batch 83201, Running Avg Loss: 4.63003
2025-03-09 23:21:20,045 - INFO - Batch 83200 finished
2025-03-09 23:21:20,046 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:21:37,561 - INFO - Batch 83226, Running Avg Loss: 4.62999
2025-03-09 23:21:55,111 - INFO - Batch 83251, Running Avg Loss: 4.62995
2025-03-09 23:22:12,651 - INFO - Batch 83276, Running Avg Loss: 4.62995
2025-03-09 23:22:30,178 - INFO - Batch 83301, Running Avg Loss: 4.62992
2025-03-09 23:22:30,193 - INFO - Batch 83300 finished
2025-03-09 23:22:30,194 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:22:47,628 - INFO - Batch 83326, Running Avg Loss: 4.62989
2025-03-09 23:23:05,127 - INFO - Batch 83351, Running Avg Loss: 4.62986
2025-03-09 23:23:22,544 - INFO - Batch 83376, Running Avg Loss: 4.62984
2025-03-09 23:23:39,993 - INFO - Batch 83401, Running Avg Loss: 4.62982
2025-03-09 23:23:40,007 - INFO - Batch 83400 finished
2025-03-09 23:23:40,008 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:23:57,563 - INFO - Batch 83426, Running Avg Loss: 4.62984
2025-03-09 23:24:14,988 - INFO - Batch 83451, Running Avg Loss: 4.62981
2025-03-09 23:24:32,503 - INFO - Batch 83476, Running Avg Loss: 4.62979
2025-03-09 23:24:50,021 - INFO - Batch 83501, Running Avg Loss: 4.62978
2025-03-09 23:24:50,038 - INFO - Batch 83500 finished
2025-03-09 23:24:50,038 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:25:07,550 - INFO - Batch 83526, Running Avg Loss: 4.62978
2025-03-09 23:25:25,066 - INFO - Batch 83551, Running Avg Loss: 4.62975
2025-03-09 23:25:42,791 - INFO - Batch 83576, Running Avg Loss: 4.62973
2025-03-09 23:26:00,288 - INFO - Batch 83601, Running Avg Loss: 4.62973
2025-03-09 23:26:00,303 - INFO - Batch 83600 finished
2025-03-09 23:26:00,303 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:26:17,812 - INFO - Batch 83626, Running Avg Loss: 4.62971
2025-03-09 23:26:35,347 - INFO - Batch 83651, Running Avg Loss: 4.62969
2025-03-09 23:26:52,891 - INFO - Batch 83676, Running Avg Loss: 4.62967
2025-03-09 23:27:10,573 - INFO - Batch 83701, Running Avg Loss: 4.62964
2025-03-09 23:27:10,589 - INFO - Batch 83700 finished
2025-03-09 23:27:10,589 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:27:28,071 - INFO - Batch 83726, Running Avg Loss: 4.62961
2025-03-09 23:27:45,631 - INFO - Batch 83751, Running Avg Loss: 4.62956
2025-03-09 23:28:03,165 - INFO - Batch 83776, Running Avg Loss: 4.62956
2025-03-09 23:28:20,678 - INFO - Batch 83801, Running Avg Loss: 4.62954
2025-03-09 23:28:20,698 - INFO - Batch 83800 finished
2025-03-09 23:28:20,699 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:28:38,206 - INFO - Batch 83826, Running Avg Loss: 4.62949
2025-03-09 23:28:55,697 - INFO - Batch 83851, Running Avg Loss: 4.62946
2025-03-09 23:29:13,276 - INFO - Batch 83876, Running Avg Loss: 4.62946
2025-03-09 23:29:30,761 - INFO - Batch 83901, Running Avg Loss: 4.62946
2025-03-09 23:29:30,780 - INFO - Batch 83900 finished
2025-03-09 23:29:30,780 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:29:48,174 - INFO - Batch 83926, Running Avg Loss: 4.62943
2025-03-09 23:30:05,560 - INFO - Batch 83951, Running Avg Loss: 4.62943
2025-03-09 23:30:23,139 - INFO - Batch 83976, Running Avg Loss: 4.62942
2025-03-09 23:30:40,649 - INFO - Batch 84001, Running Avg Loss: 4.62942
2025-03-09 23:30:40,665 - INFO - Batch 84000 finished
2025-03-09 23:30:40,666 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:30:58,212 - INFO - Batch 84026, Running Avg Loss: 4.62941
2025-03-09 23:31:15,609 - INFO - Batch 84051, Running Avg Loss: 4.62937
2025-03-09 23:31:32,978 - INFO - Batch 84076, Running Avg Loss: 4.62933
2025-03-09 23:31:50,338 - INFO - Batch 84101, Running Avg Loss: 4.62931
2025-03-09 23:31:50,355 - INFO - Batch 84100 finished
2025-03-09 23:31:50,355 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:32:07,839 - INFO - Batch 84126, Running Avg Loss: 4.62927
2025-03-09 23:32:25,380 - INFO - Batch 84151, Running Avg Loss: 4.62926
2025-03-09 23:32:42,918 - INFO - Batch 84176, Running Avg Loss: 4.62922
2025-03-09 23:33:00,538 - INFO - Batch 84201, Running Avg Loss: 4.62919
2025-03-09 23:33:00,553 - INFO - Batch 84200 finished
2025-03-09 23:33:00,554 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:33:18,082 - INFO - Batch 84226, Running Avg Loss: 4.62917
2025-03-09 23:33:35,543 - INFO - Batch 84251, Running Avg Loss: 4.62913
2025-03-09 23:33:53,199 - INFO - Batch 84276, Running Avg Loss: 4.62913
2025-03-09 23:34:10,661 - INFO - Batch 84301, Running Avg Loss: 4.62912
2025-03-09 23:34:10,677 - INFO - Batch 84300 finished
2025-03-09 23:34:10,678 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:34:28,292 - INFO - Batch 84326, Running Avg Loss: 4.62913
2025-03-09 23:34:45,753 - INFO - Batch 84351, Running Avg Loss: 4.62911
2025-03-09 23:35:03,164 - INFO - Batch 84376, Running Avg Loss: 4.62912
2025-03-09 23:35:20,614 - INFO - Batch 84401, Running Avg Loss: 4.62910
2025-03-09 23:35:20,631 - INFO - Batch 84400 finished
2025-03-09 23:35:20,631 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:35:38,050 - INFO - Batch 84426, Running Avg Loss: 4.62908
2025-03-09 23:35:55,612 - INFO - Batch 84451, Running Avg Loss: 4.62907
2025-03-09 23:36:13,246 - INFO - Batch 84476, Running Avg Loss: 4.62907
2025-03-09 23:36:30,866 - INFO - Batch 84501, Running Avg Loss: 4.62908
2025-03-09 23:36:30,880 - INFO - Batch 84500 finished
2025-03-09 23:36:30,881 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:36:48,496 - INFO - Batch 84526, Running Avg Loss: 4.62905
2025-03-09 23:37:06,236 - INFO - Batch 84551, Running Avg Loss: 4.62903
2025-03-09 23:37:23,753 - INFO - Batch 84576, Running Avg Loss: 4.62902
2025-03-09 23:37:41,300 - INFO - Batch 84601, Running Avg Loss: 4.62898
2025-03-09 23:37:41,319 - INFO - Batch 84600 finished
2025-03-09 23:37:41,319 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:37:58,873 - INFO - Batch 84626, Running Avg Loss: 4.62896
2025-03-09 23:38:16,402 - INFO - Batch 84651, Running Avg Loss: 4.62894
2025-03-09 23:38:33,910 - INFO - Batch 84676, Running Avg Loss: 4.62892
2025-03-09 23:38:51,583 - INFO - Batch 84701, Running Avg Loss: 4.62888
2025-03-09 23:38:51,600 - INFO - Batch 84700 finished
2025-03-09 23:38:51,601 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:39:09,122 - INFO - Batch 84726, Running Avg Loss: 4.62886
2025-03-09 23:39:26,550 - INFO - Batch 84751, Running Avg Loss: 4.62886
2025-03-09 23:39:43,992 - INFO - Batch 84776, Running Avg Loss: 4.62882
2025-03-09 23:40:01,549 - INFO - Batch 84801, Running Avg Loss: 4.62880
2025-03-09 23:40:01,567 - INFO - Batch 84800 finished
2025-03-09 23:40:01,567 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:40:18,871 - INFO - Batch 84826, Running Avg Loss: 4.62877
2025-03-09 23:40:36,286 - INFO - Batch 84851, Running Avg Loss: 4.62875
2025-03-09 23:40:53,759 - INFO - Batch 84876, Running Avg Loss: 4.62872
2025-03-09 23:41:11,252 - INFO - Batch 84901, Running Avg Loss: 4.62871
2025-03-09 23:41:11,270 - INFO - Batch 84900 finished
2025-03-09 23:41:11,271 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:41:28,759 - INFO - Batch 84926, Running Avg Loss: 4.62869
2025-03-09 23:41:46,245 - INFO - Batch 84951, Running Avg Loss: 4.62867
2025-03-09 23:42:03,655 - INFO - Batch 84976, Running Avg Loss: 4.62861
2025-03-09 23:42:21,150 - INFO - Batch 85001, Running Avg Loss: 4.62860
2025-03-09 23:42:21,167 - INFO - 
GPU Memory Stats at step 85000:
2025-03-09 23:42:21,167 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-09 23:42:21,168 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-09 23:42:21,168 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-09 23:42:21,168 - INFO - learning rate: 0.00000004
2025-03-09 23:42:21,168 - INFO - Ep 1 (Step 085000): Avg loss 4.629 | 348164096 tokens seen
2025-03-09 23:42:21,168 - INFO - optimizer lr: 0.00000004
2025-03-09 23:42:21,168 - INFO - scheduler lr: 0.00000004
2025-03-09 23:42:21,168 - INFO - Selected prompt: Once upon a time, there was a friendly agency called Gaudette Insurance Agency, Inc. They help
2025-03-09 23:42:21,168 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:42:21,168 - INFO - random_topk: 3
2025-03-09 23:42:21,168 - INFO - random_temperature: 0.8758151072830533
2025-03-09 23:42:21,168 - INFO - global step 85000 , batch_idx 85000 => generating text
2025-03-09 23:42:21,168 - INFO - Generating on device cuda
2025-03-09 23:42:54,035 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:42:54,036 - INFO - Once upon a time, there was a friendly agency called Gaudette Insurance Agency, Inc. They help them understand how to make decisions and how to manage it. One day, while playing with a big, they stumbled upon a big box of friends. One sunny afternoon, while walking down the street, they noticed something strange happening. The friend was a wise friend, who was a big family and wanted to make sure they could help others. So, they decided to go on an adventure and wanted to learn about this new way.

As they approached, they stumbled across a special group called the "The Bosso," which made them feel like a big box full of colors. The kind woman named Lily explained that they could use the "the term 's,' 't' and 't'. They decided to make sure they could use it to make things better.

"Wow! I see," said Dr. Sarah, "Well, I think I can't know where it's okay. I can't need to make sure we're feeling happy and healthy!"

Max smiled and replied, "But why do we use our tools and techniques to help us learn more about our own lives?"

As they continued their journey, they discovered that sometimes, they can still have a special type of technology called "tir," which means that everyone can
2025-03-09 23:42:54,036 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:43:18,982 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_85000_steps_avg_loss_4.62860_optimizer_lr_0.00000004.pth
2025-03-09 23:43:19,229 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-09 23:43:19,229 - INFO - Batch 85000 finished
2025-03-09 23:43:19,229 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:43:36,514 - INFO - Batch 85026, Running Avg Loss: 4.62856
2025-03-09 23:43:54,025 - INFO - Batch 85051, Running Avg Loss: 4.62853
2025-03-09 23:44:11,537 - INFO - Batch 85076, Running Avg Loss: 4.62849
2025-03-09 23:44:28,997 - INFO - Batch 85101, Running Avg Loss: 4.62844
2025-03-09 23:44:29,012 - INFO - Batch 85100 finished
2025-03-09 23:44:29,012 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:44:46,485 - INFO - Batch 85126, Running Avg Loss: 4.62840
2025-03-09 23:45:04,154 - INFO - Batch 85151, Running Avg Loss: 4.62839
2025-03-09 23:45:21,583 - INFO - Batch 85176, Running Avg Loss: 4.62839
2025-03-09 23:45:39,006 - INFO - Batch 85201, Running Avg Loss: 4.62836
2025-03-09 23:45:39,020 - INFO - Batch 85200 finished
2025-03-09 23:45:39,021 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:45:56,461 - INFO - Batch 85226, Running Avg Loss: 4.62834
2025-03-09 23:46:14,069 - INFO - Batch 85251, Running Avg Loss: 4.62830
2025-03-09 23:46:31,628 - INFO - Batch 85276, Running Avg Loss: 4.62830
2025-03-09 23:46:49,209 - INFO - Batch 85301, Running Avg Loss: 4.62824
2025-03-09 23:46:49,226 - INFO - Batch 85300 finished
2025-03-09 23:46:49,226 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:47:06,702 - INFO - Batch 85326, Running Avg Loss: 4.62824
2025-03-09 23:47:24,128 - INFO - Batch 85351, Running Avg Loss: 4.62823
2025-03-09 23:47:41,601 - INFO - Batch 85376, Running Avg Loss: 4.62821
2025-03-09 23:47:59,183 - INFO - Batch 85401, Running Avg Loss: 4.62818
2025-03-09 23:47:59,198 - INFO - Batch 85400 finished
2025-03-09 23:47:59,199 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:48:16,737 - INFO - Batch 85426, Running Avg Loss: 4.62813
2025-03-09 23:48:34,190 - INFO - Batch 85451, Running Avg Loss: 4.62813
2025-03-09 23:48:51,598 - INFO - Batch 85476, Running Avg Loss: 4.62809
2025-03-09 23:49:08,996 - INFO - Batch 85501, Running Avg Loss: 4.62808
2025-03-09 23:49:09,012 - INFO - Batch 85500 finished
2025-03-09 23:49:09,012 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:49:26,457 - INFO - Batch 85526, Running Avg Loss: 4.62806
2025-03-09 23:49:44,161 - INFO - Batch 85551, Running Avg Loss: 4.62804
2025-03-09 23:50:01,722 - INFO - Batch 85576, Running Avg Loss: 4.62805
2025-03-09 23:50:19,154 - INFO - Batch 85601, Running Avg Loss: 4.62804
2025-03-09 23:50:19,169 - INFO - Batch 85600 finished
2025-03-09 23:50:19,170 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:50:36,528 - INFO - Batch 85626, Running Avg Loss: 4.62799
2025-03-09 23:50:53,972 - INFO - Batch 85651, Running Avg Loss: 4.62800
2025-03-09 23:51:11,544 - INFO - Batch 85676, Running Avg Loss: 4.62799
2025-03-09 23:51:29,087 - INFO - Batch 85701, Running Avg Loss: 4.62795
2025-03-09 23:51:29,104 - INFO - Batch 85700 finished
2025-03-09 23:51:29,105 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:51:46,585 - INFO - Batch 85726, Running Avg Loss: 4.62794
2025-03-09 23:52:04,040 - INFO - Batch 85751, Running Avg Loss: 4.62792
2025-03-09 23:52:21,577 - INFO - Batch 85776, Running Avg Loss: 4.62791
2025-03-09 23:52:39,098 - INFO - Batch 85801, Running Avg Loss: 4.62788
2025-03-09 23:52:39,116 - INFO - Batch 85800 finished
2025-03-09 23:52:39,117 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:52:56,748 - INFO - Batch 85826, Running Avg Loss: 4.62787
2025-03-09 23:53:14,195 - INFO - Batch 85851, Running Avg Loss: 4.62786
2025-03-09 23:53:31,631 - INFO - Batch 85876, Running Avg Loss: 4.62784
2025-03-09 23:53:49,116 - INFO - Batch 85901, Running Avg Loss: 4.62783
2025-03-09 23:53:49,137 - INFO - Batch 85900 finished
2025-03-09 23:53:49,137 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:54:06,673 - INFO - Batch 85926, Running Avg Loss: 4.62780
2025-03-09 23:54:24,202 - INFO - Batch 85951, Running Avg Loss: 4.62777
2025-03-09 23:54:41,662 - INFO - Batch 85976, Running Avg Loss: 4.62778
2025-03-09 23:54:59,165 - INFO - Batch 86001, Running Avg Loss: 4.62776
2025-03-09 23:54:59,185 - INFO - Batch 86000 finished
2025-03-09 23:54:59,185 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:55:16,697 - INFO - Batch 86026, Running Avg Loss: 4.62774
2025-03-09 23:55:34,238 - INFO - Batch 86051, Running Avg Loss: 4.62773
2025-03-09 23:55:51,798 - INFO - Batch 86076, Running Avg Loss: 4.62772
2025-03-09 23:56:09,242 - INFO - Batch 86101, Running Avg Loss: 4.62774
2025-03-09 23:56:09,258 - INFO - Batch 86100 finished
2025-03-09 23:56:09,258 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:56:26,966 - INFO - Batch 86126, Running Avg Loss: 4.62771
2025-03-09 23:56:44,479 - INFO - Batch 86151, Running Avg Loss: 4.62771
2025-03-09 23:57:02,147 - INFO - Batch 86176, Running Avg Loss: 4.62770
2025-03-09 23:57:19,662 - INFO - Batch 86201, Running Avg Loss: 4.62771
2025-03-09 23:57:19,677 - INFO - Batch 86200 finished
2025-03-09 23:57:19,678 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:57:37,163 - INFO - Batch 86226, Running Avg Loss: 4.62768
2025-03-09 23:57:54,641 - INFO - Batch 86251, Running Avg Loss: 4.62764
2025-03-09 23:58:12,386 - INFO - Batch 86276, Running Avg Loss: 4.62763
2025-03-09 23:58:29,944 - INFO - Batch 86301, Running Avg Loss: 4.62759
2025-03-09 23:58:29,962 - INFO - Batch 86300 finished
2025-03-09 23:58:29,963 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:58:47,351 - INFO - Batch 86326, Running Avg Loss: 4.62754
2025-03-09 23:59:04,797 - INFO - Batch 86351, Running Avg Loss: 4.62751
2025-03-09 23:59:22,387 - INFO - Batch 86376, Running Avg Loss: 4.62752
2025-03-09 23:59:39,793 - INFO - Batch 86401, Running Avg Loss: 4.62751
2025-03-09 23:59:39,808 - INFO - Batch 86400 finished
2025-03-09 23:59:39,808 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-09 23:59:57,303 - INFO - Batch 86426, Running Avg Loss: 4.62748
2025-03-10 00:00:14,860 - INFO - Batch 86451, Running Avg Loss: 4.62743
2025-03-10 00:00:32,349 - INFO - Batch 86476, Running Avg Loss: 4.62741
2025-03-10 00:00:49,795 - INFO - Batch 86501, Running Avg Loss: 4.62741
2025-03-10 00:00:49,811 - INFO - Batch 86500 finished
2025-03-10 00:00:49,811 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:01:07,271 - INFO - Batch 86526, Running Avg Loss: 4.62741
2025-03-10 00:01:24,756 - INFO - Batch 86551, Running Avg Loss: 4.62739
2025-03-10 00:01:42,076 - INFO - Batch 86576, Running Avg Loss: 4.62738
2025-03-10 00:01:59,422 - INFO - Batch 86601, Running Avg Loss: 4.62737
2025-03-10 00:01:59,442 - INFO - Batch 86600 finished
2025-03-10 00:01:59,443 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:02:16,802 - INFO - Batch 86626, Running Avg Loss: 4.62735
2025-03-10 00:02:34,258 - INFO - Batch 86651, Running Avg Loss: 4.62734
2025-03-10 00:02:51,875 - INFO - Batch 86676, Running Avg Loss: 4.62731
2025-03-10 00:03:09,318 - INFO - Batch 86701, Running Avg Loss: 4.62728
2025-03-10 00:03:09,334 - INFO - Batch 86700 finished
2025-03-10 00:03:09,335 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:03:26,722 - INFO - Batch 86726, Running Avg Loss: 4.62726
2025-03-10 00:03:44,156 - INFO - Batch 86751, Running Avg Loss: 4.62722
2025-03-10 00:04:01,654 - INFO - Batch 86776, Running Avg Loss: 4.62721
2025-03-10 00:04:19,303 - INFO - Batch 86801, Running Avg Loss: 4.62719
2025-03-10 00:04:19,320 - INFO - Batch 86800 finished
2025-03-10 00:04:19,321 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:04:36,860 - INFO - Batch 86826, Running Avg Loss: 4.62714
2025-03-10 00:04:54,362 - INFO - Batch 86851, Running Avg Loss: 4.62708
2025-03-10 00:05:11,831 - INFO - Batch 86876, Running Avg Loss: 4.62703
2025-03-10 00:05:29,257 - INFO - Batch 86901, Running Avg Loss: 4.62702
2025-03-10 00:05:29,273 - INFO - Batch 86900 finished
2025-03-10 00:05:29,274 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:05:46,773 - INFO - Batch 86926, Running Avg Loss: 4.62702
2025-03-10 00:06:04,452 - INFO - Batch 86951, Running Avg Loss: 4.62700
2025-03-10 00:06:21,996 - INFO - Batch 86976, Running Avg Loss: 4.62699
2025-03-10 00:06:39,499 - INFO - Batch 87001, Running Avg Loss: 4.62696
2025-03-10 00:06:39,513 - INFO - Batch 87000 finished
2025-03-10 00:06:39,514 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:06:56,909 - INFO - Batch 87026, Running Avg Loss: 4.62692
2025-03-10 00:07:14,337 - INFO - Batch 87051, Running Avg Loss: 4.62690
2025-03-10 00:07:31,888 - INFO - Batch 87076, Running Avg Loss: 4.62688
2025-03-10 00:07:49,486 - INFO - Batch 87101, Running Avg Loss: 4.62688
2025-03-10 00:07:49,501 - INFO - Batch 87100 finished
2025-03-10 00:07:49,501 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:08:07,055 - INFO - Batch 87126, Running Avg Loss: 4.62686
2025-03-10 00:08:24,597 - INFO - Batch 87151, Running Avg Loss: 4.62682
2025-03-10 00:08:42,173 - INFO - Batch 87176, Running Avg Loss: 4.62680
2025-03-10 00:08:59,754 - INFO - Batch 87201, Running Avg Loss: 4.62678
2025-03-10 00:08:59,770 - INFO - Batch 87200 finished
2025-03-10 00:08:59,770 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:09:17,361 - INFO - Batch 87226, Running Avg Loss: 4.62674
2025-03-10 00:09:35,047 - INFO - Batch 87251, Running Avg Loss: 4.62672
2025-03-10 00:09:52,507 - INFO - Batch 87276, Running Avg Loss: 4.62672
2025-03-10 00:10:10,052 - INFO - Batch 87301, Running Avg Loss: 4.62673
2025-03-10 00:10:10,066 - INFO - Batch 87300 finished
2025-03-10 00:10:10,067 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:10:27,586 - INFO - Batch 87326, Running Avg Loss: 4.62670
2025-03-10 00:10:45,031 - INFO - Batch 87351, Running Avg Loss: 4.62671
2025-03-10 00:11:02,450 - INFO - Batch 87376, Running Avg Loss: 4.62668
2025-03-10 00:11:19,985 - INFO - Batch 87401, Running Avg Loss: 4.62666
2025-03-10 00:11:20,002 - INFO - Batch 87400 finished
2025-03-10 00:11:20,003 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:11:37,413 - INFO - Batch 87426, Running Avg Loss: 4.62665
2025-03-10 00:11:54,876 - INFO - Batch 87451, Running Avg Loss: 4.62663
2025-03-10 00:12:12,359 - INFO - Batch 87476, Running Avg Loss: 4.62662
2025-03-10 00:12:30,032 - INFO - Batch 87501, Running Avg Loss: 4.62658
2025-03-10 00:12:30,051 - INFO - 
GPU Memory Stats at step 87500:
2025-03-10 00:12:30,052 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-10 00:12:30,052 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-10 00:12:30,052 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-10 00:12:30,052 - INFO - learning rate: 0.00000004
2025-03-10 00:12:30,052 - INFO - Ep 1 (Step 087500): Avg loss 4.627 | 358404096 tokens seen
2025-03-10 00:12:30,052 - INFO - optimizer lr: 0.00000004
2025-03-10 00:12:30,052 - INFO - scheduler lr: 0.00000004
2025-03-10 00:12:30,052 - INFO - Selected prompt: Introduction: The Art of Crafting Vegan Sandwich Delights Sandwiches occupy a unique space in
2025-03-10 00:12:30,052 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:12:30,053 - INFO - random_topk: 4
2025-03-10 00:12:30,053 - INFO - random_temperature: 0.8297183312826504
2025-03-10 00:12:30,053 - INFO - global step 87500 , batch_idx 87500 => generating text
2025-03-10 00:12:30,053 - INFO - Generating on device cuda
2025-03-10 00:13:03,322 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:13:03,323 - INFO - Introduction: The Art of Crafting Vegan Sandwich Delights Sandwiches occupy a unique space in the United States. This chapter will introduce you to the concept of "The Great Britain" in modern-day the United States. We will explore how these two-year-old artists and artists can better understand their history, their history, and the challenges they face. By the end, we aim to provide a deeper understanding of these topics and explore the historical context of the world around them.

Section 1: The Importance of the National Center
Before diving into the specifics of the American Civil War, it is essential first to understand the historical context and evolution of the United Kingdom. In the context of the United States and the 19th century, it was essential to recognize the significance of the American Civil War. In the 19th century, the United Nations Union became the first African American leader in the late 18th century. However, during the 18th century, the British Empire had a unique perspective on the country's history and the United States.

Section 2: The History of the American Civil War (1945-26551848), a German man named the 1940s, which had been a significant period in the 19th century.
2025-03-10 00:13:03,323 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:13:28,166 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_87500_steps_avg_loss_4.62658_optimizer_lr_0.00000004.pth
2025-03-10 00:13:28,443 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-10 00:13:28,443 - INFO - Batch 87500 finished
2025-03-10 00:13:28,443 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:13:45,658 - INFO - Batch 87526, Running Avg Loss: 4.62656
2025-03-10 00:14:02,992 - INFO - Batch 87551, Running Avg Loss: 4.62654
2025-03-10 00:14:20,640 - INFO - Batch 87576, Running Avg Loss: 4.62651
2025-03-10 00:14:38,204 - INFO - Batch 87601, Running Avg Loss: 4.62650
2025-03-10 00:14:38,221 - INFO - Batch 87600 finished
2025-03-10 00:14:38,222 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:14:55,661 - INFO - Batch 87626, Running Avg Loss: 4.62648
2025-03-10 00:15:13,108 - INFO - Batch 87651, Running Avg Loss: 4.62647
2025-03-10 00:15:30,584 - INFO - Batch 87676, Running Avg Loss: 4.62644
2025-03-10 00:15:48,187 - INFO - Batch 87701, Running Avg Loss: 4.62640
2025-03-10 00:15:48,203 - INFO - Batch 87700 finished
2025-03-10 00:15:48,204 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:16:05,717 - INFO - Batch 87726, Running Avg Loss: 4.62639
2025-03-10 00:16:23,243 - INFO - Batch 87751, Running Avg Loss: 4.62638
2025-03-10 00:16:40,698 - INFO - Batch 87776, Running Avg Loss: 4.62634
2025-03-10 00:16:58,237 - INFO - Batch 87801, Running Avg Loss: 4.62629
2025-03-10 00:16:58,253 - INFO - Batch 87800 finished
2025-03-10 00:16:58,254 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:17:15,792 - INFO - Batch 87826, Running Avg Loss: 4.62626
2025-03-10 00:17:33,450 - INFO - Batch 87851, Running Avg Loss: 4.62623
2025-03-10 00:17:50,932 - INFO - Batch 87876, Running Avg Loss: 4.62620
2025-03-10 00:18:08,420 - INFO - Batch 87901, Running Avg Loss: 4.62618
2025-03-10 00:18:08,436 - INFO - Batch 87900 finished
2025-03-10 00:18:08,437 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:18:26,016 - INFO - Batch 87926, Running Avg Loss: 4.62615
2025-03-10 00:18:43,552 - INFO - Batch 87951, Running Avg Loss: 4.62616
2025-03-10 00:19:01,020 - INFO - Batch 87976, Running Avg Loss: 4.62610
2025-03-10 00:19:18,610 - INFO - Batch 88001, Running Avg Loss: 4.62606
2025-03-10 00:19:18,626 - INFO - Batch 88000 finished
2025-03-10 00:19:18,627 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:19:36,088 - INFO - Batch 88026, Running Avg Loss: 4.62604
2025-03-10 00:19:53,642 - INFO - Batch 88051, Running Avg Loss: 4.62602
2025-03-10 00:20:11,170 - INFO - Batch 88076, Running Avg Loss: 4.62601
2025-03-10 00:20:28,709 - INFO - Batch 88101, Running Avg Loss: 4.62599
2025-03-10 00:20:28,723 - INFO - Batch 88100 finished
2025-03-10 00:20:28,724 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:20:46,129 - INFO - Batch 88126, Running Avg Loss: 4.62595
2025-03-10 00:21:03,774 - INFO - Batch 88151, Running Avg Loss: 4.62594
2025-03-10 00:21:21,342 - INFO - Batch 88176, Running Avg Loss: 4.62591
2025-03-10 00:21:38,885 - INFO - Batch 88201, Running Avg Loss: 4.62588
2025-03-10 00:21:38,900 - INFO - Batch 88200 finished
2025-03-10 00:21:38,901 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:21:56,400 - INFO - Batch 88226, Running Avg Loss: 4.62585
2025-03-10 00:22:13,940 - INFO - Batch 88251, Running Avg Loss: 4.62584
2025-03-10 00:22:31,557 - INFO - Batch 88276, Running Avg Loss: 4.62582
2025-03-10 00:22:49,117 - INFO - Batch 88301, Running Avg Loss: 4.62581
2025-03-10 00:22:49,133 - INFO - Batch 88300 finished
2025-03-10 00:22:49,134 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:23:06,601 - INFO - Batch 88326, Running Avg Loss: 4.62579
2025-03-10 00:23:24,114 - INFO - Batch 88351, Running Avg Loss: 4.62577
2025-03-10 00:23:41,732 - INFO - Batch 88376, Running Avg Loss: 4.62575
2025-03-10 00:23:59,372 - INFO - Batch 88401, Running Avg Loss: 4.62573
2025-03-10 00:23:59,389 - INFO - Batch 88400 finished
2025-03-10 00:23:59,389 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:24:17,103 - INFO - Batch 88426, Running Avg Loss: 4.62569
2025-03-10 00:24:34,727 - INFO - Batch 88451, Running Avg Loss: 4.62570
2025-03-10 00:24:52,343 - INFO - Batch 88476, Running Avg Loss: 4.62568
2025-03-10 00:25:09,934 - INFO - Batch 88501, Running Avg Loss: 4.62563
2025-03-10 00:25:09,948 - INFO - Batch 88500 finished
2025-03-10 00:25:09,948 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:25:27,415 - INFO - Batch 88526, Running Avg Loss: 4.62561
2025-03-10 00:25:44,975 - INFO - Batch 88551, Running Avg Loss: 4.62561
2025-03-10 00:26:02,629 - INFO - Batch 88576, Running Avg Loss: 4.62558
2025-03-10 00:26:20,119 - INFO - Batch 88601, Running Avg Loss: 4.62556
2025-03-10 00:26:20,134 - INFO - Batch 88600 finished
2025-03-10 00:26:20,135 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:26:37,597 - INFO - Batch 88626, Running Avg Loss: 4.62554
2025-03-10 00:26:55,042 - INFO - Batch 88651, Running Avg Loss: 4.62550
2025-03-10 00:27:12,543 - INFO - Batch 88676, Running Avg Loss: 4.62547
2025-03-10 00:27:30,188 - INFO - Batch 88701, Running Avg Loss: 4.62548
2025-03-10 00:27:30,203 - INFO - Batch 88700 finished
2025-03-10 00:27:30,203 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:27:47,642 - INFO - Batch 88726, Running Avg Loss: 4.62545
2025-03-10 00:28:05,128 - INFO - Batch 88751, Running Avg Loss: 4.62546
2025-03-10 00:28:22,594 - INFO - Batch 88776, Running Avg Loss: 4.62544
2025-03-10 00:28:40,083 - INFO - Batch 88801, Running Avg Loss: 4.62541
2025-03-10 00:28:40,105 - INFO - Batch 88800 finished
2025-03-10 00:28:40,105 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:28:57,583 - INFO - Batch 88826, Running Avg Loss: 4.62539
2025-03-10 00:29:15,077 - INFO - Batch 88851, Running Avg Loss: 4.62537
2025-03-10 00:29:32,573 - INFO - Batch 88876, Running Avg Loss: 4.62534
2025-03-10 00:29:50,023 - INFO - Batch 88901, Running Avg Loss: 4.62530
2025-03-10 00:29:50,041 - INFO - Batch 88900 finished
2025-03-10 00:29:50,042 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:30:07,553 - INFO - Batch 88926, Running Avg Loss: 4.62529
2025-03-10 00:30:25,108 - INFO - Batch 88951, Running Avg Loss: 4.62528
2025-03-10 00:30:42,717 - INFO - Batch 88976, Running Avg Loss: 4.62529
2025-03-10 00:31:00,160 - INFO - Batch 89001, Running Avg Loss: 4.62528
2025-03-10 00:31:00,176 - INFO - Batch 89000 finished
2025-03-10 00:31:00,176 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:31:17,554 - INFO - Batch 89026, Running Avg Loss: 4.62525
2025-03-10 00:31:35,016 - INFO - Batch 89051, Running Avg Loss: 4.62524
2025-03-10 00:31:52,531 - INFO - Batch 89076, Running Avg Loss: 4.62523
2025-03-10 00:32:10,018 - INFO - Batch 89101, Running Avg Loss: 4.62519
2025-03-10 00:32:10,034 - INFO - Batch 89100 finished
2025-03-10 00:32:10,035 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:32:27,499 - INFO - Batch 89126, Running Avg Loss: 4.62518
2025-03-10 00:32:44,924 - INFO - Batch 89151, Running Avg Loss: 4.62518
2025-03-10 00:33:02,394 - INFO - Batch 89176, Running Avg Loss: 4.62515
2025-03-10 00:33:19,826 - INFO - Batch 89201, Running Avg Loss: 4.62514
2025-03-10 00:33:19,845 - INFO - Batch 89200 finished
2025-03-10 00:33:19,846 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:33:37,278 - INFO - Batch 89226, Running Avg Loss: 4.62513
2025-03-10 00:33:54,724 - INFO - Batch 89251, Running Avg Loss: 4.62512
2025-03-10 00:34:12,366 - INFO - Batch 89276, Running Avg Loss: 4.62511
2025-03-10 00:34:29,877 - INFO - Batch 89301, Running Avg Loss: 4.62508
2025-03-10 00:34:29,897 - INFO - Batch 89300 finished
2025-03-10 00:34:29,898 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:34:47,363 - INFO - Batch 89326, Running Avg Loss: 4.62509
2025-03-10 00:35:04,805 - INFO - Batch 89351, Running Avg Loss: 4.62508
2025-03-10 00:35:22,275 - INFO - Batch 89376, Running Avg Loss: 4.62507
2025-03-10 00:35:39,749 - INFO - Batch 89401, Running Avg Loss: 4.62505
2025-03-10 00:35:39,764 - INFO - Batch 89400 finished
2025-03-10 00:35:39,765 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:35:57,204 - INFO - Batch 89426, Running Avg Loss: 4.62502
2025-03-10 00:36:14,690 - INFO - Batch 89451, Running Avg Loss: 4.62498
2025-03-10 00:36:32,188 - INFO - Batch 89476, Running Avg Loss: 4.62496
2025-03-10 00:36:49,600 - INFO - Batch 89501, Running Avg Loss: 4.62493
2025-03-10 00:36:49,620 - INFO - Batch 89500 finished
2025-03-10 00:36:49,621 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:37:06,966 - INFO - Batch 89526, Running Avg Loss: 4.62492
2025-03-10 00:37:24,487 - INFO - Batch 89551, Running Avg Loss: 4.62490
2025-03-10 00:37:41,917 - INFO - Batch 89576, Running Avg Loss: 4.62490
2025-03-10 00:37:59,406 - INFO - Batch 89601, Running Avg Loss: 4.62488
2025-03-10 00:37:59,425 - INFO - Batch 89600 finished
2025-03-10 00:37:59,426 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:38:16,885 - INFO - Batch 89626, Running Avg Loss: 4.62485
2025-03-10 00:38:34,398 - INFO - Batch 89651, Running Avg Loss: 4.62484
2025-03-10 00:38:51,922 - INFO - Batch 89676, Running Avg Loss: 4.62482
2025-03-10 00:39:09,595 - INFO - Batch 89701, Running Avg Loss: 4.62482
2025-03-10 00:39:09,612 - INFO - Batch 89700 finished
2025-03-10 00:39:09,612 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:39:27,026 - INFO - Batch 89726, Running Avg Loss: 4.62482
2025-03-10 00:39:44,473 - INFO - Batch 89751, Running Avg Loss: 4.62480
2025-03-10 00:40:01,957 - INFO - Batch 89776, Running Avg Loss: 4.62478
2025-03-10 00:40:19,519 - INFO - Batch 89801, Running Avg Loss: 4.62475
2025-03-10 00:40:19,705 - INFO - Batch 89800 finished
2025-03-10 00:40:19,706 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:40:37,162 - INFO - Batch 89826, Running Avg Loss: 4.62474
2025-03-10 00:40:54,359 - INFO - Batch 89851, Running Avg Loss: 4.62471
2025-03-10 00:41:11,544 - INFO - Batch 89876, Running Avg Loss: 4.62468
2025-03-10 00:41:29,046 - INFO - Batch 89901, Running Avg Loss: 4.62468
2025-03-10 00:41:29,061 - INFO - Batch 89900 finished
2025-03-10 00:41:29,061 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:41:46,557 - INFO - Batch 89926, Running Avg Loss: 4.62466
2025-03-10 00:42:03,986 - INFO - Batch 89951, Running Avg Loss: 4.62464
2025-03-10 00:42:21,453 - INFO - Batch 89976, Running Avg Loss: 4.62464
2025-03-10 00:42:38,953 - INFO - Batch 90001, Running Avg Loss: 4.62460
2025-03-10 00:42:38,968 - INFO - 
GPU Memory Stats at step 90000:
2025-03-10 00:42:38,969 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-10 00:42:38,969 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-10 00:42:38,969 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-10 00:42:38,969 - INFO - learning rate: 0.00000005
2025-03-10 00:42:38,969 - INFO - Ep 1 (Step 090000): Avg loss 4.625 | 368644096 tokens seen
2025-03-10 00:42:38,969 - INFO - optimizer lr: 0.00000005
2025-03-10 00:42:38,969 - INFO - scheduler lr: 0.00000005
2025-03-10 00:42:38,969 - INFO - Selected prompt: Feeling Alone Together: Exploring Alienation and Isolation in Literature
2025-03-10 00:42:38,970 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:42:38,971 - INFO - random_topk: 6
2025-03-10 00:42:38,971 - INFO - random_temperature: 0.7771619436421088
2025-03-10 00:42:38,971 - INFO - global step 90000 , batch_idx 90000 => generating text
2025-03-10 00:42:38,971 - INFO - Generating on device cuda
2025-03-10 00:43:12,790 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:43:12,791 - INFO - Feeling Alone Together: Exploring Alienation and Isolation in Literature

Introduction

The world of juvenile fiction has long been a powerful tool that combines elements of storytelling, literature, and art. This course unit will delve into the art of music, drawing connections between historical events and contemporary issues while connecting them with the power of the genre. We will explore how these elements can serve as a lens to shed light on the historical context of fiction and its impact on society. By examining the role of storytelling within the context of literature, we aim to equip future generations in the ways in which these works are used to convey complex ideas and insights.

Section 1: Historical Context of the History of the National Park

The origins of the American Civil War was the foundation of the modern American community, which took place as a symbol of the American Civil War. During the mid-1900s, the 18th century emerged during the late 19th century, which took place as a form of art and art. In response to the emergence of the Middle Ages, the term a' of the American and the United States" (1979-1650).

Section 2: The Power of the World

The concept of the American Civil War was the concept of
2025-03-10 00:43:12,791 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:43:38,122 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_90000_steps_avg_loss_4.62460_optimizer_lr_0.00000005.pth
2025-03-10 00:43:38,423 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-10 00:43:38,423 - INFO - Batch 90000 finished
2025-03-10 00:43:38,423 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:43:55,774 - INFO - Batch 90026, Running Avg Loss: 4.62459
2025-03-10 00:44:13,291 - INFO - Batch 90051, Running Avg Loss: 4.62458
2025-03-10 00:44:30,781 - INFO - Batch 90076, Running Avg Loss: 4.62454
2025-03-10 00:44:48,311 - INFO - Batch 90101, Running Avg Loss: 4.62454
2025-03-10 00:44:48,327 - INFO - Batch 90100 finished
2025-03-10 00:44:48,328 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:45:05,774 - INFO - Batch 90126, Running Avg Loss: 4.62455
2025-03-10 00:45:23,391 - INFO - Batch 90151, Running Avg Loss: 4.62453
2025-03-10 00:45:40,784 - INFO - Batch 90176, Running Avg Loss: 4.62452
2025-03-10 00:45:58,296 - INFO - Batch 90201, Running Avg Loss: 4.62449
2025-03-10 00:45:58,316 - INFO - Batch 90200 finished
2025-03-10 00:45:58,316 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:46:15,852 - INFO - Batch 90226, Running Avg Loss: 4.62448
2025-03-10 00:46:33,399 - INFO - Batch 90251, Running Avg Loss: 4.62445
2025-03-10 00:46:51,073 - INFO - Batch 90276, Running Avg Loss: 4.62444
2025-03-10 00:47:08,560 - INFO - Batch 90301, Running Avg Loss: 4.62440
2025-03-10 00:47:08,575 - INFO - Batch 90300 finished
2025-03-10 00:47:08,575 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:47:26,035 - INFO - Batch 90326, Running Avg Loss: 4.62436
2025-03-10 00:47:43,480 - INFO - Batch 90351, Running Avg Loss: 4.62434
2025-03-10 00:48:00,965 - INFO - Batch 90376, Running Avg Loss: 4.62431
2025-03-10 00:48:18,471 - INFO - Batch 90401, Running Avg Loss: 4.62432
2025-03-10 00:48:18,487 - INFO - Batch 90400 finished
2025-03-10 00:48:18,487 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:48:35,940 - INFO - Batch 90426, Running Avg Loss: 4.62428
2025-03-10 00:48:53,452 - INFO - Batch 90451, Running Avg Loss: 4.62425
2025-03-10 00:49:11,012 - INFO - Batch 90476, Running Avg Loss: 4.62421
2025-03-10 00:49:28,518 - INFO - Batch 90501, Running Avg Loss: 4.62419
2025-03-10 00:49:28,537 - INFO - Batch 90500 finished
2025-03-10 00:49:28,537 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:49:45,949 - INFO - Batch 90526, Running Avg Loss: 4.62416
2025-03-10 00:50:03,569 - INFO - Batch 90551, Running Avg Loss: 4.62414
2025-03-10 00:50:21,047 - INFO - Batch 90576, Running Avg Loss: 4.62410
2025-03-10 00:50:38,525 - INFO - Batch 90601, Running Avg Loss: 4.62407
2025-03-10 00:50:38,541 - INFO - Batch 90600 finished
2025-03-10 00:50:38,542 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:50:56,009 - INFO - Batch 90626, Running Avg Loss: 4.62403
2025-03-10 00:51:13,450 - INFO - Batch 90651, Running Avg Loss: 4.62400
2025-03-10 00:51:30,790 - INFO - Batch 90676, Running Avg Loss: 4.62397
2025-03-10 00:51:48,337 - INFO - Batch 90701, Running Avg Loss: 4.62394
2025-03-10 00:51:48,352 - INFO - Batch 90700 finished
2025-03-10 00:51:48,352 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:52:05,807 - INFO - Batch 90726, Running Avg Loss: 4.62391
2025-03-10 00:52:23,283 - INFO - Batch 90751, Running Avg Loss: 4.62386
2025-03-10 00:52:40,705 - INFO - Batch 90776, Running Avg Loss: 4.62383
2025-03-10 00:52:58,267 - INFO - Batch 90801, Running Avg Loss: 4.62382
2025-03-10 00:52:58,283 - INFO - Batch 90800 finished
2025-03-10 00:52:58,283 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:53:15,819 - INFO - Batch 90826, Running Avg Loss: 4.62379
2025-03-10 00:53:33,465 - INFO - Batch 90851, Running Avg Loss: 4.62379
2025-03-10 00:53:51,087 - INFO - Batch 90876, Running Avg Loss: 4.62377
2025-03-10 00:54:08,636 - INFO - Batch 90901, Running Avg Loss: 4.62375
2025-03-10 00:54:08,654 - INFO - Batch 90900 finished
2025-03-10 00:54:08,654 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:54:26,245 - INFO - Batch 90926, Running Avg Loss: 4.62373
2025-03-10 00:54:43,760 - INFO - Batch 90951, Running Avg Loss: 4.62370
2025-03-10 00:55:01,199 - INFO - Batch 90976, Running Avg Loss: 4.62369
2025-03-10 00:55:18,696 - INFO - Batch 91001, Running Avg Loss: 4.62367
2025-03-10 00:55:18,714 - INFO - Batch 91000 finished
2025-03-10 00:55:18,714 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:55:36,250 - INFO - Batch 91026, Running Avg Loss: 4.62365
2025-03-10 00:55:53,837 - INFO - Batch 91051, Running Avg Loss: 4.62363
2025-03-10 00:56:11,380 - INFO - Batch 91076, Running Avg Loss: 4.62363
2025-03-10 00:56:28,947 - INFO - Batch 91101, Running Avg Loss: 4.62360
2025-03-10 00:56:28,965 - INFO - Batch 91100 finished
2025-03-10 00:56:28,965 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:56:46,718 - INFO - Batch 91126, Running Avg Loss: 4.62359
2025-03-10 00:57:04,192 - INFO - Batch 91151, Running Avg Loss: 4.62357
2025-03-10 00:57:21,683 - INFO - Batch 91176, Running Avg Loss: 4.62353
2025-03-10 00:57:39,147 - INFO - Batch 91201, Running Avg Loss: 4.62352
2025-03-10 00:57:39,166 - INFO - Batch 91200 finished
2025-03-10 00:57:39,167 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:57:56,580 - INFO - Batch 91226, Running Avg Loss: 4.62347
2025-03-10 00:58:14,173 - INFO - Batch 91251, Running Avg Loss: 4.62347
2025-03-10 00:58:31,782 - INFO - Batch 91276, Running Avg Loss: 4.62343
2025-03-10 00:58:49,216 - INFO - Batch 91301, Running Avg Loss: 4.62339
2025-03-10 00:58:49,233 - INFO - Batch 91300 finished
2025-03-10 00:58:49,233 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 00:59:06,713 - INFO - Batch 91326, Running Avg Loss: 4.62338
2025-03-10 00:59:24,282 - INFO - Batch 91351, Running Avg Loss: 4.62334
2025-03-10 00:59:41,703 - INFO - Batch 91376, Running Avg Loss: 4.62332
2025-03-10 00:59:59,296 - INFO - Batch 91401, Running Avg Loss: 4.62332
2025-03-10 00:59:59,315 - INFO - Batch 91400 finished
2025-03-10 00:59:59,315 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:00:16,757 - INFO - Batch 91426, Running Avg Loss: 4.62328
2025-03-10 01:00:34,314 - INFO - Batch 91451, Running Avg Loss: 4.62326
2025-03-10 01:00:51,871 - INFO - Batch 91476, Running Avg Loss: 4.62324
2025-03-10 01:01:09,356 - INFO - Batch 91501, Running Avg Loss: 4.62324
2025-03-10 01:01:09,373 - INFO - Batch 91500 finished
2025-03-10 01:01:09,373 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:01:26,835 - INFO - Batch 91526, Running Avg Loss: 4.62321
2025-03-10 01:01:44,389 - INFO - Batch 91551, Running Avg Loss: 4.62318
2025-03-10 01:02:01,925 - INFO - Batch 91576, Running Avg Loss: 4.62318
2025-03-10 01:02:19,363 - INFO - Batch 91601, Running Avg Loss: 4.62316
2025-03-10 01:02:19,381 - INFO - Batch 91600 finished
2025-03-10 01:02:19,382 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:02:36,749 - INFO - Batch 91626, Running Avg Loss: 4.62312
2025-03-10 01:02:54,222 - INFO - Batch 91651, Running Avg Loss: 4.62311
2025-03-10 01:03:11,861 - INFO - Batch 91676, Running Avg Loss: 4.62309
2025-03-10 01:03:29,411 - INFO - Batch 91701, Running Avg Loss: 4.62309
2025-03-10 01:03:29,430 - INFO - Batch 91700 finished
2025-03-10 01:03:29,431 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:03:46,942 - INFO - Batch 91726, Running Avg Loss: 4.62308
2025-03-10 01:04:04,409 - INFO - Batch 91751, Running Avg Loss: 4.62306
2025-03-10 01:04:21,860 - INFO - Batch 91776, Running Avg Loss: 4.62304
2025-03-10 01:04:39,393 - INFO - Batch 91801, Running Avg Loss: 4.62303
2025-03-10 01:04:39,408 - INFO - Batch 91800 finished
2025-03-10 01:04:39,409 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:04:56,993 - INFO - Batch 91826, Running Avg Loss: 4.62300
2025-03-10 01:05:14,488 - INFO - Batch 91851, Running Avg Loss: 4.62297
2025-03-10 01:05:31,883 - INFO - Batch 91876, Running Avg Loss: 4.62295
2025-03-10 01:05:49,435 - INFO - Batch 91901, Running Avg Loss: 4.62293
2025-03-10 01:05:49,451 - INFO - Batch 91900 finished
2025-03-10 01:05:49,452 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:06:07,025 - INFO - Batch 91926, Running Avg Loss: 4.62292
2025-03-10 01:06:24,516 - INFO - Batch 91951, Running Avg Loss: 4.62289
2025-03-10 01:06:42,277 - INFO - Batch 91976, Running Avg Loss: 4.62288
2025-03-10 01:06:59,798 - INFO - Batch 92001, Running Avg Loss: 4.62288
2025-03-10 01:06:59,814 - INFO - Batch 92000 finished
2025-03-10 01:06:59,814 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:07:17,286 - INFO - Batch 92026, Running Avg Loss: 4.62284
2025-03-10 01:07:34,750 - INFO - Batch 92051, Running Avg Loss: 4.62283
2025-03-10 01:07:52,264 - INFO - Batch 92076, Running Avg Loss: 4.62282
2025-03-10 01:08:09,749 - INFO - Batch 92101, Running Avg Loss: 4.62281
2025-03-10 01:08:09,767 - INFO - Batch 92100 finished
2025-03-10 01:08:09,767 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:08:27,118 - INFO - Batch 92126, Running Avg Loss: 4.62279
2025-03-10 01:08:44,422 - INFO - Batch 92151, Running Avg Loss: 4.62276
2025-03-10 01:09:01,732 - INFO - Batch 92176, Running Avg Loss: 4.62272
2025-03-10 01:09:19,080 - INFO - Batch 92201, Running Avg Loss: 4.62271
2025-03-10 01:09:19,096 - INFO - Batch 92200 finished
2025-03-10 01:09:19,096 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:09:36,585 - INFO - Batch 92226, Running Avg Loss: 4.62266
2025-03-10 01:09:54,240 - INFO - Batch 92251, Running Avg Loss: 4.62263
2025-03-10 01:10:11,650 - INFO - Batch 92276, Running Avg Loss: 4.62260
2025-03-10 01:10:29,030 - INFO - Batch 92301, Running Avg Loss: 4.62256
2025-03-10 01:10:29,046 - INFO - Batch 92300 finished
2025-03-10 01:10:29,047 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:10:46,458 - INFO - Batch 92326, Running Avg Loss: 4.62255
2025-03-10 01:11:03,987 - INFO - Batch 92351, Running Avg Loss: 4.62251
2025-03-10 01:11:21,495 - INFO - Batch 92376, Running Avg Loss: 4.62250
2025-03-10 01:11:39,214 - INFO - Batch 92401, Running Avg Loss: 4.62247
2025-03-10 01:11:39,228 - INFO - Batch 92400 finished
2025-03-10 01:11:39,229 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:11:56,728 - INFO - Batch 92426, Running Avg Loss: 4.62244
2025-03-10 01:12:14,182 - INFO - Batch 92451, Running Avg Loss: 4.62245
2025-03-10 01:12:31,737 - INFO - Batch 92476, Running Avg Loss: 4.62242
2025-03-10 01:12:49,413 - INFO - Batch 92501, Running Avg Loss: 4.62240
2025-03-10 01:12:49,431 - INFO - 
GPU Memory Stats at step 92500:
2025-03-10 01:12:49,431 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-10 01:12:49,432 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-10 01:12:49,432 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-10 01:12:49,432 - INFO - learning rate: 0.00000006
2025-03-10 01:12:49,432 - INFO - Ep 1 (Step 092500): Avg loss 4.622 | 378884096 tokens seen
2025-03-10 01:12:49,432 - INFO - optimizer lr: 0.00000006
2025-03-10 01:12:49,432 - INFO - scheduler lr: 0.00000006
2025-03-10 01:12:49,432 - INFO - Selected prompt: In today's ever-evolving world, technology has become an integral part of our lives
2025-03-10 01:12:49,432 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:12:49,432 - INFO - random_topk: 4
2025-03-10 01:12:49,433 - INFO - random_temperature: 0.7911158390114073
2025-03-10 01:12:49,433 - INFO - global step 92500 , batch_idx 92500 => generating text
2025-03-10 01:12:49,433 - INFO - Generating on device cuda
2025-03-10 01:13:23,032 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:13:23,033 - INFO - In today's ever-evolving world, technology has become an integral part of our lives. From smartphones to software, there are many different types of software to create new and efficient and enjoyable, so we can create a more effective environment for all students. One such area is the "Aest" and "Soh," which is a way to create something new and effective. This chapter will delve into the world of computer science, specifically exploring its significance, applications, and applications. We will also discuss the importance of this technology and learn how to create a successful business environment.

To begin, let us define what we mean. At its core, the "A" refers to the use of the data we use to create new and complex systems. These are a way to create a more efficient and effective communication between different countries and cultures. By examining the principles and techniques used by organizations like the United States, we can gain insights into how people can contribute their unique needs and opportunities.

One important aspect of the development of a business is understanding and addressing the unique needs of the world around us. A well-known example is the United States and the United States of the United States. The term The National Institute of the 5019" is an example of how a company has been used by the United States to create a more inclusive
2025-03-10 01:13:23,033 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:13:47,265 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_92500_steps_avg_loss_4.62240_optimizer_lr_0.00000006.pth
2025-03-10 01:13:47,511 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-10 01:13:47,511 - INFO - Batch 92500 finished
2025-03-10 01:13:47,511 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:14:04,655 - INFO - Batch 92526, Running Avg Loss: 4.62239
2025-03-10 01:14:21,926 - INFO - Batch 92551, Running Avg Loss: 4.62238
2025-03-10 01:14:39,479 - INFO - Batch 92576, Running Avg Loss: 4.62236
2025-03-10 01:14:56,869 - INFO - Batch 92601, Running Avg Loss: 4.62234
2025-03-10 01:14:56,884 - INFO - Batch 92600 finished
2025-03-10 01:14:56,885 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:15:14,362 - INFO - Batch 92626, Running Avg Loss: 4.62232
2025-03-10 01:15:31,919 - INFO - Batch 92651, Running Avg Loss: 4.62233
2025-03-10 01:15:49,476 - INFO - Batch 92676, Running Avg Loss: 4.62232
2025-03-10 01:16:06,896 - INFO - Batch 92701, Running Avg Loss: 4.62230
2025-03-10 01:16:06,913 - INFO - Batch 92700 finished
2025-03-10 01:16:06,913 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:16:24,241 - INFO - Batch 92726, Running Avg Loss: 4.62230
2025-03-10 01:16:41,676 - INFO - Batch 92751, Running Avg Loss: 4.62229
2025-03-10 01:16:59,143 - INFO - Batch 92776, Running Avg Loss: 4.62229
2025-03-10 01:17:16,704 - INFO - Batch 92801, Running Avg Loss: 4.62225
2025-03-10 01:17:16,720 - INFO - Batch 92800 finished
2025-03-10 01:17:16,721 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:17:34,229 - INFO - Batch 92826, Running Avg Loss: 4.62221
2025-03-10 01:17:51,785 - INFO - Batch 92851, Running Avg Loss: 4.62221
2025-03-10 01:18:09,135 - INFO - Batch 92876, Running Avg Loss: 4.62220
2025-03-10 01:18:26,556 - INFO - Batch 92901, Running Avg Loss: 4.62217
2025-03-10 01:18:26,571 - INFO - Batch 92900 finished
2025-03-10 01:18:26,571 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:18:44,019 - INFO - Batch 92926, Running Avg Loss: 4.62216
2025-03-10 01:19:01,565 - INFO - Batch 92951, Running Avg Loss: 4.62211
2025-03-10 01:19:19,063 - INFO - Batch 92976, Running Avg Loss: 4.62209
2025-03-10 01:19:36,601 - INFO - Batch 93001, Running Avg Loss: 4.62209
2025-03-10 01:19:36,618 - INFO - Batch 93000 finished
2025-03-10 01:19:36,618 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:19:54,075 - INFO - Batch 93026, Running Avg Loss: 4.62206
2025-03-10 01:20:11,600 - INFO - Batch 93051, Running Avg Loss: 4.62205
2025-03-10 01:20:29,081 - INFO - Batch 93076, Running Avg Loss: 4.62204
2025-03-10 01:20:46,568 - INFO - Batch 93101, Running Avg Loss: 4.62203
2025-03-10 01:20:46,584 - INFO - Batch 93100 finished
2025-03-10 01:20:46,584 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:21:03,884 - INFO - Batch 93126, Running Avg Loss: 4.62200
2025-03-10 01:21:21,387 - INFO - Batch 93151, Running Avg Loss: 4.62199
2025-03-10 01:21:38,761 - INFO - Batch 93176, Running Avg Loss: 4.62198
2025-03-10 01:21:56,246 - INFO - Batch 93201, Running Avg Loss: 4.62198
2025-03-10 01:21:56,261 - INFO - Batch 93200 finished
2025-03-10 01:21:56,262 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:22:13,661 - INFO - Batch 93226, Running Avg Loss: 4.62194
2025-03-10 01:22:31,106 - INFO - Batch 93251, Running Avg Loss: 4.62193
2025-03-10 01:22:48,630 - INFO - Batch 93276, Running Avg Loss: 4.62192
2025-03-10 01:23:06,153 - INFO - Batch 93301, Running Avg Loss: 4.62189
2025-03-10 01:23:06,170 - INFO - Batch 93300 finished
2025-03-10 01:23:06,170 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:23:23,637 - INFO - Batch 93326, Running Avg Loss: 4.62185
2025-03-10 01:23:41,112 - INFO - Batch 93351, Running Avg Loss: 4.62184
2025-03-10 01:23:58,557 - INFO - Batch 93376, Running Avg Loss: 4.62182
2025-03-10 01:24:16,008 - INFO - Batch 93401, Running Avg Loss: 4.62179
2025-03-10 01:24:16,024 - INFO - Batch 93400 finished
2025-03-10 01:24:16,024 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:24:33,594 - INFO - Batch 93426, Running Avg Loss: 4.62177
2025-03-10 01:24:51,118 - INFO - Batch 93451, Running Avg Loss: 4.62174
2025-03-10 01:25:08,694 - INFO - Batch 93476, Running Avg Loss: 4.62171
2025-03-10 01:25:26,148 - INFO - Batch 93501, Running Avg Loss: 4.62168
2025-03-10 01:25:26,164 - INFO - Batch 93500 finished
2025-03-10 01:25:26,164 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:25:43,581 - INFO - Batch 93526, Running Avg Loss: 4.62164
2025-03-10 01:26:01,011 - INFO - Batch 93551, Running Avg Loss: 4.62165
2025-03-10 01:26:18,510 - INFO - Batch 93576, Running Avg Loss: 4.62162
2025-03-10 01:26:36,188 - INFO - Batch 93601, Running Avg Loss: 4.62159
2025-03-10 01:26:36,205 - INFO - Batch 93600 finished
2025-03-10 01:26:36,206 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:26:53,727 - INFO - Batch 93626, Running Avg Loss: 4.62156
2025-03-10 01:27:11,127 - INFO - Batch 93651, Running Avg Loss: 4.62156
2025-03-10 01:27:28,576 - INFO - Batch 93676, Running Avg Loss: 4.62152
2025-03-10 01:27:46,124 - INFO - Batch 93701, Running Avg Loss: 4.62150
2025-03-10 01:27:46,143 - INFO - Batch 93700 finished
2025-03-10 01:27:46,143 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:28:03,549 - INFO - Batch 93726, Running Avg Loss: 4.62148
2025-03-10 01:28:21,081 - INFO - Batch 93751, Running Avg Loss: 4.62147
2025-03-10 01:28:38,543 - INFO - Batch 93776, Running Avg Loss: 4.62146
2025-03-10 01:28:55,955 - INFO - Batch 93801, Running Avg Loss: 4.62146
2025-03-10 01:28:55,972 - INFO - Batch 93800 finished
2025-03-10 01:28:55,973 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:29:13,382 - INFO - Batch 93826, Running Avg Loss: 4.62144
2025-03-10 01:29:30,909 - INFO - Batch 93851, Running Avg Loss: 4.62142
2025-03-10 01:29:48,362 - INFO - Batch 93876, Running Avg Loss: 4.62140
2025-03-10 01:30:05,678 - INFO - Batch 93901, Running Avg Loss: 4.62139
2025-03-10 01:30:05,696 - INFO - Batch 93900 finished
2025-03-10 01:30:05,696 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:30:23,012 - INFO - Batch 93926, Running Avg Loss: 4.62138
2025-03-10 01:30:40,415 - INFO - Batch 93951, Running Avg Loss: 4.62137
2025-03-10 01:30:58,001 - INFO - Batch 93976, Running Avg Loss: 4.62136
2025-03-10 01:31:15,299 - INFO - Batch 94001, Running Avg Loss: 4.62131
2025-03-10 01:31:15,312 - INFO - Batch 94000 finished
2025-03-10 01:31:15,312 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:31:32,368 - INFO - Batch 94026, Running Avg Loss: 4.62129
2025-03-10 01:31:49,499 - INFO - Batch 94051, Running Avg Loss: 4.62128
2025-03-10 01:32:07,036 - INFO - Batch 94076, Running Avg Loss: 4.62127
2025-03-10 01:32:24,227 - INFO - Batch 94101, Running Avg Loss: 4.62124
2025-03-10 01:32:24,240 - INFO - Batch 94100 finished
2025-03-10 01:32:24,240 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:32:41,338 - INFO - Batch 94126, Running Avg Loss: 4.62123
2025-03-10 01:32:58,385 - INFO - Batch 94151, Running Avg Loss: 4.62123
2025-03-10 01:33:15,460 - INFO - Batch 94176, Running Avg Loss: 4.62120
2025-03-10 01:33:32,560 - INFO - Batch 94201, Running Avg Loss: 4.62118
2025-03-10 01:33:32,572 - INFO - Batch 94200 finished
2025-03-10 01:33:32,572 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:33:49,754 - INFO - Batch 94226, Running Avg Loss: 4.62115
2025-03-10 01:34:06,952 - INFO - Batch 94251, Running Avg Loss: 4.62113
2025-03-10 01:34:24,216 - INFO - Batch 94276, Running Avg Loss: 4.62111
2025-03-10 01:34:41,345 - INFO - Batch 94301, Running Avg Loss: 4.62108
2025-03-10 01:34:41,358 - INFO - Batch 94300 finished
2025-03-10 01:34:41,358 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:34:58,540 - INFO - Batch 94326, Running Avg Loss: 4.62104
2025-03-10 01:35:15,762 - INFO - Batch 94351, Running Avg Loss: 4.62102
2025-03-10 01:35:32,988 - INFO - Batch 94376, Running Avg Loss: 4.62100
2025-03-10 01:35:50,302 - INFO - Batch 94401, Running Avg Loss: 4.62099
2025-03-10 01:35:50,319 - INFO - Batch 94400 finished
2025-03-10 01:35:50,320 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:36:07,756 - INFO - Batch 94426, Running Avg Loss: 4.62095
2025-03-10 01:36:25,187 - INFO - Batch 94451, Running Avg Loss: 4.62094
2025-03-10 01:36:42,552 - INFO - Batch 94476, Running Avg Loss: 4.62091
2025-03-10 01:36:59,967 - INFO - Batch 94501, Running Avg Loss: 4.62088
2025-03-10 01:36:59,986 - INFO - Batch 94500 finished
2025-03-10 01:36:59,987 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:37:17,548 - INFO - Batch 94526, Running Avg Loss: 4.62083
2025-03-10 01:37:35,225 - INFO - Batch 94551, Running Avg Loss: 4.62081
2025-03-10 01:37:52,749 - INFO - Batch 94576, Running Avg Loss: 4.62077
2025-03-10 01:38:10,204 - INFO - Batch 94601, Running Avg Loss: 4.62076
2025-03-10 01:38:10,220 - INFO - Batch 94600 finished
2025-03-10 01:38:10,220 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:38:27,633 - INFO - Batch 94626, Running Avg Loss: 4.62073
2025-03-10 01:38:45,087 - INFO - Batch 94651, Running Avg Loss: 4.62073
2025-03-10 01:39:02,575 - INFO - Batch 94676, Running Avg Loss: 4.62072
2025-03-10 01:39:20,111 - INFO - Batch 94701, Running Avg Loss: 4.62070
2025-03-10 01:39:20,128 - INFO - Batch 94700 finished
2025-03-10 01:39:20,128 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:39:37,547 - INFO - Batch 94726, Running Avg Loss: 4.62068
2025-03-10 01:39:54,983 - INFO - Batch 94751, Running Avg Loss: 4.62064
2025-03-10 01:40:12,406 - INFO - Batch 94776, Running Avg Loss: 4.62062
2025-03-10 01:40:29,844 - INFO - Batch 94801, Running Avg Loss: 4.62058
2025-03-10 01:40:29,859 - INFO - Batch 94800 finished
2025-03-10 01:40:29,859 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:40:47,420 - INFO - Batch 94826, Running Avg Loss: 4.62057
2025-03-10 01:41:04,588 - INFO - Batch 94851, Running Avg Loss: 4.62054
2025-03-10 01:41:21,953 - INFO - Batch 94876, Running Avg Loss: 4.62052
2025-03-10 01:41:39,445 - INFO - Batch 94901, Running Avg Loss: 4.62049
2025-03-10 01:41:39,460 - INFO - Batch 94900 finished
2025-03-10 01:41:39,461 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:41:56,969 - INFO - Batch 94926, Running Avg Loss: 4.62049
2025-03-10 01:42:14,448 - INFO - Batch 94951, Running Avg Loss: 4.62048
2025-03-10 01:42:31,985 - INFO - Batch 94976, Running Avg Loss: 4.62044
2025-03-10 01:42:49,582 - INFO - Batch 95001, Running Avg Loss: 4.62042
2025-03-10 01:42:49,600 - INFO - 
GPU Memory Stats at step 95000:
2025-03-10 01:42:49,601 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-10 01:42:49,601 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-10 01:42:49,601 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-10 01:42:49,601 - INFO - learning rate: 0.00000006
2025-03-10 01:42:49,601 - INFO - Ep 1 (Step 095000): Avg loss 4.620 | 389124096 tokens seen
2025-03-10 01:42:49,601 - INFO - optimizer lr: 0.00000006
2025-03-10 01:42:49,601 - INFO - scheduler lr: 0.00000006
2025-03-10 01:42:49,602 - INFO - Selected prompt: A couple of years ago, I was working as an extra on the set of a low-budget British film.
2025-03-10 01:42:49,602 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:42:49,603 - INFO - random_topk: 3
2025-03-10 01:42:49,603 - INFO - random_temperature: 0.7706981537071914
2025-03-10 01:42:49,603 - INFO - global step 95000 , batch_idx 95000 => generating text
2025-03-10 01:42:49,603 - INFO - Generating on device cuda
2025-03-10 01:43:23,352 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:43:23,353 - INFO - A couple of years ago, I was working as an extra on the set of a low-budget British film. It seemed like a simple yet intriguing piece of paper and a unique blend of colors and colors, but it wasn't until one day I stumbled upon a journey to learn about the history of the art of the art of art.

As we delved deeper into the world of the world, we started by a journey to explore the historical significance of the history, the history of the region. As we explore the history of the Middle East, we started to understand the historical context behind this vibrant tapestry. We will also discovered how the historical context and the historical context of the Middle Eastern.

One of the most influential figures in this era is the "The Great" by the "The Gira," a young girl named Lily who lived during the 19th century. She was known for his work, and the first five-year-old girl, a young woman named Sarah. She was born in 1864, and the British Union had a profound impact on the region's history and the world.

As the first day of the late 1900s, the American Civil War had been fascinated by the early 19th century. This time, the British Empire, had to take the beginning of the late 19
2025-03-10 01:43:23,353 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:43:48,111 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_95000_steps_avg_loss_4.62042_optimizer_lr_0.00000006.pth
2025-03-10 01:43:48,444 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-10 01:43:48,444 - INFO - Batch 95000 finished
2025-03-10 01:43:48,444 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:44:05,889 - INFO - Batch 95026, Running Avg Loss: 4.62040
2025-03-10 01:44:23,472 - INFO - Batch 95051, Running Avg Loss: 4.62037
2025-03-10 01:44:41,043 - INFO - Batch 95076, Running Avg Loss: 4.62034
2025-03-10 01:44:58,565 - INFO - Batch 95101, Running Avg Loss: 4.62031
2025-03-10 01:44:58,581 - INFO - Batch 95100 finished
2025-03-10 01:44:58,581 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:45:16,261 - INFO - Batch 95126, Running Avg Loss: 4.62029
2025-03-10 01:45:33,864 - INFO - Batch 95151, Running Avg Loss: 4.62025
2025-03-10 01:45:51,453 - INFO - Batch 95176, Running Avg Loss: 4.62021
2025-03-10 01:46:08,891 - INFO - Batch 95201, Running Avg Loss: 4.62019
2025-03-10 01:46:08,906 - INFO - Batch 95200 finished
2025-03-10 01:46:08,906 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:46:26,338 - INFO - Batch 95226, Running Avg Loss: 4.62017
2025-03-10 01:46:43,806 - INFO - Batch 95251, Running Avg Loss: 4.62014
2025-03-10 01:47:01,317 - INFO - Batch 95276, Running Avg Loss: 4.62012
2025-03-10 01:47:18,837 - INFO - Batch 95301, Running Avg Loss: 4.62010
2025-03-10 01:47:18,852 - INFO - Batch 95300 finished
2025-03-10 01:47:18,853 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:47:36,341 - INFO - Batch 95326, Running Avg Loss: 4.62008
2025-03-10 01:47:53,845 - INFO - Batch 95351, Running Avg Loss: 4.62007
2025-03-10 01:48:11,302 - INFO - Batch 95376, Running Avg Loss: 4.62005
2025-03-10 01:48:28,922 - INFO - Batch 95401, Running Avg Loss: 4.62003
2025-03-10 01:48:28,937 - INFO - Batch 95400 finished
2025-03-10 01:48:28,938 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:48:46,430 - INFO - Batch 95426, Running Avg Loss: 4.61999
2025-03-10 01:49:03,910 - INFO - Batch 95451, Running Avg Loss: 4.61997
2025-03-10 01:49:21,356 - INFO - Batch 95476, Running Avg Loss: 4.61995
2025-03-10 01:49:38,874 - INFO - Batch 95501, Running Avg Loss: 4.61993
2025-03-10 01:49:38,890 - INFO - Batch 95500 finished
2025-03-10 01:49:38,891 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:49:56,476 - INFO - Batch 95526, Running Avg Loss: 4.61991
2025-03-10 01:50:14,059 - INFO - Batch 95551, Running Avg Loss: 4.61988
2025-03-10 01:50:31,368 - INFO - Batch 95576, Running Avg Loss: 4.61985
2025-03-10 01:50:48,564 - INFO - Batch 95601, Running Avg Loss: 4.61983
2025-03-10 01:50:48,576 - INFO - Batch 95600 finished
2025-03-10 01:50:48,576 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:51:05,737 - INFO - Batch 95626, Running Avg Loss: 4.61982
2025-03-10 01:51:22,979 - INFO - Batch 95651, Running Avg Loss: 4.61981
2025-03-10 01:51:40,456 - INFO - Batch 95676, Running Avg Loss: 4.61980
2025-03-10 01:51:58,101 - INFO - Batch 95701, Running Avg Loss: 4.61977
2025-03-10 01:51:58,116 - INFO - Batch 95700 finished
2025-03-10 01:51:58,116 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:52:15,630 - INFO - Batch 95726, Running Avg Loss: 4.61974
2025-03-10 01:52:33,086 - INFO - Batch 95751, Running Avg Loss: 4.61973
2025-03-10 01:52:50,497 - INFO - Batch 95776, Running Avg Loss: 4.61972
2025-03-10 01:53:07,904 - INFO - Batch 95801, Running Avg Loss: 4.61969
2025-03-10 01:53:07,919 - INFO - Batch 95800 finished
2025-03-10 01:53:07,919 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:53:25,348 - INFO - Batch 95826, Running Avg Loss: 4.61967
2025-03-10 01:53:42,786 - INFO - Batch 95851, Running Avg Loss: 4.61965
2025-03-10 01:54:00,226 - INFO - Batch 95876, Running Avg Loss: 4.61962
2025-03-10 01:54:17,684 - INFO - Batch 95901, Running Avg Loss: 4.61959
2025-03-10 01:54:17,699 - INFO - Batch 95900 finished
2025-03-10 01:54:17,699 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:54:35,169 - INFO - Batch 95926, Running Avg Loss: 4.61956
2025-03-10 01:54:52,673 - INFO - Batch 95951, Running Avg Loss: 4.61954
2025-03-10 01:55:10,350 - INFO - Batch 95976, Running Avg Loss: 4.61953
2025-03-10 01:55:27,778 - INFO - Batch 96001, Running Avg Loss: 4.61950
2025-03-10 01:55:27,793 - INFO - Batch 96000 finished
2025-03-10 01:55:27,793 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:55:45,185 - INFO - Batch 96026, Running Avg Loss: 4.61949
2025-03-10 01:56:02,662 - INFO - Batch 96051, Running Avg Loss: 4.61950
2025-03-10 01:56:20,187 - INFO - Batch 96076, Running Avg Loss: 4.61947
2025-03-10 01:56:37,771 - INFO - Batch 96101, Running Avg Loss: 4.61945
2025-03-10 01:56:37,788 - INFO - Batch 96100 finished
2025-03-10 01:56:37,788 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:56:55,446 - INFO - Batch 96126, Running Avg Loss: 4.61942
2025-03-10 01:57:12,853 - INFO - Batch 96151, Running Avg Loss: 4.61943
2025-03-10 01:57:30,241 - INFO - Batch 96176, Running Avg Loss: 4.61941
2025-03-10 01:57:47,654 - INFO - Batch 96201, Running Avg Loss: 4.61939
2025-03-10 01:57:47,672 - INFO - Batch 96200 finished
2025-03-10 01:57:47,673 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:58:05,132 - INFO - Batch 96226, Running Avg Loss: 4.61935
2025-03-10 01:58:22,837 - INFO - Batch 96251, Running Avg Loss: 4.61933
2025-03-10 01:58:40,322 - INFO - Batch 96276, Running Avg Loss: 4.61931
2025-03-10 01:58:57,780 - INFO - Batch 96301, Running Avg Loss: 4.61929
2025-03-10 01:58:57,796 - INFO - Batch 96300 finished
2025-03-10 01:58:57,796 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 01:59:15,287 - INFO - Batch 96326, Running Avg Loss: 4.61925
2025-03-10 01:59:32,811 - INFO - Batch 96351, Running Avg Loss: 4.61922
2025-03-10 01:59:50,363 - INFO - Batch 96376, Running Avg Loss: 4.61919
2025-03-10 02:00:07,974 - INFO - Batch 96401, Running Avg Loss: 4.61919
2025-03-10 02:00:07,991 - INFO - Batch 96400 finished
2025-03-10 02:00:07,992 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:00:25,503 - INFO - Batch 96426, Running Avg Loss: 4.61916
2025-03-10 02:00:42,977 - INFO - Batch 96451, Running Avg Loss: 4.61914
2025-03-10 02:01:00,168 - INFO - Batch 96476, Running Avg Loss: 4.61912
2025-03-10 02:01:17,337 - INFO - Batch 96501, Running Avg Loss: 4.61911
2025-03-10 02:01:17,350 - INFO - Batch 96500 finished
2025-03-10 02:01:17,350 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:01:34,739 - INFO - Batch 96526, Running Avg Loss: 4.61910
2025-03-10 02:01:52,088 - INFO - Batch 96551, Running Avg Loss: 4.61908
2025-03-10 02:02:09,577 - INFO - Batch 96576, Running Avg Loss: 4.61906
2025-03-10 02:02:27,020 - INFO - Batch 96601, Running Avg Loss: 4.61903
2025-03-10 02:02:27,034 - INFO - Batch 96600 finished
2025-03-10 02:02:27,035 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:02:44,525 - INFO - Batch 96626, Running Avg Loss: 4.61902
2025-03-10 02:03:02,036 - INFO - Batch 96651, Running Avg Loss: 4.61900
2025-03-10 02:03:19,614 - INFO - Batch 96676, Running Avg Loss: 4.61901
2025-03-10 02:03:37,122 - INFO - Batch 96701, Running Avg Loss: 4.61897
2025-03-10 02:03:37,137 - INFO - Batch 96700 finished
2025-03-10 02:03:37,138 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:03:54,554 - INFO - Batch 96726, Running Avg Loss: 4.61896
2025-03-10 02:04:11,959 - INFO - Batch 96751, Running Avg Loss: 4.61894
2025-03-10 02:04:29,435 - INFO - Batch 96776, Running Avg Loss: 4.61893
2025-03-10 02:04:46,877 - INFO - Batch 96801, Running Avg Loss: 4.61892
2025-03-10 02:04:46,894 - INFO - Batch 96800 finished
2025-03-10 02:04:46,895 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:05:04,521 - INFO - Batch 96826, Running Avg Loss: 4.61887
2025-03-10 02:05:22,050 - INFO - Batch 96851, Running Avg Loss: 4.61887
2025-03-10 02:05:39,520 - INFO - Batch 96876, Running Avg Loss: 4.61886
2025-03-10 02:05:57,035 - INFO - Batch 96901, Running Avg Loss: 4.61886
2025-03-10 02:05:57,050 - INFO - Batch 96900 finished
2025-03-10 02:05:57,051 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:06:14,648 - INFO - Batch 96926, Running Avg Loss: 4.61884
2025-03-10 02:06:32,175 - INFO - Batch 96951, Running Avg Loss: 4.61882
2025-03-10 02:06:49,604 - INFO - Batch 96976, Running Avg Loss: 4.61878
2025-03-10 02:07:07,043 - INFO - Batch 97001, Running Avg Loss: 4.61876
2025-03-10 02:07:07,058 - INFO - Batch 97000 finished
2025-03-10 02:07:07,058 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:07:24,493 - INFO - Batch 97026, Running Avg Loss: 4.61874
2025-03-10 02:07:42,095 - INFO - Batch 97051, Running Avg Loss: 4.61873
2025-03-10 02:07:59,680 - INFO - Batch 97076, Running Avg Loss: 4.61871
2025-03-10 02:08:17,329 - INFO - Batch 97101, Running Avg Loss: 4.61870
2025-03-10 02:08:17,345 - INFO - Batch 97100 finished
2025-03-10 02:08:17,346 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:08:34,843 - INFO - Batch 97126, Running Avg Loss: 4.61870
2025-03-10 02:08:52,339 - INFO - Batch 97151, Running Avg Loss: 4.61868
2025-03-10 02:09:09,924 - INFO - Batch 97176, Running Avg Loss: 4.61867
2025-03-10 02:09:27,411 - INFO - Batch 97201, Running Avg Loss: 4.61867
2025-03-10 02:09:27,427 - INFO - Batch 97200 finished
2025-03-10 02:09:27,428 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:09:44,809 - INFO - Batch 97226, Running Avg Loss: 4.61867
2025-03-10 02:10:02,358 - INFO - Batch 97251, Running Avg Loss: 4.61865
2025-03-10 02:10:19,909 - INFO - Batch 97276, Running Avg Loss: 4.61861
2025-03-10 02:10:37,488 - INFO - Batch 97301, Running Avg Loss: 4.61859
2025-03-10 02:10:37,504 - INFO - Batch 97300 finished
2025-03-10 02:10:37,505 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:10:54,962 - INFO - Batch 97326, Running Avg Loss: 4.61859
2025-03-10 02:11:12,303 - INFO - Batch 97351, Running Avg Loss: 4.61855
2025-03-10 02:11:29,504 - INFO - Batch 97376, Running Avg Loss: 4.61851
2025-03-10 02:11:47,057 - INFO - Batch 97401, Running Avg Loss: 4.61850
2025-03-10 02:11:47,074 - INFO - Batch 97400 finished
2025-03-10 02:11:47,075 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:12:04,608 - INFO - Batch 97426, Running Avg Loss: 4.61848
2025-03-10 02:12:22,088 - INFO - Batch 97451, Running Avg Loss: 4.61846
2025-03-10 02:12:39,568 - INFO - Batch 97476, Running Avg Loss: 4.61846
2025-03-10 02:12:56,998 - INFO - Batch 97501, Running Avg Loss: 4.61845
2025-03-10 02:12:57,013 - INFO - 
GPU Memory Stats at step 97500:
2025-03-10 02:12:57,014 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-10 02:12:57,014 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-10 02:12:57,014 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-10 02:12:57,014 - INFO - learning rate: 0.00000007
2025-03-10 02:12:57,014 - INFO - Ep 1 (Step 097500): Avg loss 4.618 | 399364096 tokens seen
2025-03-10 02:12:57,014 - INFO - optimizer lr: 0.00000007
2025-03-10 02:12:57,014 - INFO - scheduler lr: 0.00000007
2025-03-10 02:12:57,015 - INFO - Selected prompt: A couple of years ago, I was working as an extra on the set of a low-budget British film.
2025-03-10 02:12:57,015 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:12:57,015 - INFO - random_topk: 2
2025-03-10 02:12:57,015 - INFO - random_temperature: 0.899139404798053
2025-03-10 02:12:57,015 - INFO - global step 97500 , batch_idx 97500 => generating text
2025-03-10 02:12:57,015 - INFO - Generating on device cuda
2025-03-10 02:13:31,243 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:13:31,244 - INFO - A couple of years ago, I was working as an extra on the set of a low-budget British film. It was a small but more powerful event that would be a bit of a new place.

One day, while browsing through a small, I stumbled upon a small group of friends who lived in the world. As a young girl, she noticed a group of people who had been working on the park. He had been working on the world, but there was a way to do it.

As the day of the next few weeks, the group decided to create a new world filled with their own unique style. He asked, "What does 't' mean?"

"What does it mean?"

"I am thinking," said the other side of the world, "I know what we're doing here."

"I think we need to make sure we are doing something, like a library or a computer or a library."

"Well," she said, "I think I need to do with me. I can't help but feel good, but I can't help my friend."

As the next few weeks, I realized something interesting. But I didn't know how much it was for me to make a difference. But then, it was a way of thinking that was the most popular way to do.


2025-03-10 02:13:31,244 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:13:55,773 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_97500_steps_avg_loss_4.61845_optimizer_lr_0.00000007.pth
2025-03-10 02:13:56,059 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-10 02:13:56,059 - INFO - Batch 97500 finished
2025-03-10 02:13:56,059 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:14:13,674 - INFO - Batch 97526, Running Avg Loss: 4.61843
2025-03-10 02:14:31,171 - INFO - Batch 97551, Running Avg Loss: 4.61841
2025-03-10 02:14:48,616 - INFO - Batch 97576, Running Avg Loss: 4.61839
2025-03-10 02:15:06,089 - INFO - Batch 97601, Running Avg Loss: 4.61838
2025-03-10 02:15:06,106 - INFO - Batch 97600 finished
2025-03-10 02:15:06,106 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:15:23,710 - INFO - Batch 97626, Running Avg Loss: 4.61836
2025-03-10 02:15:41,285 - INFO - Batch 97651, Running Avg Loss: 4.61834
2025-03-10 02:15:58,883 - INFO - Batch 97676, Running Avg Loss: 4.61829
2025-03-10 02:16:16,473 - INFO - Batch 97701, Running Avg Loss: 4.61826
2025-03-10 02:16:16,491 - INFO - Batch 97700 finished
2025-03-10 02:16:16,492 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:16:34,047 - INFO - Batch 97726, Running Avg Loss: 4.61827
2025-03-10 02:16:51,614 - INFO - Batch 97751, Running Avg Loss: 4.61824
2025-03-10 02:17:09,201 - INFO - Batch 97776, Running Avg Loss: 4.61822
2025-03-10 02:17:26,926 - INFO - Batch 97801, Running Avg Loss: 4.61822
2025-03-10 02:17:26,942 - INFO - Batch 97800 finished
2025-03-10 02:17:26,943 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:17:44,434 - INFO - Batch 97826, Running Avg Loss: 4.61821
2025-03-10 02:18:02,004 - INFO - Batch 97851, Running Avg Loss: 4.61818
2025-03-10 02:18:19,551 - INFO - Batch 97876, Running Avg Loss: 4.61817
2025-03-10 02:18:37,158 - INFO - Batch 97901, Running Avg Loss: 4.61815
2025-03-10 02:18:37,176 - INFO - Batch 97900 finished
2025-03-10 02:18:37,177 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:18:54,704 - INFO - Batch 97926, Running Avg Loss: 4.61813
2025-03-10 02:19:12,248 - INFO - Batch 97951, Running Avg Loss: 4.61813
2025-03-10 02:19:29,817 - INFO - Batch 97976, Running Avg Loss: 4.61813
2025-03-10 02:19:47,476 - INFO - Batch 98001, Running Avg Loss: 4.61812
2025-03-10 02:19:47,493 - INFO - Batch 98000 finished
2025-03-10 02:19:47,494 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:20:04,977 - INFO - Batch 98026, Running Avg Loss: 4.61809
2025-03-10 02:20:22,431 - INFO - Batch 98051, Running Avg Loss: 4.61805
2025-03-10 02:20:39,847 - INFO - Batch 98076, Running Avg Loss: 4.61803
2025-03-10 02:20:57,338 - INFO - Batch 98101, Running Avg Loss: 4.61801
2025-03-10 02:20:57,353 - INFO - Batch 98100 finished
2025-03-10 02:20:57,354 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:21:14,638 - INFO - Batch 98126, Running Avg Loss: 4.61800
2025-03-10 02:21:32,024 - INFO - Batch 98151, Running Avg Loss: 4.61798
2025-03-10 02:21:49,565 - INFO - Batch 98176, Running Avg Loss: 4.61796
2025-03-10 02:22:07,088 - INFO - Batch 98201, Running Avg Loss: 4.61792
2025-03-10 02:22:07,103 - INFO - Batch 98200 finished
2025-03-10 02:22:07,103 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:22:24,501 - INFO - Batch 98226, Running Avg Loss: 4.61790
2025-03-10 02:22:41,846 - INFO - Batch 98251, Running Avg Loss: 4.61787
2025-03-10 02:22:59,314 - INFO - Batch 98276, Running Avg Loss: 4.61784
2025-03-10 02:23:16,755 - INFO - Batch 98301, Running Avg Loss: 4.61783
2025-03-10 02:23:16,771 - INFO - Batch 98300 finished
2025-03-10 02:23:16,772 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:23:34,252 - INFO - Batch 98326, Running Avg Loss: 4.61782
2025-03-10 02:23:51,765 - INFO - Batch 98351, Running Avg Loss: 4.61780
2025-03-10 02:24:09,355 - INFO - Batch 98376, Running Avg Loss: 4.61780
2025-03-10 02:24:26,768 - INFO - Batch 98401, Running Avg Loss: 4.61780
2025-03-10 02:24:26,784 - INFO - Batch 98400 finished
2025-03-10 02:24:26,785 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:24:44,183 - INFO - Batch 98426, Running Avg Loss: 4.61779
2025-03-10 02:25:01,735 - INFO - Batch 98451, Running Avg Loss: 4.61778
2025-03-10 02:25:19,352 - INFO - Batch 98476, Running Avg Loss: 4.61774
2025-03-10 02:25:36,837 - INFO - Batch 98501, Running Avg Loss: 4.61772
2025-03-10 02:25:36,852 - INFO - Batch 98500 finished
2025-03-10 02:25:36,853 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:25:54,493 - INFO - Batch 98526, Running Avg Loss: 4.61768
2025-03-10 02:26:11,979 - INFO - Batch 98551, Running Avg Loss: 4.61767
2025-03-10 02:26:29,466 - INFO - Batch 98576, Running Avg Loss: 4.61765
2025-03-10 02:26:46,971 - INFO - Batch 98601, Running Avg Loss: 4.61762
2025-03-10 02:26:46,985 - INFO - Batch 98600 finished
2025-03-10 02:26:46,986 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:27:04,436 - INFO - Batch 98626, Running Avg Loss: 4.61761
2025-03-10 02:27:22,055 - INFO - Batch 98651, Running Avg Loss: 4.61760
2025-03-10 02:27:39,519 - INFO - Batch 98676, Running Avg Loss: 4.61758
2025-03-10 02:27:56,972 - INFO - Batch 98701, Running Avg Loss: 4.61755
2025-03-10 02:27:56,987 - INFO - Batch 98700 finished
2025-03-10 02:27:56,987 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:28:14,482 - INFO - Batch 98726, Running Avg Loss: 4.61752
2025-03-10 02:28:31,928 - INFO - Batch 98751, Running Avg Loss: 4.61752
2025-03-10 02:28:49,406 - INFO - Batch 98776, Running Avg Loss: 4.61752
2025-03-10 02:29:06,943 - INFO - Batch 98801, Running Avg Loss: 4.61748
2025-03-10 02:29:06,961 - INFO - Batch 98800 finished
2025-03-10 02:29:06,962 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:29:24,469 - INFO - Batch 98826, Running Avg Loss: 4.61746
2025-03-10 02:29:42,005 - INFO - Batch 98851, Running Avg Loss: 4.61744
2025-03-10 02:29:59,474 - INFO - Batch 98876, Running Avg Loss: 4.61742
2025-03-10 02:30:16,968 - INFO - Batch 98901, Running Avg Loss: 4.61740
2025-03-10 02:30:16,985 - INFO - Batch 98900 finished
2025-03-10 02:30:16,985 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:30:34,700 - INFO - Batch 98926, Running Avg Loss: 4.61738
2025-03-10 02:30:52,235 - INFO - Batch 98951, Running Avg Loss: 4.61736
2025-03-10 02:31:09,757 - INFO - Batch 98976, Running Avg Loss: 4.61735
2025-03-10 02:31:27,317 - INFO - Batch 99001, Running Avg Loss: 4.61733
2025-03-10 02:31:27,335 - INFO - Batch 99000 finished
2025-03-10 02:31:27,335 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:31:44,896 - INFO - Batch 99026, Running Avg Loss: 4.61731
2025-03-10 02:32:02,429 - INFO - Batch 99051, Running Avg Loss: 4.61730
2025-03-10 02:32:20,047 - INFO - Batch 99076, Running Avg Loss: 4.61727
2025-03-10 02:32:37,522 - INFO - Batch 99101, Running Avg Loss: 4.61724
2025-03-10 02:32:37,537 - INFO - Batch 99100 finished
2025-03-10 02:32:37,537 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:32:54,990 - INFO - Batch 99126, Running Avg Loss: 4.61725
2025-03-10 02:33:12,462 - INFO - Batch 99151, Running Avg Loss: 4.61725
2025-03-10 02:33:29,904 - INFO - Batch 99176, Running Avg Loss: 4.61722
2025-03-10 02:33:47,280 - INFO - Batch 99201, Running Avg Loss: 4.61719
2025-03-10 02:33:47,296 - INFO - Batch 99200 finished
2025-03-10 02:33:47,297 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:34:04,766 - INFO - Batch 99226, Running Avg Loss: 4.61716
2025-03-10 02:34:22,116 - INFO - Batch 99251, Running Avg Loss: 4.61717
2025-03-10 02:34:39,616 - INFO - Batch 99276, Running Avg Loss: 4.61714
2025-03-10 02:34:57,109 - INFO - Batch 99301, Running Avg Loss: 4.61714
2025-03-10 02:34:57,126 - INFO - Batch 99300 finished
2025-03-10 02:34:57,127 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:35:14,553 - INFO - Batch 99326, Running Avg Loss: 4.61712
2025-03-10 02:35:31,983 - INFO - Batch 99351, Running Avg Loss: 4.61710
2025-03-10 02:35:49,450 - INFO - Batch 99376, Running Avg Loss: 4.61707
2025-03-10 02:36:06,964 - INFO - Batch 99401, Running Avg Loss: 4.61706
2025-03-10 02:36:06,980 - INFO - Batch 99400 finished
2025-03-10 02:36:06,981 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:36:24,438 - INFO - Batch 99426, Running Avg Loss: 4.61705
2025-03-10 02:36:41,911 - INFO - Batch 99451, Running Avg Loss: 4.61704
2025-03-10 02:36:59,396 - INFO - Batch 99476, Running Avg Loss: 4.61700
2025-03-10 02:37:17,123 - INFO - Batch 99501, Running Avg Loss: 4.61699
2025-03-10 02:37:17,138 - INFO - Batch 99500 finished
2025-03-10 02:37:17,138 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:37:34,660 - INFO - Batch 99526, Running Avg Loss: 4.61698
2025-03-10 02:37:52,121 - INFO - Batch 99551, Running Avg Loss: 4.61697
2025-03-10 02:38:09,549 - INFO - Batch 99576, Running Avg Loss: 4.61696
2025-03-10 02:38:27,010 - INFO - Batch 99601, Running Avg Loss: 4.61693
2025-03-10 02:38:27,027 - INFO - Batch 99600 finished
2025-03-10 02:38:27,028 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:38:44,453 - INFO - Batch 99626, Running Avg Loss: 4.61691
2025-03-10 02:39:01,972 - INFO - Batch 99651, Running Avg Loss: 4.61691
2025-03-10 02:39:19,356 - INFO - Batch 99676, Running Avg Loss: 4.61688
2025-03-10 02:39:36,811 - INFO - Batch 99701, Running Avg Loss: 4.61687
2025-03-10 02:39:36,828 - INFO - Batch 99700 finished
2025-03-10 02:39:36,828 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:39:54,333 - INFO - Batch 99726, Running Avg Loss: 4.61685
2025-03-10 02:40:11,842 - INFO - Batch 99751, Running Avg Loss: 4.61683
2025-03-10 02:40:29,348 - INFO - Batch 99776, Running Avg Loss: 4.61681
2025-03-10 02:40:46,705 - INFO - Batch 99801, Running Avg Loss: 4.61681
2025-03-10 02:40:46,724 - INFO - Batch 99800 finished
2025-03-10 02:40:46,724 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:41:03,987 - INFO - Batch 99826, Running Avg Loss: 4.61678
2025-03-10 02:41:21,175 - INFO - Batch 99851, Running Avg Loss: 4.61677
2025-03-10 02:41:38,635 - INFO - Batch 99876, Running Avg Loss: 4.61675
2025-03-10 02:41:56,054 - INFO - Batch 99901, Running Avg Loss: 4.61673
2025-03-10 02:41:56,071 - INFO - Batch 99900 finished
2025-03-10 02:41:56,071 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:42:13,565 - INFO - Batch 99926, Running Avg Loss: 4.61672
2025-03-10 02:42:31,031 - INFO - Batch 99951, Running Avg Loss: 4.61670
2025-03-10 02:42:48,536 - INFO - Batch 99976, Running Avg Loss: 4.61665
2025-03-10 02:43:06,193 - INFO - Batch 100001, Running Avg Loss: 4.61663
2025-03-10 02:43:06,209 - INFO - 
GPU Memory Stats at step 100000:
2025-03-10 02:43:06,209 - INFO - GPU Memory allocated: 4714.87 MB
2025-03-10 02:43:06,210 - INFO - GPU Memory reserved: 16584.00 MB
2025-03-10 02:43:06,210 - INFO - Max GPU Memory allocated: 15325.68 MB
2025-03-10 02:43:06,210 - INFO - learning rate: 0.00000007
2025-03-10 02:43:06,210 - INFO - Ep 1 (Step 100000): Avg loss 4.617 | 409604096 tokens seen
2025-03-10 02:43:06,210 - INFO - optimizer lr: 0.00000007
2025-03-10 02:43:06,210 - INFO - scheduler lr: 0.00000007
2025-03-10 02:43:06,210 - INFO - Selected prompt: Once upon a time, there was a friendly agency called Gaudette Insurance Agency, Inc. They help
2025-03-10 02:43:06,210 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:43:06,211 - INFO - random_topk: 9
2025-03-10 02:43:06,211 - INFO - random_temperature: 0.727414034217082
2025-03-10 02:43:06,211 - INFO - global step 100000 , batch_idx 100000 => generating text
2025-03-10 02:43:06,211 - INFO - Generating on device cuda
2025-03-10 02:43:39,964 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:43:39,966 - INFO - Once upon a time, there was a friendly agency called Gaudette Insurance Agency, Inc. They help make sure everything else was made, like a little girl, a little girl named Lily.

One day, Mr. Sarah, who came across a big, beautiful park called a "The Rog," which was like a big family.

One day, a big and beautiful city named The Gira who had been working on the park, but there was a special place called "Cay."

Sally asked, "What is a big building and special? Is it a good time, and we want to keep track of your friends?" To help her feel better, Lily explained that when someone is sick or sad, they can do something to be happy, and then put on a new place to make a big difference.

But then, Mr. Johnson asked, "Why do you want to help you find out if we don't want to make sure it's safe?"

"Oh!" exclaimed Timmy. "That sounds like a big city, right? We need to learn more about our world. And when you're thinking, we create a big, strong bond with others and the people who need to do it.

Bob then asked if someone was the best person who wants to be a good idea. They wanted to do
2025-03-10 02:43:39,966 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:44:05,049 - INFO - Model saved to S3: s3://deepseek-v3-train-mar-2025/checkpoints/model_100000_steps_avg_loss_4.61663_optimizer_lr_0.00000007.pth
2025-03-10 02:44:05,365 - INFO - Log saved to S3: s3://deepseek-v3-train-mar-2025/logs/training.log
2025-03-10 02:44:05,365 - INFO - Batch 100000 finished
2025-03-10 02:44:05,365 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:44:22,637 - INFO - Batch 100026, Running Avg Loss: 4.61660
2025-03-10 02:44:40,077 - INFO - Batch 100051, Running Avg Loss: 4.61657
2025-03-10 02:44:57,620 - INFO - Batch 100076, Running Avg Loss: 4.61657
2025-03-10 02:45:15,177 - INFO - Batch 100101, Running Avg Loss: 4.61656
2025-03-10 02:45:15,195 - INFO - Batch 100100 finished
2025-03-10 02:45:15,196 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:45:32,687 - INFO - Batch 100126, Running Avg Loss: 4.61655
2025-03-10 02:45:50,218 - INFO - Batch 100151, Running Avg Loss: 4.61652
2025-03-10 02:46:07,691 - INFO - Batch 100176, Running Avg Loss: 4.61652
2025-03-10 02:46:25,209 - INFO - Batch 100201, Running Avg Loss: 4.61650
2025-03-10 02:46:25,226 - INFO - Batch 100200 finished
2025-03-10 02:46:25,227 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:46:42,901 - INFO - Batch 100226, Running Avg Loss: 4.61647
2025-03-10 02:47:00,301 - INFO - Batch 100251, Running Avg Loss: 4.61643
2025-03-10 02:47:17,771 - INFO - Batch 100276, Running Avg Loss: 4.61643
2025-03-10 02:47:35,256 - INFO - Batch 100301, Running Avg Loss: 4.61640
2025-03-10 02:47:35,275 - INFO - Batch 100300 finished
2025-03-10 02:47:35,275 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:47:52,728 - INFO - Batch 100326, Running Avg Loss: 4.61639
2025-03-10 02:48:10,193 - INFO - Batch 100351, Running Avg Loss: 4.61638
2025-03-10 02:48:27,827 - INFO - Batch 100376, Running Avg Loss: 4.61636
2025-03-10 02:48:45,251 - INFO - Batch 100401, Running Avg Loss: 4.61634
2025-03-10 02:48:45,270 - INFO - Batch 100400 finished
2025-03-10 02:48:45,270 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:49:02,607 - INFO - Batch 100426, Running Avg Loss: 4.61634
2025-03-10 02:49:20,004 - INFO - Batch 100451, Running Avg Loss: 4.61633
2025-03-10 02:49:37,666 - INFO - Batch 100476, Running Avg Loss: 4.61631
2025-03-10 02:49:55,187 - INFO - Batch 100501, Running Avg Loss: 4.61626
2025-03-10 02:49:55,202 - INFO - Batch 100500 finished
2025-03-10 02:49:55,202 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:50:12,645 - INFO - Batch 100526, Running Avg Loss: 4.61626
2025-03-10 02:50:30,104 - INFO - Batch 100551, Running Avg Loss: 4.61624
2025-03-10 02:50:47,282 - INFO - Batch 100576, Running Avg Loss: 4.61622
2025-03-10 02:51:04,472 - INFO - Batch 100601, Running Avg Loss: 4.61620
2025-03-10 02:51:04,484 - INFO - Batch 100600 finished
2025-03-10 02:51:04,485 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:51:21,744 - INFO - Batch 100626, Running Avg Loss: 4.61618
2025-03-10 02:51:39,218 - INFO - Batch 100651, Running Avg Loss: 4.61616
2025-03-10 02:51:56,669 - INFO - Batch 100676, Running Avg Loss: 4.61614
2025-03-10 02:52:14,126 - INFO - Batch 100701, Running Avg Loss: 4.61615
2025-03-10 02:52:14,147 - INFO - Batch 100700 finished
2025-03-10 02:52:14,149 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:52:31,567 - INFO - Batch 100726, Running Avg Loss: 4.61611
2025-03-10 02:52:48,964 - INFO - Batch 100751, Running Avg Loss: 4.61609
2025-03-10 02:53:06,495 - INFO - Batch 100776, Running Avg Loss: 4.61606
2025-03-10 02:53:23,926 - INFO - Batch 100801, Running Avg Loss: 4.61603
2025-03-10 02:53:23,941 - INFO - Batch 100800 finished
2025-03-10 02:53:23,942 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:53:41,395 - INFO - Batch 100826, Running Avg Loss: 4.61601
2025-03-10 02:53:58,799 - INFO - Batch 100851, Running Avg Loss: 4.61602
2025-03-10 02:54:16,236 - INFO - Batch 100876, Running Avg Loss: 4.61600
2025-03-10 02:54:33,815 - INFO - Batch 100901, Running Avg Loss: 4.61597
2025-03-10 02:54:33,833 - INFO - Batch 100900 finished
2025-03-10 02:54:33,834 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:54:51,297 - INFO - Batch 100926, Running Avg Loss: 4.61592
2025-03-10 02:55:08,734 - INFO - Batch 100951, Running Avg Loss: 4.61591
2025-03-10 02:55:26,141 - INFO - Batch 100976, Running Avg Loss: 4.61590
2025-03-10 02:55:43,592 - INFO - Batch 101001, Running Avg Loss: 4.61589
2025-03-10 02:55:43,607 - INFO - Batch 101000 finished
2025-03-10 02:55:43,608 - INFO - ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2025-03-10 02:56:01,036 - INFO - Batch 101026, Running Avg Loss: 4.61587
